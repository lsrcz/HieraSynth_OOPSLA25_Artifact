; ModuleID = 'concat.cpp'
source_filename = "concat.cpp"
target datalayout = "e-m:e-p:64:64-i64:64-i128:128-n32:64-S128"
target triple = "riscv64-unknown-linux-gnu"

$_Z16ConcatUpperLowerIu16__rvv_uint8mf8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint8mf8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint8mf8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint8mf8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_uint8mf4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint8mf4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint8mf4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint8mf4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_uint8mf2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint8mf2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint8mf2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint8mf2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_uint8m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_uint8m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_uint8m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_uint8m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_uint8m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_uint8m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_uint8m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_uint8m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_uint8m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_uint8m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_uint8m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_uint8m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_uint8m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_uint8m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_uint8m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_uint8m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu17__rvv_uint16mf4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu17__rvv_uint16mf4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu17__rvv_uint16mf4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu17__rvv_uint16mf4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu17__rvv_uint16mf2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu17__rvv_uint16mf2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu17__rvv_uint16mf2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu17__rvv_uint16mf2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_uint16m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint16m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint16m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint16m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_uint16m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint16m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint16m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint16m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_uint16m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint16m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint16m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint16m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_uint16m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint16m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint16m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint16m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu17__rvv_uint32mf2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu17__rvv_uint32mf2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu17__rvv_uint32mf2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu17__rvv_uint32mf2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_uint32m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint32m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint32m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint32m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_uint32m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint32m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint32m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint32m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_uint32m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint32m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint32m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint32m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_uint32m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint32m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint32m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint32m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_uint64m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint64m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint64m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint64m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_uint64m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint64m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint64m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint64m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_uint64m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint64m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint64m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint64m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_uint64m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_uint64m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_uint64m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_uint64m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int8mf8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int8mf8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int8mf8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int8mf8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int8mf4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int8mf4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int8mf4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int8mf4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int8mf2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int8mf2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int8mf2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int8mf2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu14__rvv_int8m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu14__rvv_int8m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu14__rvv_int8m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu14__rvv_int8m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu14__rvv_int8m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu14__rvv_int8m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu14__rvv_int8m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu14__rvv_int8m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu14__rvv_int8m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu14__rvv_int8m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu14__rvv_int8m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu14__rvv_int8m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu14__rvv_int8m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu14__rvv_int8m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu14__rvv_int8m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu14__rvv_int8m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_int16mf4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_int16mf4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_int16mf4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_int16mf4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_int16mf2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_int16mf2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_int16mf2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_int16mf2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int16m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int16m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int16m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int16m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int16m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int16m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int16m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int16m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int16m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int16m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int16m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int16m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int16m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int16m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int16m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int16m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu16__rvv_int32mf2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu16__rvv_int32mf2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu16__rvv_int32mf2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu16__rvv_int32mf2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int32m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int32m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int32m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int32m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int32m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int32m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int32m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int32m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int32m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int32m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int32m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int32m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int32m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int32m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int32m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int32m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int64m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int64m1_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int64m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int64m1_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int64m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int64m2_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int64m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int64m2_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int64m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int64m4_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int64m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int64m4_tET_S0_S0_ = comdat any

$_Z16ConcatUpperLowerIu15__rvv_int64m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerUpperIu15__rvv_int64m8_tET_S0_S0_ = comdat any

$_Z16ConcatLowerLowerIu15__rvv_int64m8_tET_S0_S0_ = comdat any

$_Z16ConcatUpperUpperIu15__rvv_int64m8_tET_S0_S0_ = comdat any

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z16ConcatUpperLowerIu16__rvv_uint8mf8_tET_S0_S0_(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vslidedown.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %1, <vscale x 1 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z16ConcatLowerUpperIu16__rvv_uint8mf8_tET_S0_S0_(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vslidedown.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %5, <vscale x 1 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z16ConcatLowerLowerIu16__rvv_uint8mf8_tET_S0_S0_(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %1, <vscale x 1 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i8> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z16ConcatUpperUpperIu16__rvv_uint8mf8_tET_S0_S0_(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vslidedown.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vslidedown.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %6, <vscale x 1 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z16ConcatUpperLowerIu16__rvv_uint8mf4_tET_S0_S0_(<vscale x 2 x i8> %0, <vscale x 2 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslidedown.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %1, <vscale x 2 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z16ConcatLowerUpperIu16__rvv_uint8mf4_tET_S0_S0_(<vscale x 2 x i8> %0, <vscale x 2 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslidedown.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z16ConcatLowerLowerIu16__rvv_uint8mf4_tET_S0_S0_(<vscale x 2 x i8> %0, <vscale x 2 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %1, <vscale x 2 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i8> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z16ConcatUpperUpperIu16__rvv_uint8mf4_tET_S0_S0_(<vscale x 2 x i8> %0, <vscale x 2 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslidedown.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vslidedown.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z16ConcatUpperLowerIu16__rvv_uint8mf2_tET_S0_S0_(<vscale x 4 x i8> %0, <vscale x 4 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslidedown.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %1, <vscale x 4 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z16ConcatLowerUpperIu16__rvv_uint8mf2_tET_S0_S0_(<vscale x 4 x i8> %0, <vscale x 4 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslidedown.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z16ConcatLowerLowerIu16__rvv_uint8mf2_tET_S0_S0_(<vscale x 4 x i8> %0, <vscale x 4 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %1, <vscale x 4 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i8> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z16ConcatUpperUpperIu16__rvv_uint8mf2_tET_S0_S0_(<vscale x 4 x i8> %0, <vscale x 4 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslidedown.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslidedown.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z16ConcatUpperLowerIu15__rvv_uint8m1_tET_S0_S0_(<vscale x 8 x i8> %0, <vscale x 8 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %1, <vscale x 8 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z16ConcatLowerUpperIu15__rvv_uint8m1_tET_S0_S0_(<vscale x 8 x i8> %0, <vscale x 8 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z16ConcatLowerLowerIu15__rvv_uint8m1_tET_S0_S0_(<vscale x 8 x i8> %0, <vscale x 8 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %1, <vscale x 8 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i8> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z16ConcatUpperUpperIu15__rvv_uint8m1_tET_S0_S0_(<vscale x 8 x i8> %0, <vscale x 8 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z16ConcatUpperLowerIu15__rvv_uint8m2_tET_S0_S0_(<vscale x 16 x i8> %0, <vscale x 16 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslidedown.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %1, <vscale x 16 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z16ConcatLowerUpperIu15__rvv_uint8m2_tET_S0_S0_(<vscale x 16 x i8> %0, <vscale x 16 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslidedown.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z16ConcatLowerLowerIu15__rvv_uint8m2_tET_S0_S0_(<vscale x 16 x i8> %0, <vscale x 16 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %1, <vscale x 16 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i8> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z16ConcatUpperUpperIu15__rvv_uint8m2_tET_S0_S0_(<vscale x 16 x i8> %0, <vscale x 16 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslidedown.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslidedown.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z16ConcatUpperLowerIu15__rvv_uint8m4_tET_S0_S0_(<vscale x 32 x i8> %0, <vscale x 32 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslidedown.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %1, <vscale x 32 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z16ConcatLowerUpperIu15__rvv_uint8m4_tET_S0_S0_(<vscale x 32 x i8> %0, <vscale x 32 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslidedown.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z16ConcatLowerLowerIu15__rvv_uint8m4_tET_S0_S0_(<vscale x 32 x i8> %0, <vscale x 32 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %1, <vscale x 32 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i8> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z16ConcatUpperUpperIu15__rvv_uint8m4_tET_S0_S0_(<vscale x 32 x i8> %0, <vscale x 32 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslidedown.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslidedown.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z16ConcatUpperLowerIu15__rvv_uint8m8_tET_S0_S0_(<vscale x 64 x i8> %0, <vscale x 64 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslidedown.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %1, <vscale x 64 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 64 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z16ConcatLowerUpperIu15__rvv_uint8m8_tET_S0_S0_(<vscale x 64 x i8> %0, <vscale x 64 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslidedown.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 64 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z16ConcatLowerLowerIu15__rvv_uint8m8_tET_S0_S0_(<vscale x 64 x i8> %0, <vscale x 64 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %1, <vscale x 64 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 64 x i8> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z16ConcatUpperUpperIu15__rvv_uint8m8_tET_S0_S0_(<vscale x 64 x i8> %0, <vscale x 64 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslidedown.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslidedown.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 64 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z16ConcatUpperLowerIu17__rvv_uint16mf4_tET_S0_S0_(<vscale x 1 x i16> %0, <vscale x 1 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i16> @llvm.riscv.vslidedown.nxv1i16.i64(<vscale x 1 x i16> poison, <vscale x 1 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i16> @llvm.riscv.vslideup.nxv1i16.i64(<vscale x 1 x i16> %1, <vscale x 1 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z16ConcatLowerUpperIu17__rvv_uint16mf4_tET_S0_S0_(<vscale x 1 x i16> %0, <vscale x 1 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i16> @llvm.riscv.vslidedown.nxv1i16.i64(<vscale x 1 x i16> poison, <vscale x 1 x i16> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i16> @llvm.riscv.vslideup.nxv1i16.i64(<vscale x 1 x i16> %5, <vscale x 1 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z16ConcatLowerLowerIu17__rvv_uint16mf4_tET_S0_S0_(<vscale x 1 x i16> %0, <vscale x 1 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i16> @llvm.riscv.vslideup.nxv1i16.i64(<vscale x 1 x i16> %1, <vscale x 1 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i16> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z16ConcatUpperUpperIu17__rvv_uint16mf4_tET_S0_S0_(<vscale x 1 x i16> %0, <vscale x 1 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i16> @llvm.riscv.vslidedown.nxv1i16.i64(<vscale x 1 x i16> poison, <vscale x 1 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i16> @llvm.riscv.vslidedown.nxv1i16.i64(<vscale x 1 x i16> poison, <vscale x 1 x i16> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 1 x i16> @llvm.riscv.vslideup.nxv1i16.i64(<vscale x 1 x i16> %6, <vscale x 1 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i16> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z16ConcatUpperLowerIu17__rvv_uint16mf2_tET_S0_S0_(<vscale x 2 x i16> %0, <vscale x 2 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i16> @llvm.riscv.vslidedown.nxv2i16.i64(<vscale x 2 x i16> poison, <vscale x 2 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i16> @llvm.riscv.vslideup.nxv2i16.i64(<vscale x 2 x i16> %1, <vscale x 2 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z16ConcatLowerUpperIu17__rvv_uint16mf2_tET_S0_S0_(<vscale x 2 x i16> %0, <vscale x 2 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i16> @llvm.riscv.vslidedown.nxv2i16.i64(<vscale x 2 x i16> poison, <vscale x 2 x i16> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i16> @llvm.riscv.vslideup.nxv2i16.i64(<vscale x 2 x i16> %5, <vscale x 2 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z16ConcatLowerLowerIu17__rvv_uint16mf2_tET_S0_S0_(<vscale x 2 x i16> %0, <vscale x 2 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i16> @llvm.riscv.vslideup.nxv2i16.i64(<vscale x 2 x i16> %1, <vscale x 2 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i16> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z16ConcatUpperUpperIu17__rvv_uint16mf2_tET_S0_S0_(<vscale x 2 x i16> %0, <vscale x 2 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i16> @llvm.riscv.vslidedown.nxv2i16.i64(<vscale x 2 x i16> poison, <vscale x 2 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i16> @llvm.riscv.vslidedown.nxv2i16.i64(<vscale x 2 x i16> poison, <vscale x 2 x i16> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i16> @llvm.riscv.vslideup.nxv2i16.i64(<vscale x 2 x i16> %6, <vscale x 2 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i16> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z16ConcatUpperLowerIu16__rvv_uint16m1_tET_S0_S0_(<vscale x 4 x i16> %0, <vscale x 4 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i16> @llvm.riscv.vslidedown.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i16> @llvm.riscv.vslideup.nxv4i16.i64(<vscale x 4 x i16> %1, <vscale x 4 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z16ConcatLowerUpperIu16__rvv_uint16m1_tET_S0_S0_(<vscale x 4 x i16> %0, <vscale x 4 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i16> @llvm.riscv.vslidedown.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i16> @llvm.riscv.vslideup.nxv4i16.i64(<vscale x 4 x i16> %5, <vscale x 4 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z16ConcatLowerLowerIu16__rvv_uint16m1_tET_S0_S0_(<vscale x 4 x i16> %0, <vscale x 4 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i16> @llvm.riscv.vslideup.nxv4i16.i64(<vscale x 4 x i16> %1, <vscale x 4 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i16> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z16ConcatUpperUpperIu16__rvv_uint16m1_tET_S0_S0_(<vscale x 4 x i16> %0, <vscale x 4 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i16> @llvm.riscv.vslidedown.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i16> @llvm.riscv.vslidedown.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i16> @llvm.riscv.vslideup.nxv4i16.i64(<vscale x 4 x i16> %6, <vscale x 4 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i16> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z16ConcatUpperLowerIu16__rvv_uint16m2_tET_S0_S0_(<vscale x 8 x i16> %0, <vscale x 8 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i16> @llvm.riscv.vslidedown.nxv8i16.i64(<vscale x 8 x i16> poison, <vscale x 8 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i16> @llvm.riscv.vslideup.nxv8i16.i64(<vscale x 8 x i16> %1, <vscale x 8 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z16ConcatLowerUpperIu16__rvv_uint16m2_tET_S0_S0_(<vscale x 8 x i16> %0, <vscale x 8 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i16> @llvm.riscv.vslidedown.nxv8i16.i64(<vscale x 8 x i16> poison, <vscale x 8 x i16> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i16> @llvm.riscv.vslideup.nxv8i16.i64(<vscale x 8 x i16> %5, <vscale x 8 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z16ConcatLowerLowerIu16__rvv_uint16m2_tET_S0_S0_(<vscale x 8 x i16> %0, <vscale x 8 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i16> @llvm.riscv.vslideup.nxv8i16.i64(<vscale x 8 x i16> %1, <vscale x 8 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i16> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z16ConcatUpperUpperIu16__rvv_uint16m2_tET_S0_S0_(<vscale x 8 x i16> %0, <vscale x 8 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i16> @llvm.riscv.vslidedown.nxv8i16.i64(<vscale x 8 x i16> poison, <vscale x 8 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i16> @llvm.riscv.vslidedown.nxv8i16.i64(<vscale x 8 x i16> poison, <vscale x 8 x i16> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i16> @llvm.riscv.vslideup.nxv8i16.i64(<vscale x 8 x i16> %6, <vscale x 8 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i16> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z16ConcatUpperLowerIu16__rvv_uint16m4_tET_S0_S0_(<vscale x 16 x i16> %0, <vscale x 16 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i16> @llvm.riscv.vslidedown.nxv16i16.i64(<vscale x 16 x i16> poison, <vscale x 16 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i16> @llvm.riscv.vslideup.nxv16i16.i64(<vscale x 16 x i16> %1, <vscale x 16 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z16ConcatLowerUpperIu16__rvv_uint16m4_tET_S0_S0_(<vscale x 16 x i16> %0, <vscale x 16 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i16> @llvm.riscv.vslidedown.nxv16i16.i64(<vscale x 16 x i16> poison, <vscale x 16 x i16> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i16> @llvm.riscv.vslideup.nxv16i16.i64(<vscale x 16 x i16> %5, <vscale x 16 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z16ConcatLowerLowerIu16__rvv_uint16m4_tET_S0_S0_(<vscale x 16 x i16> %0, <vscale x 16 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i16> @llvm.riscv.vslideup.nxv16i16.i64(<vscale x 16 x i16> %1, <vscale x 16 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i16> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z16ConcatUpperUpperIu16__rvv_uint16m4_tET_S0_S0_(<vscale x 16 x i16> %0, <vscale x 16 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i16> @llvm.riscv.vslidedown.nxv16i16.i64(<vscale x 16 x i16> poison, <vscale x 16 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i16> @llvm.riscv.vslidedown.nxv16i16.i64(<vscale x 16 x i16> poison, <vscale x 16 x i16> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i16> @llvm.riscv.vslideup.nxv16i16.i64(<vscale x 16 x i16> %6, <vscale x 16 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i16> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z16ConcatUpperLowerIu16__rvv_uint16m8_tET_S0_S0_(<vscale x 32 x i16> %0, <vscale x 32 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i16> @llvm.riscv.vslidedown.nxv32i16.i64(<vscale x 32 x i16> poison, <vscale x 32 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 32 x i16> @llvm.riscv.vslideup.nxv32i16.i64(<vscale x 32 x i16> %1, <vscale x 32 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z16ConcatLowerUpperIu16__rvv_uint16m8_tET_S0_S0_(<vscale x 32 x i16> %0, <vscale x 32 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i16> @llvm.riscv.vslidedown.nxv32i16.i64(<vscale x 32 x i16> poison, <vscale x 32 x i16> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 32 x i16> @llvm.riscv.vslideup.nxv32i16.i64(<vscale x 32 x i16> %5, <vscale x 32 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z16ConcatLowerLowerIu16__rvv_uint16m8_tET_S0_S0_(<vscale x 32 x i16> %0, <vscale x 32 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i16> @llvm.riscv.vslideup.nxv32i16.i64(<vscale x 32 x i16> %1, <vscale x 32 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i16> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z16ConcatUpperUpperIu16__rvv_uint16m8_tET_S0_S0_(<vscale x 32 x i16> %0, <vscale x 32 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i16> @llvm.riscv.vslidedown.nxv32i16.i64(<vscale x 32 x i16> poison, <vscale x 32 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 32 x i16> @llvm.riscv.vslidedown.nxv32i16.i64(<vscale x 32 x i16> poison, <vscale x 32 x i16> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i16> @llvm.riscv.vslideup.nxv32i16.i64(<vscale x 32 x i16> %6, <vscale x 32 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i16> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z16ConcatUpperLowerIu17__rvv_uint32mf2_tET_S0_S0_(<vscale x 1 x i32> %0, <vscale x 1 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i32> @llvm.riscv.vslidedown.nxv1i32.i64(<vscale x 1 x i32> poison, <vscale x 1 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i32> @llvm.riscv.vslideup.nxv1i32.i64(<vscale x 1 x i32> %1, <vscale x 1 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z16ConcatLowerUpperIu17__rvv_uint32mf2_tET_S0_S0_(<vscale x 1 x i32> %0, <vscale x 1 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i32> @llvm.riscv.vslidedown.nxv1i32.i64(<vscale x 1 x i32> poison, <vscale x 1 x i32> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i32> @llvm.riscv.vslideup.nxv1i32.i64(<vscale x 1 x i32> %5, <vscale x 1 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z16ConcatLowerLowerIu17__rvv_uint32mf2_tET_S0_S0_(<vscale x 1 x i32> %0, <vscale x 1 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i32> @llvm.riscv.vslideup.nxv1i32.i64(<vscale x 1 x i32> %1, <vscale x 1 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i32> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z16ConcatUpperUpperIu17__rvv_uint32mf2_tET_S0_S0_(<vscale x 1 x i32> %0, <vscale x 1 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i32> @llvm.riscv.vslidedown.nxv1i32.i64(<vscale x 1 x i32> poison, <vscale x 1 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i32> @llvm.riscv.vslidedown.nxv1i32.i64(<vscale x 1 x i32> poison, <vscale x 1 x i32> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 1 x i32> @llvm.riscv.vslideup.nxv1i32.i64(<vscale x 1 x i32> %6, <vscale x 1 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i32> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z16ConcatUpperLowerIu16__rvv_uint32m1_tET_S0_S0_(<vscale x 2 x i32> %0, <vscale x 2 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i32> @llvm.riscv.vslidedown.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i32> @llvm.riscv.vslideup.nxv2i32.i64(<vscale x 2 x i32> %1, <vscale x 2 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z16ConcatLowerUpperIu16__rvv_uint32m1_tET_S0_S0_(<vscale x 2 x i32> %0, <vscale x 2 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i32> @llvm.riscv.vslidedown.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i32> @llvm.riscv.vslideup.nxv2i32.i64(<vscale x 2 x i32> %5, <vscale x 2 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z16ConcatLowerLowerIu16__rvv_uint32m1_tET_S0_S0_(<vscale x 2 x i32> %0, <vscale x 2 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i32> @llvm.riscv.vslideup.nxv2i32.i64(<vscale x 2 x i32> %1, <vscale x 2 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i32> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z16ConcatUpperUpperIu16__rvv_uint32m1_tET_S0_S0_(<vscale x 2 x i32> %0, <vscale x 2 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i32> @llvm.riscv.vslidedown.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i32> @llvm.riscv.vslidedown.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i32> @llvm.riscv.vslideup.nxv2i32.i64(<vscale x 2 x i32> %6, <vscale x 2 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i32> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z16ConcatUpperLowerIu16__rvv_uint32m2_tET_S0_S0_(<vscale x 4 x i32> %0, <vscale x 4 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i32> @llvm.riscv.vslidedown.nxv4i32.i64(<vscale x 4 x i32> poison, <vscale x 4 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i32> @llvm.riscv.vslideup.nxv4i32.i64(<vscale x 4 x i32> %1, <vscale x 4 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z16ConcatLowerUpperIu16__rvv_uint32m2_tET_S0_S0_(<vscale x 4 x i32> %0, <vscale x 4 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i32> @llvm.riscv.vslidedown.nxv4i32.i64(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i32> @llvm.riscv.vslideup.nxv4i32.i64(<vscale x 4 x i32> %5, <vscale x 4 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z16ConcatLowerLowerIu16__rvv_uint32m2_tET_S0_S0_(<vscale x 4 x i32> %0, <vscale x 4 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i32> @llvm.riscv.vslideup.nxv4i32.i64(<vscale x 4 x i32> %1, <vscale x 4 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i32> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z16ConcatUpperUpperIu16__rvv_uint32m2_tET_S0_S0_(<vscale x 4 x i32> %0, <vscale x 4 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i32> @llvm.riscv.vslidedown.nxv4i32.i64(<vscale x 4 x i32> poison, <vscale x 4 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i32> @llvm.riscv.vslidedown.nxv4i32.i64(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i32> @llvm.riscv.vslideup.nxv4i32.i64(<vscale x 4 x i32> %6, <vscale x 4 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i32> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z16ConcatUpperLowerIu16__rvv_uint32m4_tET_S0_S0_(<vscale x 8 x i32> %0, <vscale x 8 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i32> @llvm.riscv.vslidedown.nxv8i32.i64(<vscale x 8 x i32> poison, <vscale x 8 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i32> @llvm.riscv.vslideup.nxv8i32.i64(<vscale x 8 x i32> %1, <vscale x 8 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z16ConcatLowerUpperIu16__rvv_uint32m4_tET_S0_S0_(<vscale x 8 x i32> %0, <vscale x 8 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i32> @llvm.riscv.vslidedown.nxv8i32.i64(<vscale x 8 x i32> poison, <vscale x 8 x i32> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i32> @llvm.riscv.vslideup.nxv8i32.i64(<vscale x 8 x i32> %5, <vscale x 8 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z16ConcatLowerLowerIu16__rvv_uint32m4_tET_S0_S0_(<vscale x 8 x i32> %0, <vscale x 8 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i32> @llvm.riscv.vslideup.nxv8i32.i64(<vscale x 8 x i32> %1, <vscale x 8 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i32> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z16ConcatUpperUpperIu16__rvv_uint32m4_tET_S0_S0_(<vscale x 8 x i32> %0, <vscale x 8 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i32> @llvm.riscv.vslidedown.nxv8i32.i64(<vscale x 8 x i32> poison, <vscale x 8 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i32> @llvm.riscv.vslidedown.nxv8i32.i64(<vscale x 8 x i32> poison, <vscale x 8 x i32> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i32> @llvm.riscv.vslideup.nxv8i32.i64(<vscale x 8 x i32> %6, <vscale x 8 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i32> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z16ConcatUpperLowerIu16__rvv_uint32m8_tET_S0_S0_(<vscale x 16 x i32> %0, <vscale x 16 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i32> @llvm.riscv.vslidedown.nxv16i32.i64(<vscale x 16 x i32> poison, <vscale x 16 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i32> @llvm.riscv.vslideup.nxv16i32.i64(<vscale x 16 x i32> %1, <vscale x 16 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z16ConcatLowerUpperIu16__rvv_uint32m8_tET_S0_S0_(<vscale x 16 x i32> %0, <vscale x 16 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i32> @llvm.riscv.vslidedown.nxv16i32.i64(<vscale x 16 x i32> poison, <vscale x 16 x i32> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i32> @llvm.riscv.vslideup.nxv16i32.i64(<vscale x 16 x i32> %5, <vscale x 16 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z16ConcatLowerLowerIu16__rvv_uint32m8_tET_S0_S0_(<vscale x 16 x i32> %0, <vscale x 16 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i32> @llvm.riscv.vslideup.nxv16i32.i64(<vscale x 16 x i32> %1, <vscale x 16 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i32> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z16ConcatUpperUpperIu16__rvv_uint32m8_tET_S0_S0_(<vscale x 16 x i32> %0, <vscale x 16 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i32> @llvm.riscv.vslidedown.nxv16i32.i64(<vscale x 16 x i32> poison, <vscale x 16 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i32> @llvm.riscv.vslidedown.nxv16i32.i64(<vscale x 16 x i32> poison, <vscale x 16 x i32> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i32> @llvm.riscv.vslideup.nxv16i32.i64(<vscale x 16 x i32> %6, <vscale x 16 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i32> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z16ConcatUpperLowerIu16__rvv_uint64m1_tET_S0_S0_(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i64> @llvm.riscv.vslidedown.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i64> @llvm.riscv.vslideup.nxv1i64.i64(<vscale x 1 x i64> %1, <vscale x 1 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z16ConcatLowerUpperIu16__rvv_uint64m1_tET_S0_S0_(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i64> @llvm.riscv.vslidedown.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i64> @llvm.riscv.vslideup.nxv1i64.i64(<vscale x 1 x i64> %5, <vscale x 1 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z16ConcatLowerLowerIu16__rvv_uint64m1_tET_S0_S0_(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i64> @llvm.riscv.vslideup.nxv1i64.i64(<vscale x 1 x i64> %1, <vscale x 1 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i64> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z16ConcatUpperUpperIu16__rvv_uint64m1_tET_S0_S0_(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i64> @llvm.riscv.vslidedown.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i64> @llvm.riscv.vslidedown.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 1 x i64> @llvm.riscv.vslideup.nxv1i64.i64(<vscale x 1 x i64> %6, <vscale x 1 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i64> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z16ConcatUpperLowerIu16__rvv_uint64m2_tET_S0_S0_(<vscale x 2 x i64> %0, <vscale x 2 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i64> @llvm.riscv.vslidedown.nxv2i64.i64(<vscale x 2 x i64> poison, <vscale x 2 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i64> @llvm.riscv.vslideup.nxv2i64.i64(<vscale x 2 x i64> %1, <vscale x 2 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z16ConcatLowerUpperIu16__rvv_uint64m2_tET_S0_S0_(<vscale x 2 x i64> %0, <vscale x 2 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i64> @llvm.riscv.vslidedown.nxv2i64.i64(<vscale x 2 x i64> poison, <vscale x 2 x i64> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i64> @llvm.riscv.vslideup.nxv2i64.i64(<vscale x 2 x i64> %5, <vscale x 2 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z16ConcatLowerLowerIu16__rvv_uint64m2_tET_S0_S0_(<vscale x 2 x i64> %0, <vscale x 2 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i64> @llvm.riscv.vslideup.nxv2i64.i64(<vscale x 2 x i64> %1, <vscale x 2 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i64> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z16ConcatUpperUpperIu16__rvv_uint64m2_tET_S0_S0_(<vscale x 2 x i64> %0, <vscale x 2 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i64> @llvm.riscv.vslidedown.nxv2i64.i64(<vscale x 2 x i64> poison, <vscale x 2 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i64> @llvm.riscv.vslidedown.nxv2i64.i64(<vscale x 2 x i64> poison, <vscale x 2 x i64> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i64> @llvm.riscv.vslideup.nxv2i64.i64(<vscale x 2 x i64> %6, <vscale x 2 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i64> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z16ConcatUpperLowerIu16__rvv_uint64m4_tET_S0_S0_(<vscale x 4 x i64> %0, <vscale x 4 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i64> @llvm.riscv.vslidedown.nxv4i64.i64(<vscale x 4 x i64> poison, <vscale x 4 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i64> @llvm.riscv.vslideup.nxv4i64.i64(<vscale x 4 x i64> %1, <vscale x 4 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z16ConcatLowerUpperIu16__rvv_uint64m4_tET_S0_S0_(<vscale x 4 x i64> %0, <vscale x 4 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i64> @llvm.riscv.vslidedown.nxv4i64.i64(<vscale x 4 x i64> poison, <vscale x 4 x i64> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i64> @llvm.riscv.vslideup.nxv4i64.i64(<vscale x 4 x i64> %5, <vscale x 4 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z16ConcatLowerLowerIu16__rvv_uint64m4_tET_S0_S0_(<vscale x 4 x i64> %0, <vscale x 4 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i64> @llvm.riscv.vslideup.nxv4i64.i64(<vscale x 4 x i64> %1, <vscale x 4 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i64> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z16ConcatUpperUpperIu16__rvv_uint64m4_tET_S0_S0_(<vscale x 4 x i64> %0, <vscale x 4 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i64> @llvm.riscv.vslidedown.nxv4i64.i64(<vscale x 4 x i64> poison, <vscale x 4 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i64> @llvm.riscv.vslidedown.nxv4i64.i64(<vscale x 4 x i64> poison, <vscale x 4 x i64> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i64> @llvm.riscv.vslideup.nxv4i64.i64(<vscale x 4 x i64> %6, <vscale x 4 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i64> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z16ConcatUpperLowerIu16__rvv_uint64m8_tET_S0_S0_(<vscale x 8 x i64> %0, <vscale x 8 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i64> @llvm.riscv.vslidedown.nxv8i64.i64(<vscale x 8 x i64> poison, <vscale x 8 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i64> @llvm.riscv.vslideup.nxv8i64.i64(<vscale x 8 x i64> %1, <vscale x 8 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z16ConcatLowerUpperIu16__rvv_uint64m8_tET_S0_S0_(<vscale x 8 x i64> %0, <vscale x 8 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i64> @llvm.riscv.vslidedown.nxv8i64.i64(<vscale x 8 x i64> poison, <vscale x 8 x i64> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i64> @llvm.riscv.vslideup.nxv8i64.i64(<vscale x 8 x i64> %5, <vscale x 8 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z16ConcatLowerLowerIu16__rvv_uint64m8_tET_S0_S0_(<vscale x 8 x i64> %0, <vscale x 8 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i64> @llvm.riscv.vslideup.nxv8i64.i64(<vscale x 8 x i64> %1, <vscale x 8 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i64> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z16ConcatUpperUpperIu16__rvv_uint64m8_tET_S0_S0_(<vscale x 8 x i64> %0, <vscale x 8 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i64> @llvm.riscv.vslidedown.nxv8i64.i64(<vscale x 8 x i64> poison, <vscale x 8 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i64> @llvm.riscv.vslidedown.nxv8i64.i64(<vscale x 8 x i64> poison, <vscale x 8 x i64> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i64> @llvm.riscv.vslideup.nxv8i64.i64(<vscale x 8 x i64> %6, <vscale x 8 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i64> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z16ConcatUpperLowerIu15__rvv_int8mf8_tET_S0_S0_(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vslidedown.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %1, <vscale x 1 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z16ConcatLowerUpperIu15__rvv_int8mf8_tET_S0_S0_(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vslidedown.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %5, <vscale x 1 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z16ConcatLowerLowerIu15__rvv_int8mf8_tET_S0_S0_(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %1, <vscale x 1 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i8> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z16ConcatUpperUpperIu15__rvv_int8mf8_tET_S0_S0_(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vslidedown.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vslidedown.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %6, <vscale x 1 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z16ConcatUpperLowerIu15__rvv_int8mf4_tET_S0_S0_(<vscale x 2 x i8> %0, <vscale x 2 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslidedown.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %1, <vscale x 2 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z16ConcatLowerUpperIu15__rvv_int8mf4_tET_S0_S0_(<vscale x 2 x i8> %0, <vscale x 2 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslidedown.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z16ConcatLowerLowerIu15__rvv_int8mf4_tET_S0_S0_(<vscale x 2 x i8> %0, <vscale x 2 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %1, <vscale x 2 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i8> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z16ConcatUpperUpperIu15__rvv_int8mf4_tET_S0_S0_(<vscale x 2 x i8> %0, <vscale x 2 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslidedown.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vslidedown.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z16ConcatUpperLowerIu15__rvv_int8mf2_tET_S0_S0_(<vscale x 4 x i8> %0, <vscale x 4 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslidedown.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %1, <vscale x 4 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z16ConcatLowerUpperIu15__rvv_int8mf2_tET_S0_S0_(<vscale x 4 x i8> %0, <vscale x 4 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslidedown.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z16ConcatLowerLowerIu15__rvv_int8mf2_tET_S0_S0_(<vscale x 4 x i8> %0, <vscale x 4 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %1, <vscale x 4 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i8> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z16ConcatUpperUpperIu15__rvv_int8mf2_tET_S0_S0_(<vscale x 4 x i8> %0, <vscale x 4 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslidedown.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslidedown.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z16ConcatUpperLowerIu14__rvv_int8m1_tET_S0_S0_(<vscale x 8 x i8> %0, <vscale x 8 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %1, <vscale x 8 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z16ConcatLowerUpperIu14__rvv_int8m1_tET_S0_S0_(<vscale x 8 x i8> %0, <vscale x 8 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z16ConcatLowerLowerIu14__rvv_int8m1_tET_S0_S0_(<vscale x 8 x i8> %0, <vscale x 8 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %1, <vscale x 8 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i8> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z16ConcatUpperUpperIu14__rvv_int8m1_tET_S0_S0_(<vscale x 8 x i8> %0, <vscale x 8 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z16ConcatUpperLowerIu14__rvv_int8m2_tET_S0_S0_(<vscale x 16 x i8> %0, <vscale x 16 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslidedown.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %1, <vscale x 16 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z16ConcatLowerUpperIu14__rvv_int8m2_tET_S0_S0_(<vscale x 16 x i8> %0, <vscale x 16 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslidedown.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z16ConcatLowerLowerIu14__rvv_int8m2_tET_S0_S0_(<vscale x 16 x i8> %0, <vscale x 16 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %1, <vscale x 16 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i8> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z16ConcatUpperUpperIu14__rvv_int8m2_tET_S0_S0_(<vscale x 16 x i8> %0, <vscale x 16 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslidedown.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslidedown.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z16ConcatUpperLowerIu14__rvv_int8m4_tET_S0_S0_(<vscale x 32 x i8> %0, <vscale x 32 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslidedown.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %1, <vscale x 32 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z16ConcatLowerUpperIu14__rvv_int8m4_tET_S0_S0_(<vscale x 32 x i8> %0, <vscale x 32 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslidedown.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z16ConcatLowerLowerIu14__rvv_int8m4_tET_S0_S0_(<vscale x 32 x i8> %0, <vscale x 32 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %1, <vscale x 32 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i8> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z16ConcatUpperUpperIu14__rvv_int8m4_tET_S0_S0_(<vscale x 32 x i8> %0, <vscale x 32 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslidedown.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslidedown.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z16ConcatUpperLowerIu14__rvv_int8m8_tET_S0_S0_(<vscale x 64 x i8> %0, <vscale x 64 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslidedown.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %1, <vscale x 64 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 64 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z16ConcatLowerUpperIu14__rvv_int8m8_tET_S0_S0_(<vscale x 64 x i8> %0, <vscale x 64 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslidedown.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 64 x i8> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z16ConcatLowerLowerIu14__rvv_int8m8_tET_S0_S0_(<vscale x 64 x i8> %0, <vscale x 64 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %1, <vscale x 64 x i8> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 64 x i8> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z16ConcatUpperUpperIu14__rvv_int8m8_tET_S0_S0_(<vscale x 64 x i8> %0, <vscale x 64 x i8> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslidedown.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslidedown.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 64 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z16ConcatUpperLowerIu16__rvv_int16mf4_tET_S0_S0_(<vscale x 1 x i16> %0, <vscale x 1 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i16> @llvm.riscv.vslidedown.nxv1i16.i64(<vscale x 1 x i16> poison, <vscale x 1 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i16> @llvm.riscv.vslideup.nxv1i16.i64(<vscale x 1 x i16> %1, <vscale x 1 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z16ConcatLowerUpperIu16__rvv_int16mf4_tET_S0_S0_(<vscale x 1 x i16> %0, <vscale x 1 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i16> @llvm.riscv.vslidedown.nxv1i16.i64(<vscale x 1 x i16> poison, <vscale x 1 x i16> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i16> @llvm.riscv.vslideup.nxv1i16.i64(<vscale x 1 x i16> %5, <vscale x 1 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z16ConcatLowerLowerIu16__rvv_int16mf4_tET_S0_S0_(<vscale x 1 x i16> %0, <vscale x 1 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i16> @llvm.riscv.vslideup.nxv1i16.i64(<vscale x 1 x i16> %1, <vscale x 1 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i16> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z16ConcatUpperUpperIu16__rvv_int16mf4_tET_S0_S0_(<vscale x 1 x i16> %0, <vscale x 1 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 6)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i16> @llvm.riscv.vslidedown.nxv1i16.i64(<vscale x 1 x i16> poison, <vscale x 1 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i16> @llvm.riscv.vslidedown.nxv1i16.i64(<vscale x 1 x i16> poison, <vscale x 1 x i16> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 1 x i16> @llvm.riscv.vslideup.nxv1i16.i64(<vscale x 1 x i16> %6, <vscale x 1 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i16> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z16ConcatUpperLowerIu16__rvv_int16mf2_tET_S0_S0_(<vscale x 2 x i16> %0, <vscale x 2 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i16> @llvm.riscv.vslidedown.nxv2i16.i64(<vscale x 2 x i16> poison, <vscale x 2 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i16> @llvm.riscv.vslideup.nxv2i16.i64(<vscale x 2 x i16> %1, <vscale x 2 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z16ConcatLowerUpperIu16__rvv_int16mf2_tET_S0_S0_(<vscale x 2 x i16> %0, <vscale x 2 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i16> @llvm.riscv.vslidedown.nxv2i16.i64(<vscale x 2 x i16> poison, <vscale x 2 x i16> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i16> @llvm.riscv.vslideup.nxv2i16.i64(<vscale x 2 x i16> %5, <vscale x 2 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z16ConcatLowerLowerIu16__rvv_int16mf2_tET_S0_S0_(<vscale x 2 x i16> %0, <vscale x 2 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i16> @llvm.riscv.vslideup.nxv2i16.i64(<vscale x 2 x i16> %1, <vscale x 2 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i16> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z16ConcatUpperUpperIu16__rvv_int16mf2_tET_S0_S0_(<vscale x 2 x i16> %0, <vscale x 2 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i16> @llvm.riscv.vslidedown.nxv2i16.i64(<vscale x 2 x i16> poison, <vscale x 2 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i16> @llvm.riscv.vslidedown.nxv2i16.i64(<vscale x 2 x i16> poison, <vscale x 2 x i16> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i16> @llvm.riscv.vslideup.nxv2i16.i64(<vscale x 2 x i16> %6, <vscale x 2 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i16> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z16ConcatUpperLowerIu15__rvv_int16m1_tET_S0_S0_(<vscale x 4 x i16> %0, <vscale x 4 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i16> @llvm.riscv.vslidedown.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i16> @llvm.riscv.vslideup.nxv4i16.i64(<vscale x 4 x i16> %1, <vscale x 4 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z16ConcatLowerUpperIu15__rvv_int16m1_tET_S0_S0_(<vscale x 4 x i16> %0, <vscale x 4 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i16> @llvm.riscv.vslidedown.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i16> @llvm.riscv.vslideup.nxv4i16.i64(<vscale x 4 x i16> %5, <vscale x 4 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z16ConcatLowerLowerIu15__rvv_int16m1_tET_S0_S0_(<vscale x 4 x i16> %0, <vscale x 4 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i16> @llvm.riscv.vslideup.nxv4i16.i64(<vscale x 4 x i16> %1, <vscale x 4 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i16> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z16ConcatUpperUpperIu15__rvv_int16m1_tET_S0_S0_(<vscale x 4 x i16> %0, <vscale x 4 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i16> @llvm.riscv.vslidedown.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i16> @llvm.riscv.vslidedown.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i16> @llvm.riscv.vslideup.nxv4i16.i64(<vscale x 4 x i16> %6, <vscale x 4 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i16> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z16ConcatUpperLowerIu15__rvv_int16m2_tET_S0_S0_(<vscale x 8 x i16> %0, <vscale x 8 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i16> @llvm.riscv.vslidedown.nxv8i16.i64(<vscale x 8 x i16> poison, <vscale x 8 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i16> @llvm.riscv.vslideup.nxv8i16.i64(<vscale x 8 x i16> %1, <vscale x 8 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z16ConcatLowerUpperIu15__rvv_int16m2_tET_S0_S0_(<vscale x 8 x i16> %0, <vscale x 8 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i16> @llvm.riscv.vslidedown.nxv8i16.i64(<vscale x 8 x i16> poison, <vscale x 8 x i16> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i16> @llvm.riscv.vslideup.nxv8i16.i64(<vscale x 8 x i16> %5, <vscale x 8 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z16ConcatLowerLowerIu15__rvv_int16m2_tET_S0_S0_(<vscale x 8 x i16> %0, <vscale x 8 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i16> @llvm.riscv.vslideup.nxv8i16.i64(<vscale x 8 x i16> %1, <vscale x 8 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i16> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z16ConcatUpperUpperIu15__rvv_int16m2_tET_S0_S0_(<vscale x 8 x i16> %0, <vscale x 8 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i16> @llvm.riscv.vslidedown.nxv8i16.i64(<vscale x 8 x i16> poison, <vscale x 8 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i16> @llvm.riscv.vslidedown.nxv8i16.i64(<vscale x 8 x i16> poison, <vscale x 8 x i16> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i16> @llvm.riscv.vslideup.nxv8i16.i64(<vscale x 8 x i16> %6, <vscale x 8 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i16> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z16ConcatUpperLowerIu15__rvv_int16m4_tET_S0_S0_(<vscale x 16 x i16> %0, <vscale x 16 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i16> @llvm.riscv.vslidedown.nxv16i16.i64(<vscale x 16 x i16> poison, <vscale x 16 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i16> @llvm.riscv.vslideup.nxv16i16.i64(<vscale x 16 x i16> %1, <vscale x 16 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z16ConcatLowerUpperIu15__rvv_int16m4_tET_S0_S0_(<vscale x 16 x i16> %0, <vscale x 16 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i16> @llvm.riscv.vslidedown.nxv16i16.i64(<vscale x 16 x i16> poison, <vscale x 16 x i16> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i16> @llvm.riscv.vslideup.nxv16i16.i64(<vscale x 16 x i16> %5, <vscale x 16 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z16ConcatLowerLowerIu15__rvv_int16m4_tET_S0_S0_(<vscale x 16 x i16> %0, <vscale x 16 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i16> @llvm.riscv.vslideup.nxv16i16.i64(<vscale x 16 x i16> %1, <vscale x 16 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i16> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z16ConcatUpperUpperIu15__rvv_int16m4_tET_S0_S0_(<vscale x 16 x i16> %0, <vscale x 16 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i16> @llvm.riscv.vslidedown.nxv16i16.i64(<vscale x 16 x i16> poison, <vscale x 16 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i16> @llvm.riscv.vslidedown.nxv16i16.i64(<vscale x 16 x i16> poison, <vscale x 16 x i16> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i16> @llvm.riscv.vslideup.nxv16i16.i64(<vscale x 16 x i16> %6, <vscale x 16 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i16> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z16ConcatUpperLowerIu15__rvv_int16m8_tET_S0_S0_(<vscale x 32 x i16> %0, <vscale x 32 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i16> @llvm.riscv.vslidedown.nxv32i16.i64(<vscale x 32 x i16> poison, <vscale x 32 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 32 x i16> @llvm.riscv.vslideup.nxv32i16.i64(<vscale x 32 x i16> %1, <vscale x 32 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z16ConcatLowerUpperIu15__rvv_int16m8_tET_S0_S0_(<vscale x 32 x i16> %0, <vscale x 32 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i16> @llvm.riscv.vslidedown.nxv32i16.i64(<vscale x 32 x i16> poison, <vscale x 32 x i16> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 32 x i16> @llvm.riscv.vslideup.nxv32i16.i64(<vscale x 32 x i16> %5, <vscale x 32 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i16> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z16ConcatLowerLowerIu15__rvv_int16m8_tET_S0_S0_(<vscale x 32 x i16> %0, <vscale x 32 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i16> @llvm.riscv.vslideup.nxv32i16.i64(<vscale x 32 x i16> %1, <vscale x 32 x i16> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i16> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z16ConcatUpperUpperIu15__rvv_int16m8_tET_S0_S0_(<vscale x 32 x i16> %0, <vscale x 32 x i16> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 1, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 32 x i16> @llvm.riscv.vslidedown.nxv32i16.i64(<vscale x 32 x i16> poison, <vscale x 32 x i16> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 32 x i16> @llvm.riscv.vslidedown.nxv32i16.i64(<vscale x 32 x i16> poison, <vscale x 32 x i16> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i16> @llvm.riscv.vslideup.nxv32i16.i64(<vscale x 32 x i16> %6, <vscale x 32 x i16> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 32 x i16> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z16ConcatUpperLowerIu16__rvv_int32mf2_tET_S0_S0_(<vscale x 1 x i32> %0, <vscale x 1 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i32> @llvm.riscv.vslidedown.nxv1i32.i64(<vscale x 1 x i32> poison, <vscale x 1 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i32> @llvm.riscv.vslideup.nxv1i32.i64(<vscale x 1 x i32> %1, <vscale x 1 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z16ConcatLowerUpperIu16__rvv_int32mf2_tET_S0_S0_(<vscale x 1 x i32> %0, <vscale x 1 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i32> @llvm.riscv.vslidedown.nxv1i32.i64(<vscale x 1 x i32> poison, <vscale x 1 x i32> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i32> @llvm.riscv.vslideup.nxv1i32.i64(<vscale x 1 x i32> %5, <vscale x 1 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z16ConcatLowerLowerIu16__rvv_int32mf2_tET_S0_S0_(<vscale x 1 x i32> %0, <vscale x 1 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i32> @llvm.riscv.vslideup.nxv1i32.i64(<vscale x 1 x i32> %1, <vscale x 1 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i32> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z16ConcatUpperUpperIu16__rvv_int32mf2_tET_S0_S0_(<vscale x 1 x i32> %0, <vscale x 1 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 7)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i32> @llvm.riscv.vslidedown.nxv1i32.i64(<vscale x 1 x i32> poison, <vscale x 1 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i32> @llvm.riscv.vslidedown.nxv1i32.i64(<vscale x 1 x i32> poison, <vscale x 1 x i32> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 1 x i32> @llvm.riscv.vslideup.nxv1i32.i64(<vscale x 1 x i32> %6, <vscale x 1 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i32> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z16ConcatUpperLowerIu15__rvv_int32m1_tET_S0_S0_(<vscale x 2 x i32> %0, <vscale x 2 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i32> @llvm.riscv.vslidedown.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i32> @llvm.riscv.vslideup.nxv2i32.i64(<vscale x 2 x i32> %1, <vscale x 2 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z16ConcatLowerUpperIu15__rvv_int32m1_tET_S0_S0_(<vscale x 2 x i32> %0, <vscale x 2 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i32> @llvm.riscv.vslidedown.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i32> @llvm.riscv.vslideup.nxv2i32.i64(<vscale x 2 x i32> %5, <vscale x 2 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z16ConcatLowerLowerIu15__rvv_int32m1_tET_S0_S0_(<vscale x 2 x i32> %0, <vscale x 2 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i32> @llvm.riscv.vslideup.nxv2i32.i64(<vscale x 2 x i32> %1, <vscale x 2 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i32> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z16ConcatUpperUpperIu15__rvv_int32m1_tET_S0_S0_(<vscale x 2 x i32> %0, <vscale x 2 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i32> @llvm.riscv.vslidedown.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i32> @llvm.riscv.vslidedown.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i32> @llvm.riscv.vslideup.nxv2i32.i64(<vscale x 2 x i32> %6, <vscale x 2 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i32> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z16ConcatUpperLowerIu15__rvv_int32m2_tET_S0_S0_(<vscale x 4 x i32> %0, <vscale x 4 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i32> @llvm.riscv.vslidedown.nxv4i32.i64(<vscale x 4 x i32> poison, <vscale x 4 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i32> @llvm.riscv.vslideup.nxv4i32.i64(<vscale x 4 x i32> %1, <vscale x 4 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z16ConcatLowerUpperIu15__rvv_int32m2_tET_S0_S0_(<vscale x 4 x i32> %0, <vscale x 4 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i32> @llvm.riscv.vslidedown.nxv4i32.i64(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i32> @llvm.riscv.vslideup.nxv4i32.i64(<vscale x 4 x i32> %5, <vscale x 4 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z16ConcatLowerLowerIu15__rvv_int32m2_tET_S0_S0_(<vscale x 4 x i32> %0, <vscale x 4 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i32> @llvm.riscv.vslideup.nxv4i32.i64(<vscale x 4 x i32> %1, <vscale x 4 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i32> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z16ConcatUpperUpperIu15__rvv_int32m2_tET_S0_S0_(<vscale x 4 x i32> %0, <vscale x 4 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i32> @llvm.riscv.vslidedown.nxv4i32.i64(<vscale x 4 x i32> poison, <vscale x 4 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i32> @llvm.riscv.vslidedown.nxv4i32.i64(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i32> @llvm.riscv.vslideup.nxv4i32.i64(<vscale x 4 x i32> %6, <vscale x 4 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i32> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z16ConcatUpperLowerIu15__rvv_int32m4_tET_S0_S0_(<vscale x 8 x i32> %0, <vscale x 8 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i32> @llvm.riscv.vslidedown.nxv8i32.i64(<vscale x 8 x i32> poison, <vscale x 8 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i32> @llvm.riscv.vslideup.nxv8i32.i64(<vscale x 8 x i32> %1, <vscale x 8 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z16ConcatLowerUpperIu15__rvv_int32m4_tET_S0_S0_(<vscale x 8 x i32> %0, <vscale x 8 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i32> @llvm.riscv.vslidedown.nxv8i32.i64(<vscale x 8 x i32> poison, <vscale x 8 x i32> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i32> @llvm.riscv.vslideup.nxv8i32.i64(<vscale x 8 x i32> %5, <vscale x 8 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z16ConcatLowerLowerIu15__rvv_int32m4_tET_S0_S0_(<vscale x 8 x i32> %0, <vscale x 8 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i32> @llvm.riscv.vslideup.nxv8i32.i64(<vscale x 8 x i32> %1, <vscale x 8 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i32> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z16ConcatUpperUpperIu15__rvv_int32m4_tET_S0_S0_(<vscale x 8 x i32> %0, <vscale x 8 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i32> @llvm.riscv.vslidedown.nxv8i32.i64(<vscale x 8 x i32> poison, <vscale x 8 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i32> @llvm.riscv.vslidedown.nxv8i32.i64(<vscale x 8 x i32> poison, <vscale x 8 x i32> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i32> @llvm.riscv.vslideup.nxv8i32.i64(<vscale x 8 x i32> %6, <vscale x 8 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i32> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z16ConcatUpperLowerIu15__rvv_int32m8_tET_S0_S0_(<vscale x 16 x i32> %0, <vscale x 16 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i32> @llvm.riscv.vslidedown.nxv16i32.i64(<vscale x 16 x i32> poison, <vscale x 16 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i32> @llvm.riscv.vslideup.nxv16i32.i64(<vscale x 16 x i32> %1, <vscale x 16 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z16ConcatLowerUpperIu15__rvv_int32m8_tET_S0_S0_(<vscale x 16 x i32> %0, <vscale x 16 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i32> @llvm.riscv.vslidedown.nxv16i32.i64(<vscale x 16 x i32> poison, <vscale x 16 x i32> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i32> @llvm.riscv.vslideup.nxv16i32.i64(<vscale x 16 x i32> %5, <vscale x 16 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i32> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z16ConcatLowerLowerIu15__rvv_int32m8_tET_S0_S0_(<vscale x 16 x i32> %0, <vscale x 16 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i32> @llvm.riscv.vslideup.nxv16i32.i64(<vscale x 16 x i32> %1, <vscale x 16 x i32> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i32> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z16ConcatUpperUpperIu15__rvv_int32m8_tET_S0_S0_(<vscale x 16 x i32> %0, <vscale x 16 x i32> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 2, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 16 x i32> @llvm.riscv.vslidedown.nxv16i32.i64(<vscale x 16 x i32> poison, <vscale x 16 x i32> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 16 x i32> @llvm.riscv.vslidedown.nxv16i32.i64(<vscale x 16 x i32> poison, <vscale x 16 x i32> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i32> @llvm.riscv.vslideup.nxv16i32.i64(<vscale x 16 x i32> %6, <vscale x 16 x i32> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 16 x i32> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z16ConcatUpperLowerIu15__rvv_int64m1_tET_S0_S0_(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i64> @llvm.riscv.vslidedown.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i64> @llvm.riscv.vslideup.nxv1i64.i64(<vscale x 1 x i64> %1, <vscale x 1 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z16ConcatLowerUpperIu15__rvv_int64m1_tET_S0_S0_(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i64> @llvm.riscv.vslidedown.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i64> @llvm.riscv.vslideup.nxv1i64.i64(<vscale x 1 x i64> %5, <vscale x 1 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z16ConcatLowerLowerIu15__rvv_int64m1_tET_S0_S0_(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i64> @llvm.riscv.vslideup.nxv1i64.i64(<vscale x 1 x i64> %1, <vscale x 1 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i64> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z16ConcatUpperUpperIu15__rvv_int64m1_tET_S0_S0_(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 0)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 1 x i64> @llvm.riscv.vslidedown.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 1 x i64> @llvm.riscv.vslidedown.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 1 x i64> @llvm.riscv.vslideup.nxv1i64.i64(<vscale x 1 x i64> %6, <vscale x 1 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 1 x i64> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z16ConcatUpperLowerIu15__rvv_int64m2_tET_S0_S0_(<vscale x 2 x i64> %0, <vscale x 2 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i64> @llvm.riscv.vslidedown.nxv2i64.i64(<vscale x 2 x i64> poison, <vscale x 2 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i64> @llvm.riscv.vslideup.nxv2i64.i64(<vscale x 2 x i64> %1, <vscale x 2 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z16ConcatLowerUpperIu15__rvv_int64m2_tET_S0_S0_(<vscale x 2 x i64> %0, <vscale x 2 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i64> @llvm.riscv.vslidedown.nxv2i64.i64(<vscale x 2 x i64> poison, <vscale x 2 x i64> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i64> @llvm.riscv.vslideup.nxv2i64.i64(<vscale x 2 x i64> %5, <vscale x 2 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z16ConcatLowerLowerIu15__rvv_int64m2_tET_S0_S0_(<vscale x 2 x i64> %0, <vscale x 2 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i64> @llvm.riscv.vslideup.nxv2i64.i64(<vscale x 2 x i64> %1, <vscale x 2 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i64> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z16ConcatUpperUpperIu15__rvv_int64m2_tET_S0_S0_(<vscale x 2 x i64> %0, <vscale x 2 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 1)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 2 x i64> @llvm.riscv.vslidedown.nxv2i64.i64(<vscale x 2 x i64> poison, <vscale x 2 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 2 x i64> @llvm.riscv.vslidedown.nxv2i64.i64(<vscale x 2 x i64> poison, <vscale x 2 x i64> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i64> @llvm.riscv.vslideup.nxv2i64.i64(<vscale x 2 x i64> %6, <vscale x 2 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 2 x i64> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z16ConcatUpperLowerIu15__rvv_int64m4_tET_S0_S0_(<vscale x 4 x i64> %0, <vscale x 4 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i64> @llvm.riscv.vslidedown.nxv4i64.i64(<vscale x 4 x i64> poison, <vscale x 4 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i64> @llvm.riscv.vslideup.nxv4i64.i64(<vscale x 4 x i64> %1, <vscale x 4 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z16ConcatLowerUpperIu15__rvv_int64m4_tET_S0_S0_(<vscale x 4 x i64> %0, <vscale x 4 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i64> @llvm.riscv.vslidedown.nxv4i64.i64(<vscale x 4 x i64> poison, <vscale x 4 x i64> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i64> @llvm.riscv.vslideup.nxv4i64.i64(<vscale x 4 x i64> %5, <vscale x 4 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z16ConcatLowerLowerIu15__rvv_int64m4_tET_S0_S0_(<vscale x 4 x i64> %0, <vscale x 4 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i64> @llvm.riscv.vslideup.nxv4i64.i64(<vscale x 4 x i64> %1, <vscale x 4 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i64> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z16ConcatUpperUpperIu15__rvv_int64m4_tET_S0_S0_(<vscale x 4 x i64> %0, <vscale x 4 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 2)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 4 x i64> @llvm.riscv.vslidedown.nxv4i64.i64(<vscale x 4 x i64> poison, <vscale x 4 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 4 x i64> @llvm.riscv.vslidedown.nxv4i64.i64(<vscale x 4 x i64> poison, <vscale x 4 x i64> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i64> @llvm.riscv.vslideup.nxv4i64.i64(<vscale x 4 x i64> %6, <vscale x 4 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 4 x i64> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z16ConcatUpperLowerIu15__rvv_int64m8_tET_S0_S0_(<vscale x 8 x i64> %0, <vscale x 8 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i64> @llvm.riscv.vslidedown.nxv8i64.i64(<vscale x 8 x i64> poison, <vscale x 8 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i64> @llvm.riscv.vslideup.nxv8i64.i64(<vscale x 8 x i64> %1, <vscale x 8 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z16ConcatLowerUpperIu15__rvv_int64m8_tET_S0_S0_(<vscale x 8 x i64> %0, <vscale x 8 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i64> @llvm.riscv.vslidedown.nxv8i64.i64(<vscale x 8 x i64> poison, <vscale x 8 x i64> %1, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i64> @llvm.riscv.vslideup.nxv8i64.i64(<vscale x 8 x i64> %5, <vscale x 8 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i64> %6
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z16ConcatLowerLowerIu15__rvv_int64m8_tET_S0_S0_(<vscale x 8 x i64> %0, <vscale x 8 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i64> @llvm.riscv.vslideup.nxv8i64.i64(<vscale x 8 x i64> %1, <vscale x 8 x i64> %0, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i64> %5
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z16ConcatUpperUpperIu15__rvv_int64m8_tET_S0_S0_(<vscale x 8 x i64> %0, <vscale x 8 x i64> %1) local_unnamed_addr #0 comdat {
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 3, i64 3)
  %4 = lshr i64 %3, 1
  %5 = tail call <vscale x 8 x i64> @llvm.riscv.vslidedown.nxv8i64.i64(<vscale x 8 x i64> poison, <vscale x 8 x i64> %0, i64 %4, i64 %3, i64 3)
  %6 = tail call <vscale x 8 x i64> @llvm.riscv.vslidedown.nxv8i64.i64(<vscale x 8 x i64> poison, <vscale x 8 x i64> %1, i64 %4, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i64> @llvm.riscv.vslideup.nxv8i64.i64(<vscale x 8 x i64> %6, <vscale x 8 x i64> %5, i64 %4, i64 %3, i64 3)
  ret <vscale x 8 x i64> %7
}

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare i64 @llvm.riscv.vsetvlimax.i64(i64 immarg, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 1 x i8> @llvm.riscv.vslidedown.nxv1i8.i64(<vscale x 1 x i8>, <vscale x 1 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8>, <vscale x 1 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 2 x i8> @llvm.riscv.vslidedown.nxv2i8.i64(<vscale x 2 x i8>, <vscale x 2 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8>, <vscale x 2 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 4 x i8> @llvm.riscv.vslidedown.nxv4i8.i64(<vscale x 4 x i8>, <vscale x 4 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8>, <vscale x 4 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8>, <vscale x 8 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8>, <vscale x 8 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 16 x i8> @llvm.riscv.vslidedown.nxv16i8.i64(<vscale x 16 x i8>, <vscale x 16 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8>, <vscale x 16 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 32 x i8> @llvm.riscv.vslidedown.nxv32i8.i64(<vscale x 32 x i8>, <vscale x 32 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8>, <vscale x 32 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 64 x i8> @llvm.riscv.vslidedown.nxv64i8.i64(<vscale x 64 x i8>, <vscale x 64 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8>, <vscale x 64 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 1 x i16> @llvm.riscv.vslidedown.nxv1i16.i64(<vscale x 1 x i16>, <vscale x 1 x i16>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 1 x i16> @llvm.riscv.vslideup.nxv1i16.i64(<vscale x 1 x i16>, <vscale x 1 x i16>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 2 x i16> @llvm.riscv.vslidedown.nxv2i16.i64(<vscale x 2 x i16>, <vscale x 2 x i16>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 2 x i16> @llvm.riscv.vslideup.nxv2i16.i64(<vscale x 2 x i16>, <vscale x 2 x i16>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 4 x i16> @llvm.riscv.vslidedown.nxv4i16.i64(<vscale x 4 x i16>, <vscale x 4 x i16>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 4 x i16> @llvm.riscv.vslideup.nxv4i16.i64(<vscale x 4 x i16>, <vscale x 4 x i16>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 8 x i16> @llvm.riscv.vslidedown.nxv8i16.i64(<vscale x 8 x i16>, <vscale x 8 x i16>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 8 x i16> @llvm.riscv.vslideup.nxv8i16.i64(<vscale x 8 x i16>, <vscale x 8 x i16>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 16 x i16> @llvm.riscv.vslidedown.nxv16i16.i64(<vscale x 16 x i16>, <vscale x 16 x i16>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 16 x i16> @llvm.riscv.vslideup.nxv16i16.i64(<vscale x 16 x i16>, <vscale x 16 x i16>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 32 x i16> @llvm.riscv.vslidedown.nxv32i16.i64(<vscale x 32 x i16>, <vscale x 32 x i16>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 32 x i16> @llvm.riscv.vslideup.nxv32i16.i64(<vscale x 32 x i16>, <vscale x 32 x i16>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 1 x i32> @llvm.riscv.vslidedown.nxv1i32.i64(<vscale x 1 x i32>, <vscale x 1 x i32>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 1 x i32> @llvm.riscv.vslideup.nxv1i32.i64(<vscale x 1 x i32>, <vscale x 1 x i32>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 2 x i32> @llvm.riscv.vslidedown.nxv2i32.i64(<vscale x 2 x i32>, <vscale x 2 x i32>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 2 x i32> @llvm.riscv.vslideup.nxv2i32.i64(<vscale x 2 x i32>, <vscale x 2 x i32>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 4 x i32> @llvm.riscv.vslidedown.nxv4i32.i64(<vscale x 4 x i32>, <vscale x 4 x i32>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 4 x i32> @llvm.riscv.vslideup.nxv4i32.i64(<vscale x 4 x i32>, <vscale x 4 x i32>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 8 x i32> @llvm.riscv.vslidedown.nxv8i32.i64(<vscale x 8 x i32>, <vscale x 8 x i32>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 8 x i32> @llvm.riscv.vslideup.nxv8i32.i64(<vscale x 8 x i32>, <vscale x 8 x i32>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 16 x i32> @llvm.riscv.vslidedown.nxv16i32.i64(<vscale x 16 x i32>, <vscale x 16 x i32>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 16 x i32> @llvm.riscv.vslideup.nxv16i32.i64(<vscale x 16 x i32>, <vscale x 16 x i32>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 1 x i64> @llvm.riscv.vslidedown.nxv1i64.i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 1 x i64> @llvm.riscv.vslideup.nxv1i64.i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 2 x i64> @llvm.riscv.vslidedown.nxv2i64.i64(<vscale x 2 x i64>, <vscale x 2 x i64>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 2 x i64> @llvm.riscv.vslideup.nxv2i64.i64(<vscale x 2 x i64>, <vscale x 2 x i64>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 4 x i64> @llvm.riscv.vslidedown.nxv4i64.i64(<vscale x 4 x i64>, <vscale x 4 x i64>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 4 x i64> @llvm.riscv.vslideup.nxv4i64.i64(<vscale x 4 x i64>, <vscale x 4 x i64>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 8 x i64> @llvm.riscv.vslidedown.nxv8i64.i64(<vscale x 8 x i64>, <vscale x 8 x i64>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 8 x i64> @llvm.riscv.vslideup.nxv8i64.i64(<vscale x 8 x i64>, <vscale x 8 x i64>, i64, i64, i64 immarg) #1

attributes #0 = { mustprogress uwtable vscale_range(2,1024) "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="sifive-x280" "target-features"="+64bit,+a,+c,+d,+f,+m,+relax,+v,+zicsr,+zifencei,+zmmul,+zve32f,+zve32x,+zve64d,+zve64f,+zve64x,+zvl128b,+zvl32b,+zvl64b,-b,-e,-experimental-smmpm,-experimental-smnpm,-experimental-ssnpm,-experimental-sspm,-experimental-ssqosid,-experimental-supm,-experimental-zacas,-experimental-zalasr,-experimental-zicfilp,-experimental-zicfiss,-h,-shcounterenw,-shgatpa,-shtvala,-shvsatpa,-shvstvala,-shvstvecd,-smaia,-smcdeleg,-smcsrind,-smepmp,-smstateen,-ssaia,-ssccfg,-ssccptr,-sscofpmf,-sscounterenw,-sscsrind,-ssstateen,-ssstrict,-sstc,-sstvala,-sstvecd,-ssu64xl,-svade,-svadu,-svbare,-svinval,-svnapot,-svpbmt,-xcvalu,-xcvbi,-xcvbitmanip,-xcvelw,-xcvmac,-xcvmem,-xcvsimd,-xsfcease,-xsfvcp,-xsfvfnrclipxfqf,-xsfvfwmaccqqq,-xsfvqmaccdod,-xsfvqmaccqoq,-xsifivecdiscarddlone,-xsifivecflushdlone,-xtheadba,-xtheadbb,-xtheadbs,-xtheadcmo,-xtheadcondmov,-xtheadfmemidx,-xtheadmac,-xtheadmemidx,-xtheadmempair,-xtheadsync,-xtheadvdot,-xventanacondops,-xwchc,-za128rs,-za64rs,-zaamo,-zabha,-zalrsc,-zama16b,-zawrs,-zba,-zbb,-zbc,-zbkb,-zbkc,-zbkx,-zbs,-zca,-zcb,-zcd,-zce,-zcf,-zcmop,-zcmp,-zcmt,-zdinx,-zfa,-zfbfmin,-zfh,-zfhmin,-zfinx,-zhinx,-zhinxmin,-zic64b,-zicbom,-zicbop,-zicboz,-ziccamoa,-ziccif,-zicclsm,-ziccrse,-zicntr,-zicond,-zihintntl,-zihintpause,-zihpm,-zimop,-zk,-zkn,-zknd,-zkne,-zknh,-zkr,-zks,-zksed,-zksh,-zkt,-ztso,-zvbb,-zvbc,-zvfbfmin,-zvfbfwma,-zvfh,-zvfhmin,-zvkb,-zvkg,-zvkn,-zvknc,-zvkned,-zvkng,-zvknha,-zvknhb,-zvks,-zvksc,-zvksed,-zvksg,-zvksh,-zvkt,-zvl1024b,-zvl16384b,-zvl2048b,-zvl256b,-zvl32768b,-zvl4096b,-zvl512b,-zvl65536b,-zvl8192b" }
attributes #1 = { mustprogress nocallback nofree nosync nounwind willreturn  }

!llvm.module.flags = !{!0, !1, !2, !4, !5, !6, !7}
!llvm.ident = !{!8}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 1, !"target-abi", !"lp64d"}
!2 = !{i32 6, !"riscv-isa", !3}
!3 = !{!"rv64i2p1_m2p0_a2p1_f2p2_d2p2_c2p0_v1p0_zicsr2p0_zifencei2p0_zmmul1p0_zve32f1p0_zve32x1p0_zve64d1p0_zve64f1p0_zve64x1p0_zvl128b1p0_zvl32b1p0_zvl64b1p0"}
!4 = !{i32 8, !"PIC Level", i32 2}
!5 = !{i32 7, !"PIE Level", i32 2}
!6 = !{i32 7, !"uwtable", i32 2}
!7 = !{i32 8, !"SmallDataLimit", i32 8}
!8 = !{!"clang version 19.1.0-rc2"}
