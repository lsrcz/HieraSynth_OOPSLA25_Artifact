def _Z7MulEvenIu17__rvv_uint16mf4_tu16__rvv_uint8mf8_tET_T0_S1_(v0: vec<ef8mf8>, v1: vec<ef8mf8>) -> vec<ef4mf4>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[ef8mf8, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef8mf8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef8mf8, ud, fm](v2, v6, v9)
  (v11) = vlenb()
  (v12) = scalar[xmul=1, imm=0x0000000000000004]()
  (v13) = srl.xx[width=1](v11, v12)
  (v14) = scalar[xmul=1, imm=0x0000000000000200]()
  (v15) = minu.xx[width=1](v13, v14)
  (v18) = vsetvl[mmul=1, tama](v15)
  (v19) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef4mf4, ud](v18, v19)
  (v20) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v16)
  (v21) = vget[part=ef8mf8, src=ef8mf4, idx=0](v20)
  (v23) = vsetvlrelay[mmul=1, src_mmul=1, tama](v2)
  (v22) = vmsne.vx[ef8mf8, ud, fm](v23, v21, v9)
  (v24) = vmerge.vvm[ef8mf8, ud](v23, v8, v3, v22)
  (v26) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v24)
  (v28) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v26)
  return (v28)

def _Z6MulOddIu17__rvv_uint16mf4_tu16__rvv_uint8mf8_tET_T0_S1_(v0: vec<ef8mf8>, v1: vec<ef8mf8>) -> vec<ef4mf4>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[ef8mf8, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef8mf8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef8mf8, ud, fm](v2, v3, v9)
  (v11) = vlenb()
  (v12) = scalar[xmul=1, imm=0x0000000000000004]()
  (v13) = srl.xx[width=1](v11, v12)
  (v14) = scalar[xmul=1, imm=0x0000000000000200]()
  (v15) = minu.xx[width=1](v13, v14)
  (v18) = vsetvl[mmul=1, tama](v15)
  (v19) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef4mf4, ud](v18, v19)
  (v20) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v16)
  (v21) = vget[part=ef8mf8, src=ef8mf4, idx=0](v20)
  (v23) = vsetvlrelay[mmul=1, src_mmul=1, tama](v2)
  (v22) = vmsne.vx[ef8mf8, ud, fm](v23, v21, v9)
  (v24) = vmerge.vvm[ef8mf8, ud](v23, v6, v8, v22)
  (v26) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v24)
  (v28) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v26)
  return (v28)

def _Z7MulEvenIu17__rvv_uint16mf4_tu16__rvv_uint8mf4_tET_T0_S1_(v0: vec<ef8mf4>, v1: vec<ef8mf4>) -> vec<ef4mf4>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[ef8mf4, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef8mf4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef8mf4, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4mf4, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v12)
  (v18) = vsetvlrelay[mmul=2, src_mmul=2, tama](v2)
  (v17) = vmsne.vx[ef8mf4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8mf4, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z6MulOddIu17__rvv_uint16mf4_tu16__rvv_uint8mf4_tET_T0_S1_(v0: vec<ef8mf4>, v1: vec<ef8mf4>) -> vec<ef4mf4>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[ef8mf4, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef8mf4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef8mf4, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4mf4, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v12)
  (v18) = vsetvlrelay[mmul=2, src_mmul=2, tama](v2)
  (v17) = vmsne.vx[ef8mf4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8mf4, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z7MulEvenIu17__rvv_uint16mf2_tu16__rvv_uint8mf2_tET_T0_S1_(v0: vec<ef8mf2>, v1: vec<ef8mf2>) -> vec<ef4mf2>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[ef8mf2, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef8mf2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef8mf2, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4mf2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v12)
  (v18) = vsetvlrelay[mmul=4, src_mmul=4, tama](v2)
  (v17) = vmsne.vx[ef8mf2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8mf2, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z6MulOddIu17__rvv_uint16mf2_tu16__rvv_uint8mf2_tET_T0_S1_(v0: vec<ef8mf2>, v1: vec<ef8mf2>) -> vec<ef4mf2>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[ef8mf2, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef8mf2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef8mf2, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4mf2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v12)
  (v18) = vsetvlrelay[mmul=4, src_mmul=4, tama](v2)
  (v17) = vmsne.vx[ef8mf2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8mf2, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_uint16m1_tu15__rvv_uint8m1_tET_T0_S1_(v0: vec<ef8m1>, v1: vec<ef8m1>) -> vec<ef4m1>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[ef8m1, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef8m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef8m1, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=4, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m1, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m1, dest=ef8m1](v12)
  (v18) = vsetvlrelay[mmul=8, src_mmul=8, tama](v2)
  (v17) = vmsne.vx[ef8m1, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m1, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z6MulOddIu16__rvv_uint16m1_tu15__rvv_uint8m1_tET_T0_S1_(v0: vec<ef8m1>, v1: vec<ef8m1>) -> vec<ef4m1>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[ef8m1, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef8m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef8m1, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=4, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m1, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m1, dest=ef8m1](v12)
  (v18) = vsetvlrelay[mmul=8, src_mmul=8, tama](v2)
  (v17) = vmsne.vx[ef8m1, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m1, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_uint16m2_tu15__rvv_uint8m2_tET_T0_S1_(v0: vec<ef8m2>, v1: vec<ef8m2>) -> vec<ef4m2>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vmul.vv[ef8m2, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef8m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef8m2, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=8, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m2, dest=ef8m2](v12)
  (v18) = vsetvlrelay[mmul=16, src_mmul=16, tama](v2)
  (v17) = vmsne.vx[ef8m2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m2, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef4m2](v19)
  return (v21)

def _Z6MulOddIu16__rvv_uint16m2_tu15__rvv_uint8m2_tET_T0_S1_(v0: vec<ef8m2>, v1: vec<ef8m2>) -> vec<ef4m2>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vmul.vv[ef8m2, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef8m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef8m2, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=8, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m2, dest=ef8m2](v12)
  (v18) = vsetvlrelay[mmul=16, src_mmul=16, tama](v2)
  (v17) = vmsne.vx[ef8m2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m2, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef4m2](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_uint16m4_tu15__rvv_uint8m4_tET_T0_S1_(v0: vec<ef8m4>, v1: vec<ef8m4>) -> vec<ef4m4>:
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vmul.vv[ef8m4, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef8m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef8m4, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=16, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m4, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m4, dest=ef8m4](v12)
  (v18) = vsetvlrelay[mmul=32, src_mmul=32, tama](v2)
  (v17) = vmsne.vx[ef8m4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m4, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z6MulOddIu16__rvv_uint16m4_tu15__rvv_uint8m4_tET_T0_S1_(v0: vec<ef8m4>, v1: vec<ef8m4>) -> vec<ef4m4>:
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vmul.vv[ef8m4, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef8m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef8m4, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=16, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m4, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m4, dest=ef8m4](v12)
  (v18) = vsetvlrelay[mmul=32, src_mmul=32, tama](v2)
  (v17) = vmsne.vx[ef8m4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m4, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_uint16m8_tu15__rvv_uint8m8_tET_T0_S1_(v0: vec<ef8m8>, v1: vec<ef8m8>) -> vec<ef4m8>:
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vmul.vv[ef8m8, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef8m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef8m8, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m8, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m8, dest=ef8m8](v12)
  (v18) = vsetvlrelay[mmul=64, src_mmul=64, tama](v2)
  (v17) = vmsne.vx[ef8m8, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m8, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z6MulOddIu16__rvv_uint16m8_tu15__rvv_uint8m8_tET_T0_S1_(v0: vec<ef8m8>, v1: vec<ef8m8>) -> vec<ef4m8>:
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vmul.vv[ef8m8, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef8m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef8m8, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m8, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m8, dest=ef8m8](v12)
  (v18) = vsetvlrelay[mmul=64, src_mmul=64, tama](v2)
  (v17) = vmsne.vx[ef8m8, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m8, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z7MulEvenIu17__rvv_uint32mf2_tu17__rvv_uint16mf4_tET_T0_S1_(v0: vec<ef4mf4>, v1: vec<ef4mf4>) -> vec<ef2mf2>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[ef4mf4, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef4mf4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef4mf4, ud, fm](v2, v6, v9)
  (v11) = vlenb()
  (v12) = scalar[xmul=1, imm=0x0000000000000004]()
  (v13) = srl.xx[width=1](v11, v12)
  (v14) = scalar[xmul=1, imm=0x0000000000000200]()
  (v15) = minu.xx[width=1](v13, v14)
  (v18) = vsetvl[mmul=1, tama](v15)
  (v19) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef2mf2, ud](v18, v19)
  (v20) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v16)
  (v21) = vget[part=ef8mf4, src=ef8mf2, idx=0](v20)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  (v24) = vsetvlrelay[mmul=1, src_mmul=1, tama](v2)
  (v23) = vmsne.vx[ef4mf4, ud, fm](v24, v22, v9)
  (v25) = vmerge.vvm[ef4mf4, ud](v24, v8, v3, v23)
  (v27) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v25)
  (v28) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v27)
  (v30) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v28)
  return (v30)

def _Z6MulOddIu17__rvv_uint32mf2_tu17__rvv_uint16mf4_tET_T0_S1_(v0: vec<ef4mf4>, v1: vec<ef4mf4>) -> vec<ef2mf2>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[ef4mf4, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef4mf4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef4mf4, ud, fm](v2, v3, v9)
  (v11) = vlenb()
  (v12) = scalar[xmul=1, imm=0x0000000000000004]()
  (v13) = srl.xx[width=1](v11, v12)
  (v14) = scalar[xmul=1, imm=0x0000000000000200]()
  (v15) = minu.xx[width=1](v13, v14)
  (v18) = vsetvl[mmul=1, tama](v15)
  (v19) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef2mf2, ud](v18, v19)
  (v20) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v16)
  (v21) = vget[part=ef8mf4, src=ef8mf2, idx=0](v20)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  (v24) = vsetvlrelay[mmul=1, src_mmul=1, tama](v2)
  (v23) = vmsne.vx[ef4mf4, ud, fm](v24, v22, v9)
  (v25) = vmerge.vvm[ef4mf4, ud](v24, v6, v8, v23)
  (v27) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v25)
  (v28) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v27)
  (v30) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v28)
  return (v30)

def _Z7MulEvenIu17__rvv_uint32mf2_tu17__rvv_uint16mf2_tET_T0_S1_(v0: vec<ef4mf2>, v1: vec<ef4mf2>) -> vec<ef2mf2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[ef4mf2, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef4mf2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef4mf2, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2mf2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2mf2, dest=ef4mf2](v12)
  (v18) = vsetvlrelay[mmul=2, src_mmul=2, tama](v2)
  (v17) = vmsne.vx[ef4mf2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4mf2, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef4mf2, dest=ef2mf2](v19)
  return (v21)

def _Z6MulOddIu17__rvv_uint32mf2_tu17__rvv_uint16mf2_tET_T0_S1_(v0: vec<ef4mf2>, v1: vec<ef4mf2>) -> vec<ef2mf2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[ef4mf2, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef4mf2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef4mf2, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2mf2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2mf2, dest=ef4mf2](v12)
  (v18) = vsetvlrelay[mmul=2, src_mmul=2, tama](v2)
  (v17) = vmsne.vx[ef4mf2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4mf2, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef4mf2, dest=ef2mf2](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_uint32m1_tu16__rvv_uint16m1_tET_T0_S1_(v0: vec<ef4m1>, v1: vec<ef4m1>) -> vec<ef2m1>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[ef4m1, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef4m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef4m1, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m1, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m1, dest=ef4m1](v12)
  (v18) = vsetvlrelay[mmul=4, src_mmul=4, tama](v2)
  (v17) = vmsne.vx[ef4m1, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m1, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef4m1, dest=ef2m1](v19)
  return (v21)

def _Z6MulOddIu16__rvv_uint32m1_tu16__rvv_uint16m1_tET_T0_S1_(v0: vec<ef4m1>, v1: vec<ef4m1>) -> vec<ef2m1>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[ef4m1, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef4m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef4m1, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m1, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m1, dest=ef4m1](v12)
  (v18) = vsetvlrelay[mmul=4, src_mmul=4, tama](v2)
  (v17) = vmsne.vx[ef4m1, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m1, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef4m1, dest=ef2m1](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_uint32m2_tu16__rvv_uint16m2_tET_T0_S1_(v0: vec<ef4m2>, v1: vec<ef4m2>) -> vec<ef2m2>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[ef4m2, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef4m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef4m2, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=4, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m2, dest=ef4m2](v12)
  (v18) = vsetvlrelay[mmul=8, src_mmul=8, tama](v2)
  (v17) = vmsne.vx[ef4m2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m2, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef4m2, dest=ef2m2](v19)
  return (v21)

def _Z6MulOddIu16__rvv_uint32m2_tu16__rvv_uint16m2_tET_T0_S1_(v0: vec<ef4m2>, v1: vec<ef4m2>) -> vec<ef2m2>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[ef4m2, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef4m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef4m2, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=4, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m2, dest=ef4m2](v12)
  (v18) = vsetvlrelay[mmul=8, src_mmul=8, tama](v2)
  (v17) = vmsne.vx[ef4m2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m2, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef4m2, dest=ef2m2](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_uint32m4_tu16__rvv_uint16m4_tET_T0_S1_(v0: vec<ef4m4>, v1: vec<ef4m4>) -> vec<ef2m4>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vmul.vv[ef4m4, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef4m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef4m4, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=8, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m4, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m4, dest=ef4m4](v12)
  (v18) = vsetvlrelay[mmul=16, src_mmul=16, tama](v2)
  (v17) = vmsne.vx[ef4m4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m4, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef4m4, dest=ef2m4](v19)
  return (v21)

def _Z6MulOddIu16__rvv_uint32m4_tu16__rvv_uint16m4_tET_T0_S1_(v0: vec<ef4m4>, v1: vec<ef4m4>) -> vec<ef2m4>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vmul.vv[ef4m4, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef4m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef4m4, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=8, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m4, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m4, dest=ef4m4](v12)
  (v18) = vsetvlrelay[mmul=16, src_mmul=16, tama](v2)
  (v17) = vmsne.vx[ef4m4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m4, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef4m4, dest=ef2m4](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_uint32m8_tu16__rvv_uint16m8_tET_T0_S1_(v0: vec<ef4m8>, v1: vec<ef4m8>) -> vec<ef2m8>:
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vmul.vv[ef4m8, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef4m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef4m8, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=16, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m8, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m8, dest=ef4m8](v12)
  (v18) = vsetvlrelay[mmul=32, src_mmul=32, tama](v2)
  (v17) = vmsne.vx[ef4m8, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m8, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef4m8, dest=ef2m8](v19)
  return (v21)

def _Z6MulOddIu16__rvv_uint32m8_tu16__rvv_uint16m8_tET_T0_S1_(v0: vec<ef4m8>, v1: vec<ef4m8>) -> vec<ef2m8>:
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vmul.vv[ef4m8, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef4m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef4m8, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=16, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m8, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m8, dest=ef4m8](v12)
  (v18) = vsetvlrelay[mmul=32, src_mmul=32, tama](v2)
  (v17) = vmsne.vx[ef4m8, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m8, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef4m8, dest=ef2m8](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_uint64m1_tu17__rvv_uint32mf2_tET_T0_S1_(v0: vec<ef2mf2>, v1: vec<ef2mf2>) -> vec<e1m1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[ef2mf2, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef2mf2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef2mf2, ud, fm](v2, v6, v9)
  (v11) = vlenb()
  (v12) = scalar[xmul=1, imm=0x0000000000000004]()
  (v13) = srl.xx[width=1](v11, v12)
  (v14) = scalar[xmul=1, imm=0x0000000000000200]()
  (v15) = minu.xx[width=1](v13, v14)
  (v18) = vsetvl[mmul=1, tama](v15)
  (v19) = scalar[xmul=1, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[e1m1, ud](v18, v19)
  (v20) = vec_to_vec[src=e1m1, dest=ef8m1](v16)
  (v21) = vget[part=ef8mf2, src=ef8m1, idx=0](v20)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v21)
  (v24) = vsetvlrelay[mmul=1, src_mmul=1, tama](v2)
  (v23) = vmsne.vx[ef2mf2, ud, fm](v24, v22, v9)
  (v25) = vmerge.vvm[ef2mf2, ud](v24, v8, v3, v23)
  (v27) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v25)
  (v28) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v27)
  (v30) = vec_to_vec[src=ef8m1, dest=e1m1](v28)
  return (v30)

def _Z6MulOddIu16__rvv_uint64m1_tu17__rvv_uint32mf2_tET_T0_S1_(v0: vec<ef2mf2>, v1: vec<ef2mf2>) -> vec<e1m1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[ef2mf2, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef2mf2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef2mf2, ud, fm](v2, v3, v9)
  (v11) = vlenb()
  (v12) = scalar[xmul=1, imm=0x0000000000000004]()
  (v13) = srl.xx[width=1](v11, v12)
  (v14) = scalar[xmul=1, imm=0x0000000000000200]()
  (v15) = minu.xx[width=1](v13, v14)
  (v18) = vsetvl[mmul=1, tama](v15)
  (v19) = scalar[xmul=1, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[e1m1, ud](v18, v19)
  (v20) = vec_to_vec[src=e1m1, dest=ef8m1](v16)
  (v21) = vget[part=ef8mf2, src=ef8m1, idx=0](v20)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v21)
  (v24) = vsetvlrelay[mmul=1, src_mmul=1, tama](v2)
  (v23) = vmsne.vx[ef2mf2, ud, fm](v24, v22, v9)
  (v25) = vmerge.vvm[ef2mf2, ud](v24, v6, v8, v23)
  (v27) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v25)
  (v28) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v27)
  (v30) = vec_to_vec[src=ef8m1, dest=e1m1](v28)
  return (v30)

def _Z7MulEvenIu16__rvv_uint64m1_tu16__rvv_uint32m1_tET_T0_S1_(v0: vec<ef2m1>, v1: vec<ef2m1>) -> vec<e1m1>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[ef2m1, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef2m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef2m1, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m1, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m1, dest=ef2m1](v12)
  (v18) = vsetvlrelay[mmul=2, src_mmul=2, tama](v2)
  (v17) = vmsne.vx[ef2m1, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m1, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef2m1, dest=e1m1](v19)
  return (v21)

def _Z6MulOddIu16__rvv_uint64m1_tu16__rvv_uint32m1_tET_T0_S1_(v0: vec<ef2m1>, v1: vec<ef2m1>) -> vec<e1m1>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[ef2m1, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef2m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef2m1, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m1, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m1, dest=ef2m1](v12)
  (v18) = vsetvlrelay[mmul=2, src_mmul=2, tama](v2)
  (v17) = vmsne.vx[ef2m1, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m1, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef2m1, dest=e1m1](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_uint64m2_tu16__rvv_uint32m2_tET_T0_S1_(v0: vec<ef2m2>, v1: vec<ef2m2>) -> vec<e1m2>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[ef2m2, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef2m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef2m2, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m2, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m2, dest=ef2m2](v12)
  (v18) = vsetvlrelay[mmul=4, src_mmul=4, tama](v2)
  (v17) = vmsne.vx[ef2m2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m2, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef2m2, dest=e1m2](v19)
  return (v21)

def _Z6MulOddIu16__rvv_uint64m2_tu16__rvv_uint32m2_tET_T0_S1_(v0: vec<ef2m2>, v1: vec<ef2m2>) -> vec<e1m2>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[ef2m2, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef2m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef2m2, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m2, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m2, dest=ef2m2](v12)
  (v18) = vsetvlrelay[mmul=4, src_mmul=4, tama](v2)
  (v17) = vmsne.vx[ef2m2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m2, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef2m2, dest=e1m2](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_uint64m4_tu16__rvv_uint32m4_tET_T0_S1_(v0: vec<ef2m4>, v1: vec<ef2m4>) -> vec<e1m4>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[ef2m4, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef2m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef2m4, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=4, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m4, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m4, dest=ef2m4](v12)
  (v18) = vsetvlrelay[mmul=8, src_mmul=8, tama](v2)
  (v17) = vmsne.vx[ef2m4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m4, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef2m4, dest=e1m4](v19)
  return (v21)

def _Z6MulOddIu16__rvv_uint64m4_tu16__rvv_uint32m4_tET_T0_S1_(v0: vec<ef2m4>, v1: vec<ef2m4>) -> vec<e1m4>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[ef2m4, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef2m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef2m4, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=4, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m4, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m4, dest=ef2m4](v12)
  (v18) = vsetvlrelay[mmul=8, src_mmul=8, tama](v2)
  (v17) = vmsne.vx[ef2m4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m4, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef2m4, dest=e1m4](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_uint64m8_tu16__rvv_uint32m8_tET_T0_S1_(v0: vec<ef2m8>, v1: vec<ef2m8>) -> vec<e1m8>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vmul.vv[ef2m8, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef2m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef2m8, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=8, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m8, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m8, dest=ef2m8](v12)
  (v18) = vsetvlrelay[mmul=16, src_mmul=16, tama](v2)
  (v17) = vmsne.vx[ef2m8, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m8, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef2m8, dest=e1m8](v19)
  return (v21)

def _Z6MulOddIu16__rvv_uint64m8_tu16__rvv_uint32m8_tET_T0_S1_(v0: vec<ef2m8>, v1: vec<ef2m8>) -> vec<e1m8>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vmul.vv[ef2m8, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[ef2m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef2m8, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=8, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m8, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m8, dest=ef2m8](v12)
  (v18) = vsetvlrelay[mmul=16, src_mmul=16, tama](v2)
  (v17) = vmsne.vx[ef2m8, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m8, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef2m8, dest=e1m8](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_uint64m1_tu16__rvv_uint64m1_tET_T0_S1_(v0: vec<e1m1>, v1: vec<e1m1>) -> vec<e1m1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[e1m1, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[e1m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[e1m1, ud, fm](v2, v6, v9)
  (v11) = vid.v[e1m1, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m1, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m1, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m1, ud](v2, v8, v3, v16)
  return (v18)

def _Z6MulOddIu16__rvv_uint64m1_tu16__rvv_uint64m1_tET_T0_S1_(v0: vec<e1m1>, v1: vec<e1m1>) -> vec<e1m1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[e1m1, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[e1m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[e1m1, ud, fm](v2, v3, v9)
  (v11) = vid.v[e1m1, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m1, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m1, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m1, ud](v2, v6, v8, v16)
  return (v18)

def _Z7MulEvenIu16__rvv_uint64m2_tu16__rvv_uint64m2_tET_T0_S1_(v0: vec<e1m2>, v1: vec<e1m2>) -> vec<e1m2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[e1m2, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[e1m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[e1m2, ud, fm](v2, v6, v9)
  (v11) = vid.v[e1m2, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m2, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m2, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m2, ud](v2, v8, v3, v16)
  return (v18)

def _Z6MulOddIu16__rvv_uint64m2_tu16__rvv_uint64m2_tET_T0_S1_(v0: vec<e1m2>, v1: vec<e1m2>) -> vec<e1m2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[e1m2, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[e1m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[e1m2, ud, fm](v2, v3, v9)
  (v11) = vid.v[e1m2, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m2, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m2, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m2, ud](v2, v6, v8, v16)
  return (v18)

def _Z7MulEvenIu16__rvv_uint64m4_tu16__rvv_uint64m4_tET_T0_S1_(v0: vec<e1m4>, v1: vec<e1m4>) -> vec<e1m4>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[e1m4, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[e1m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[e1m4, ud, fm](v2, v6, v9)
  (v11) = vid.v[e1m4, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m4, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m4, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m4, ud](v2, v8, v3, v16)
  return (v18)

def _Z6MulOddIu16__rvv_uint64m4_tu16__rvv_uint64m4_tET_T0_S1_(v0: vec<e1m4>, v1: vec<e1m4>) -> vec<e1m4>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[e1m4, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[e1m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[e1m4, ud, fm](v2, v3, v9)
  (v11) = vid.v[e1m4, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m4, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m4, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m4, ud](v2, v6, v8, v16)
  return (v18)

def _Z7MulEvenIu16__rvv_uint64m8_tu16__rvv_uint64m8_tET_T0_S1_(v0: vec<e1m8>, v1: vec<e1m8>) -> vec<e1m8>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[e1m8, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[e1m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[e1m8, ud, fm](v2, v6, v9)
  (v11) = vid.v[e1m8, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m8, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m8, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m8, ud](v2, v8, v3, v16)
  return (v18)

def _Z6MulOddIu16__rvv_uint64m8_tu16__rvv_uint64m8_tET_T0_S1_(v0: vec<e1m8>, v1: vec<e1m8>) -> vec<e1m8>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[e1m8, ud, fm](v2, v0, v1)
  (v6) = vmulhu.vv[e1m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[e1m8, ud, fm](v2, v3, v9)
  (v11) = vid.v[e1m8, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m8, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m8, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m8, ud](v2, v6, v8, v16)
  return (v18)

def _Z7MulEvenIu16__rvv_int16mf4_tu15__rvv_int8mf8_tET_T0_S1_(v0: vec<ef8mf8>, v1: vec<ef8mf8>) -> vec<ef4mf4>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[ef8mf8, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef8mf8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef8mf8, ud, fm](v2, v6, v9)
  (v11) = vlenb()
  (v12) = scalar[xmul=1, imm=0x0000000000000004]()
  (v13) = srl.xx[width=1](v11, v12)
  (v14) = scalar[xmul=1, imm=0x0000000000000200]()
  (v15) = minu.xx[width=1](v13, v14)
  (v18) = vsetvl[mmul=1, tama](v15)
  (v19) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef4mf4, ud](v18, v19)
  (v20) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v16)
  (v21) = vget[part=ef8mf8, src=ef8mf4, idx=0](v20)
  (v23) = vsetvlrelay[mmul=1, src_mmul=1, tama](v2)
  (v22) = vmsne.vx[ef8mf8, ud, fm](v23, v21, v9)
  (v24) = vmerge.vvm[ef8mf8, ud](v23, v8, v3, v22)
  (v26) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v24)
  (v28) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v26)
  return (v28)

def _Z6MulOddIu16__rvv_int16mf4_tu15__rvv_int8mf8_tET_T0_S1_(v0: vec<ef8mf8>, v1: vec<ef8mf8>) -> vec<ef4mf4>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[ef8mf8, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef8mf8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef8mf8, ud, fm](v2, v3, v9)
  (v11) = vlenb()
  (v12) = scalar[xmul=1, imm=0x0000000000000004]()
  (v13) = srl.xx[width=1](v11, v12)
  (v14) = scalar[xmul=1, imm=0x0000000000000200]()
  (v15) = minu.xx[width=1](v13, v14)
  (v18) = vsetvl[mmul=1, tama](v15)
  (v19) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef4mf4, ud](v18, v19)
  (v20) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v16)
  (v21) = vget[part=ef8mf8, src=ef8mf4, idx=0](v20)
  (v23) = vsetvlrelay[mmul=1, src_mmul=1, tama](v2)
  (v22) = vmsne.vx[ef8mf8, ud, fm](v23, v21, v9)
  (v24) = vmerge.vvm[ef8mf8, ud](v23, v6, v8, v22)
  (v26) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v24)
  (v28) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v26)
  return (v28)

def _Z7MulEvenIu16__rvv_int16mf4_tu15__rvv_int8mf4_tET_T0_S1_(v0: vec<ef8mf4>, v1: vec<ef8mf4>) -> vec<ef4mf4>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[ef8mf4, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef8mf4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef8mf4, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4mf4, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v12)
  (v18) = vsetvlrelay[mmul=2, src_mmul=2, tama](v2)
  (v17) = vmsne.vx[ef8mf4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8mf4, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z6MulOddIu16__rvv_int16mf4_tu15__rvv_int8mf4_tET_T0_S1_(v0: vec<ef8mf4>, v1: vec<ef8mf4>) -> vec<ef4mf4>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[ef8mf4, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef8mf4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef8mf4, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4mf4, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v12)
  (v18) = vsetvlrelay[mmul=2, src_mmul=2, tama](v2)
  (v17) = vmsne.vx[ef8mf4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8mf4, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_int16mf2_tu15__rvv_int8mf2_tET_T0_S1_(v0: vec<ef8mf2>, v1: vec<ef8mf2>) -> vec<ef4mf2>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[ef8mf2, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef8mf2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef8mf2, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4mf2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v12)
  (v18) = vsetvlrelay[mmul=4, src_mmul=4, tama](v2)
  (v17) = vmsne.vx[ef8mf2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8mf2, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z6MulOddIu16__rvv_int16mf2_tu15__rvv_int8mf2_tET_T0_S1_(v0: vec<ef8mf2>, v1: vec<ef8mf2>) -> vec<ef4mf2>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[ef8mf2, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef8mf2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef8mf2, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4mf2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v12)
  (v18) = vsetvlrelay[mmul=4, src_mmul=4, tama](v2)
  (v17) = vmsne.vx[ef8mf2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8mf2, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z7MulEvenIu15__rvv_int16m1_tu14__rvv_int8m1_tET_T0_S1_(v0: vec<ef8m1>, v1: vec<ef8m1>) -> vec<ef4m1>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[ef8m1, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef8m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef8m1, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=4, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m1, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m1, dest=ef8m1](v12)
  (v18) = vsetvlrelay[mmul=8, src_mmul=8, tama](v2)
  (v17) = vmsne.vx[ef8m1, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m1, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z6MulOddIu15__rvv_int16m1_tu14__rvv_int8m1_tET_T0_S1_(v0: vec<ef8m1>, v1: vec<ef8m1>) -> vec<ef4m1>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[ef8m1, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef8m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef8m1, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=4, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m1, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m1, dest=ef8m1](v12)
  (v18) = vsetvlrelay[mmul=8, src_mmul=8, tama](v2)
  (v17) = vmsne.vx[ef8m1, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m1, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z7MulEvenIu15__rvv_int16m2_tu14__rvv_int8m2_tET_T0_S1_(v0: vec<ef8m2>, v1: vec<ef8m2>) -> vec<ef4m2>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vmul.vv[ef8m2, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef8m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef8m2, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=8, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m2, dest=ef8m2](v12)
  (v18) = vsetvlrelay[mmul=16, src_mmul=16, tama](v2)
  (v17) = vmsne.vx[ef8m2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m2, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef4m2](v19)
  return (v21)

def _Z6MulOddIu15__rvv_int16m2_tu14__rvv_int8m2_tET_T0_S1_(v0: vec<ef8m2>, v1: vec<ef8m2>) -> vec<ef4m2>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vmul.vv[ef8m2, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef8m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef8m2, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=8, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m2, dest=ef8m2](v12)
  (v18) = vsetvlrelay[mmul=16, src_mmul=16, tama](v2)
  (v17) = vmsne.vx[ef8m2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m2, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef4m2](v19)
  return (v21)

def _Z7MulEvenIu15__rvv_int16m4_tu14__rvv_int8m4_tET_T0_S1_(v0: vec<ef8m4>, v1: vec<ef8m4>) -> vec<ef4m4>:
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vmul.vv[ef8m4, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef8m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef8m4, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=16, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m4, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m4, dest=ef8m4](v12)
  (v18) = vsetvlrelay[mmul=32, src_mmul=32, tama](v2)
  (v17) = vmsne.vx[ef8m4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m4, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z6MulOddIu15__rvv_int16m4_tu14__rvv_int8m4_tET_T0_S1_(v0: vec<ef8m4>, v1: vec<ef8m4>) -> vec<ef4m4>:
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vmul.vv[ef8m4, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef8m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef8m4, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=16, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m4, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m4, dest=ef8m4](v12)
  (v18) = vsetvlrelay[mmul=32, src_mmul=32, tama](v2)
  (v17) = vmsne.vx[ef8m4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m4, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z7MulEvenIu15__rvv_int16m8_tu14__rvv_int8m8_tET_T0_S1_(v0: vec<ef8m8>, v1: vec<ef8m8>) -> vec<ef4m8>:
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vmul.vv[ef8m8, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef8m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef8m8, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m8, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m8, dest=ef8m8](v12)
  (v18) = vsetvlrelay[mmul=64, src_mmul=64, tama](v2)
  (v17) = vmsne.vx[ef8m8, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m8, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z6MulOddIu15__rvv_int16m8_tu14__rvv_int8m8_tET_T0_S1_(v0: vec<ef8m8>, v1: vec<ef8m8>) -> vec<ef4m8>:
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vmul.vv[ef8m8, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef8m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef8m8, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef4m8, ud](v11, v15)
  (v16) = vec_to_vec[src=ef4m8, dest=ef8m8](v12)
  (v18) = vsetvlrelay[mmul=64, src_mmul=64, tama](v2)
  (v17) = vmsne.vx[ef8m8, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef8m8, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z7MulEvenIu16__rvv_int32mf2_tu16__rvv_int16mf4_tET_T0_S1_(v0: vec<ef4mf4>, v1: vec<ef4mf4>) -> vec<ef2mf2>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[ef4mf4, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef4mf4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef4mf4, ud, fm](v2, v6, v9)
  (v11) = vlenb()
  (v12) = scalar[xmul=1, imm=0x0000000000000004]()
  (v13) = srl.xx[width=1](v11, v12)
  (v14) = scalar[xmul=1, imm=0x0000000000000200]()
  (v15) = minu.xx[width=1](v13, v14)
  (v18) = vsetvl[mmul=1, tama](v15)
  (v19) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef2mf2, ud](v18, v19)
  (v20) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v16)
  (v21) = vget[part=ef8mf4, src=ef8mf2, idx=0](v20)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  (v24) = vsetvlrelay[mmul=1, src_mmul=1, tama](v2)
  (v23) = vmsne.vx[ef4mf4, ud, fm](v24, v22, v9)
  (v25) = vmerge.vvm[ef4mf4, ud](v24, v8, v3, v23)
  (v27) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v25)
  (v28) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v27)
  (v30) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v28)
  return (v30)

def _Z6MulOddIu16__rvv_int32mf2_tu16__rvv_int16mf4_tET_T0_S1_(v0: vec<ef4mf4>, v1: vec<ef4mf4>) -> vec<ef2mf2>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[ef4mf4, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef4mf4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef4mf4, ud, fm](v2, v3, v9)
  (v11) = vlenb()
  (v12) = scalar[xmul=1, imm=0x0000000000000004]()
  (v13) = srl.xx[width=1](v11, v12)
  (v14) = scalar[xmul=1, imm=0x0000000000000200]()
  (v15) = minu.xx[width=1](v13, v14)
  (v18) = vsetvl[mmul=1, tama](v15)
  (v19) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef2mf2, ud](v18, v19)
  (v20) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v16)
  (v21) = vget[part=ef8mf4, src=ef8mf2, idx=0](v20)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  (v24) = vsetvlrelay[mmul=1, src_mmul=1, tama](v2)
  (v23) = vmsne.vx[ef4mf4, ud, fm](v24, v22, v9)
  (v25) = vmerge.vvm[ef4mf4, ud](v24, v6, v8, v23)
  (v27) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v25)
  (v28) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v27)
  (v30) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v28)
  return (v30)

def _Z7MulEvenIu16__rvv_int32mf2_tu16__rvv_int16mf2_tET_T0_S1_(v0: vec<ef4mf2>, v1: vec<ef4mf2>) -> vec<ef2mf2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[ef4mf2, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef4mf2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef4mf2, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2mf2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2mf2, dest=ef4mf2](v12)
  (v18) = vsetvlrelay[mmul=2, src_mmul=2, tama](v2)
  (v17) = vmsne.vx[ef4mf2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4mf2, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef4mf2, dest=ef2mf2](v19)
  return (v21)

def _Z6MulOddIu16__rvv_int32mf2_tu16__rvv_int16mf2_tET_T0_S1_(v0: vec<ef4mf2>, v1: vec<ef4mf2>) -> vec<ef2mf2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[ef4mf2, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef4mf2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef4mf2, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2mf2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2mf2, dest=ef4mf2](v12)
  (v18) = vsetvlrelay[mmul=2, src_mmul=2, tama](v2)
  (v17) = vmsne.vx[ef4mf2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4mf2, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef4mf2, dest=ef2mf2](v19)
  return (v21)

def _Z7MulEvenIu15__rvv_int32m1_tu15__rvv_int16m1_tET_T0_S1_(v0: vec<ef4m1>, v1: vec<ef4m1>) -> vec<ef2m1>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[ef4m1, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef4m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef4m1, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m1, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m1, dest=ef4m1](v12)
  (v18) = vsetvlrelay[mmul=4, src_mmul=4, tama](v2)
  (v17) = vmsne.vx[ef4m1, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m1, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef4m1, dest=ef2m1](v19)
  return (v21)

def _Z6MulOddIu15__rvv_int32m1_tu15__rvv_int16m1_tET_T0_S1_(v0: vec<ef4m1>, v1: vec<ef4m1>) -> vec<ef2m1>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[ef4m1, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef4m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef4m1, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m1, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m1, dest=ef4m1](v12)
  (v18) = vsetvlrelay[mmul=4, src_mmul=4, tama](v2)
  (v17) = vmsne.vx[ef4m1, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m1, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef4m1, dest=ef2m1](v19)
  return (v21)

def _Z7MulEvenIu15__rvv_int32m2_tu15__rvv_int16m2_tET_T0_S1_(v0: vec<ef4m2>, v1: vec<ef4m2>) -> vec<ef2m2>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[ef4m2, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef4m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef4m2, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=4, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m2, dest=ef4m2](v12)
  (v18) = vsetvlrelay[mmul=8, src_mmul=8, tama](v2)
  (v17) = vmsne.vx[ef4m2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m2, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef4m2, dest=ef2m2](v19)
  return (v21)

def _Z6MulOddIu15__rvv_int32m2_tu15__rvv_int16m2_tET_T0_S1_(v0: vec<ef4m2>, v1: vec<ef4m2>) -> vec<ef2m2>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[ef4m2, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef4m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef4m2, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=4, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m2, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m2, dest=ef4m2](v12)
  (v18) = vsetvlrelay[mmul=8, src_mmul=8, tama](v2)
  (v17) = vmsne.vx[ef4m2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m2, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef4m2, dest=ef2m2](v19)
  return (v21)

def _Z7MulEvenIu15__rvv_int32m4_tu15__rvv_int16m4_tET_T0_S1_(v0: vec<ef4m4>, v1: vec<ef4m4>) -> vec<ef2m4>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vmul.vv[ef4m4, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef4m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef4m4, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=8, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m4, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m4, dest=ef4m4](v12)
  (v18) = vsetvlrelay[mmul=16, src_mmul=16, tama](v2)
  (v17) = vmsne.vx[ef4m4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m4, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef4m4, dest=ef2m4](v19)
  return (v21)

def _Z6MulOddIu15__rvv_int32m4_tu15__rvv_int16m4_tET_T0_S1_(v0: vec<ef4m4>, v1: vec<ef4m4>) -> vec<ef2m4>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vmul.vv[ef4m4, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef4m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef4m4, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=8, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m4, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m4, dest=ef4m4](v12)
  (v18) = vsetvlrelay[mmul=16, src_mmul=16, tama](v2)
  (v17) = vmsne.vx[ef4m4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m4, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef4m4, dest=ef2m4](v19)
  return (v21)

def _Z7MulEvenIu15__rvv_int32m8_tu15__rvv_int16m8_tET_T0_S1_(v0: vec<ef4m8>, v1: vec<ef4m8>) -> vec<ef2m8>:
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vmul.vv[ef4m8, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef4m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef4m8, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=16, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m8, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m8, dest=ef4m8](v12)
  (v18) = vsetvlrelay[mmul=32, src_mmul=32, tama](v2)
  (v17) = vmsne.vx[ef4m8, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m8, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef4m8, dest=ef2m8](v19)
  return (v21)

def _Z6MulOddIu15__rvv_int32m8_tu15__rvv_int16m8_tET_T0_S1_(v0: vec<ef4m8>, v1: vec<ef4m8>) -> vec<ef2m8>:
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vmul.vv[ef4m8, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef4m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef4m8, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=16, tama]()
  (v15) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef2m8, ud](v11, v15)
  (v16) = vec_to_vec[src=ef2m8, dest=ef4m8](v12)
  (v18) = vsetvlrelay[mmul=32, src_mmul=32, tama](v2)
  (v17) = vmsne.vx[ef4m8, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef4m8, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef4m8, dest=ef2m8](v19)
  return (v21)

def _Z7MulEvenIu15__rvv_int64m1_tu16__rvv_int32mf2_tET_T0_S1_(v0: vec<ef2mf2>, v1: vec<ef2mf2>) -> vec<e1m1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[ef2mf2, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef2mf2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef2mf2, ud, fm](v2, v6, v9)
  (v11) = vlenb()
  (v12) = scalar[xmul=1, imm=0x0000000000000004]()
  (v13) = srl.xx[width=1](v11, v12)
  (v14) = scalar[xmul=1, imm=0x0000000000000200]()
  (v15) = minu.xx[width=1](v13, v14)
  (v18) = vsetvl[mmul=1, tama](v15)
  (v19) = scalar[xmul=1, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[e1m1, ud](v18, v19)
  (v20) = vec_to_vec[src=e1m1, dest=ef8m1](v16)
  (v21) = vget[part=ef8mf2, src=ef8m1, idx=0](v20)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v21)
  (v24) = vsetvlrelay[mmul=1, src_mmul=1, tama](v2)
  (v23) = vmsne.vx[ef2mf2, ud, fm](v24, v22, v9)
  (v25) = vmerge.vvm[ef2mf2, ud](v24, v8, v3, v23)
  (v27) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v25)
  (v28) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v27)
  (v30) = vec_to_vec[src=ef8m1, dest=e1m1](v28)
  return (v30)

def _Z6MulOddIu15__rvv_int64m1_tu16__rvv_int32mf2_tET_T0_S1_(v0: vec<ef2mf2>, v1: vec<ef2mf2>) -> vec<e1m1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[ef2mf2, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef2mf2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef2mf2, ud, fm](v2, v3, v9)
  (v11) = vlenb()
  (v12) = scalar[xmul=1, imm=0x0000000000000004]()
  (v13) = srl.xx[width=1](v11, v12)
  (v14) = scalar[xmul=1, imm=0x0000000000000200]()
  (v15) = minu.xx[width=1](v13, v14)
  (v18) = vsetvl[mmul=1, tama](v15)
  (v19) = scalar[xmul=1, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[e1m1, ud](v18, v19)
  (v20) = vec_to_vec[src=e1m1, dest=ef8m1](v16)
  (v21) = vget[part=ef8mf2, src=ef8m1, idx=0](v20)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v21)
  (v24) = vsetvlrelay[mmul=1, src_mmul=1, tama](v2)
  (v23) = vmsne.vx[ef2mf2, ud, fm](v24, v22, v9)
  (v25) = vmerge.vvm[ef2mf2, ud](v24, v6, v8, v23)
  (v27) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v25)
  (v28) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v27)
  (v30) = vec_to_vec[src=ef8m1, dest=e1m1](v28)
  return (v30)

def _Z7MulEvenIu15__rvv_int64m1_tu15__rvv_int32m1_tET_T0_S1_(v0: vec<ef2m1>, v1: vec<ef2m1>) -> vec<e1m1>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[ef2m1, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef2m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef2m1, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m1, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m1, dest=ef2m1](v12)
  (v18) = vsetvlrelay[mmul=2, src_mmul=2, tama](v2)
  (v17) = vmsne.vx[ef2m1, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m1, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef2m1, dest=e1m1](v19)
  return (v21)

def _Z6MulOddIu15__rvv_int64m1_tu15__rvv_int32m1_tET_T0_S1_(v0: vec<ef2m1>, v1: vec<ef2m1>) -> vec<e1m1>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[ef2m1, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef2m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef2m1, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m1, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m1, dest=ef2m1](v12)
  (v18) = vsetvlrelay[mmul=2, src_mmul=2, tama](v2)
  (v17) = vmsne.vx[ef2m1, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m1, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef2m1, dest=e1m1](v19)
  return (v21)

def _Z7MulEvenIu15__rvv_int64m2_tu15__rvv_int32m2_tET_T0_S1_(v0: vec<ef2m2>, v1: vec<ef2m2>) -> vec<e1m2>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[ef2m2, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef2m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef2m2, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m2, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m2, dest=ef2m2](v12)
  (v18) = vsetvlrelay[mmul=4, src_mmul=4, tama](v2)
  (v17) = vmsne.vx[ef2m2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m2, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef2m2, dest=e1m2](v19)
  return (v21)

def _Z6MulOddIu15__rvv_int64m2_tu15__rvv_int32m2_tET_T0_S1_(v0: vec<ef2m2>, v1: vec<ef2m2>) -> vec<e1m2>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[ef2m2, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef2m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef2m2, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m2, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m2, dest=ef2m2](v12)
  (v18) = vsetvlrelay[mmul=4, src_mmul=4, tama](v2)
  (v17) = vmsne.vx[ef2m2, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m2, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef2m2, dest=e1m2](v19)
  return (v21)

def _Z7MulEvenIu15__rvv_int64m4_tu15__rvv_int32m4_tET_T0_S1_(v0: vec<ef2m4>, v1: vec<ef2m4>) -> vec<e1m4>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[ef2m4, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef2m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef2m4, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=4, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m4, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m4, dest=ef2m4](v12)
  (v18) = vsetvlrelay[mmul=8, src_mmul=8, tama](v2)
  (v17) = vmsne.vx[ef2m4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m4, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef2m4, dest=e1m4](v19)
  return (v21)

def _Z6MulOddIu15__rvv_int64m4_tu15__rvv_int32m4_tET_T0_S1_(v0: vec<ef2m4>, v1: vec<ef2m4>) -> vec<e1m4>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[ef2m4, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef2m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef2m4, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=4, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m4, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m4, dest=ef2m4](v12)
  (v18) = vsetvlrelay[mmul=8, src_mmul=8, tama](v2)
  (v17) = vmsne.vx[ef2m4, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m4, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef2m4, dest=e1m4](v19)
  return (v21)

def _Z7MulEvenIu15__rvv_int64m8_tu15__rvv_int32m8_tET_T0_S1_(v0: vec<ef2m8>, v1: vec<ef2m8>) -> vec<e1m8>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vmul.vv[ef2m8, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef2m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[ef2m8, ud, fm](v2, v6, v9)
  (v11) = vsetvlmax[mmul=8, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m8, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m8, dest=ef2m8](v12)
  (v18) = vsetvlrelay[mmul=16, src_mmul=16, tama](v2)
  (v17) = vmsne.vx[ef2m8, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m8, ud](v18, v8, v3, v17)
  (v21) = vec_to_vec[src=ef2m8, dest=e1m8](v19)
  return (v21)

def _Z6MulOddIu15__rvv_int64m8_tu15__rvv_int32m8_tET_T0_S1_(v0: vec<ef2m8>, v1: vec<ef2m8>) -> vec<e1m8>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vmul.vv[ef2m8, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[ef2m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[ef2m8, ud, fm](v2, v3, v9)
  (v11) = vsetvlmax[mmul=8, tama]()
  (v15) = scalar[xmul=1, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[e1m8, ud](v11, v15)
  (v16) = vec_to_vec[src=e1m8, dest=ef2m8](v12)
  (v18) = vsetvlrelay[mmul=16, src_mmul=16, tama](v2)
  (v17) = vmsne.vx[ef2m8, ud, fm](v18, v16, v9)
  (v19) = vmerge.vvm[ef2m8, ud](v18, v6, v8, v17)
  (v21) = vec_to_vec[src=ef2m8, dest=e1m8](v19)
  return (v21)

def _Z7MulEvenIu15__rvv_int64m1_tu15__rvv_int64m1_tET_T0_S1_(v0: vec<e1m1>, v1: vec<e1m1>) -> vec<e1m1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[e1m1, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[e1m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[e1m1, ud, fm](v2, v6, v9)
  (v11) = vid.v[e1m1, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m1, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m1, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m1, ud](v2, v8, v3, v16)
  return (v18)

def _Z6MulOddIu15__rvv_int64m1_tu15__rvv_int64m1_tET_T0_S1_(v0: vec<e1m1>, v1: vec<e1m1>) -> vec<e1m1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmul.vv[e1m1, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[e1m1, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[e1m1, ud, fm](v2, v3, v9)
  (v11) = vid.v[e1m1, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m1, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m1, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m1, ud](v2, v6, v8, v16)
  return (v18)

def _Z7MulEvenIu15__rvv_int64m2_tu15__rvv_int64m2_tET_T0_S1_(v0: vec<e1m2>, v1: vec<e1m2>) -> vec<e1m2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[e1m2, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[e1m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[e1m2, ud, fm](v2, v6, v9)
  (v11) = vid.v[e1m2, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m2, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m2, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m2, ud](v2, v8, v3, v16)
  return (v18)

def _Z6MulOddIu15__rvv_int64m2_tu15__rvv_int64m2_tET_T0_S1_(v0: vec<e1m2>, v1: vec<e1m2>) -> vec<e1m2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmul.vv[e1m2, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[e1m2, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[e1m2, ud, fm](v2, v3, v9)
  (v11) = vid.v[e1m2, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m2, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m2, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m2, ud](v2, v6, v8, v16)
  return (v18)

def _Z7MulEvenIu15__rvv_int64m4_tu15__rvv_int64m4_tET_T0_S1_(v0: vec<e1m4>, v1: vec<e1m4>) -> vec<e1m4>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[e1m4, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[e1m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[e1m4, ud, fm](v2, v6, v9)
  (v11) = vid.v[e1m4, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m4, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m4, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m4, ud](v2, v8, v3, v16)
  return (v18)

def _Z6MulOddIu15__rvv_int64m4_tu15__rvv_int64m4_tET_T0_S1_(v0: vec<e1m4>, v1: vec<e1m4>) -> vec<e1m4>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmul.vv[e1m4, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[e1m4, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[e1m4, ud, fm](v2, v3, v9)
  (v11) = vid.v[e1m4, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m4, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m4, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m4, ud](v2, v6, v8, v16)
  return (v18)

def _Z7MulEvenIu15__rvv_int64m8_tu15__rvv_int64m8_tET_T0_S1_(v0: vec<e1m8>, v1: vec<e1m8>) -> vec<e1m8>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[e1m8, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[e1m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1up.vx[e1m8, ud, fm](v2, v6, v9)
  (v11) = vid.v[e1m8, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m8, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m8, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m8, ud](v2, v8, v3, v16)
  return (v18)

def _Z6MulOddIu15__rvv_int64m8_tu15__rvv_int64m8_tET_T0_S1_(v0: vec<e1m8>, v1: vec<e1m8>) -> vec<e1m8>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmul.vv[e1m8, ud, fm](v2, v0, v1)
  (v6) = vmulh.vv[e1m8, ud, fm](v2, v0, v1)
  (v9) = scalar[xmul=1, imm=0x0000000000000000]()
  (v8) = vslide1down.vx[e1m8, ud, fm](v2, v3, v9)
  (v11) = vid.v[e1m8, ud, fm](v2)
  (v14) = scalar[xmul=1, imm=0x0000000000000001]()
  (v13) = vand.vx[e1m8, ud, fm](v2, v11, v14)
  (v16) = vmseq.vx[e1m8, ud, fm](v2, v13, v9)
  (v18) = vmerge.vvm[e1m8, ud](v2, v6, v8, v16)
  return (v18)

