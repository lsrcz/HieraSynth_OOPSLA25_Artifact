def _Z5Lt128Iu14__rvv_bool64_tu16__rvv_uint64m1_tET_T0_S1_(v0: vec<e1m1>, v1: vec<e1m1>) -> mask<mmul=1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmseq.vv[e1m1, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m1, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m1, ud](v2, v5, v10, v3)
  (v12) = vmsltu.vv[e1m1, ud, fm](v2, v0, v1)
  (v14) = vmerge.vxm[e1m1, ud](v2, v5, v10, v12)
  (v16) = vslide1up.vx[e1m1, ud, fm](v2, v14, v8)
  (v18) = vand.vv[e1m1, ud, fm](v2, v9, v16)
  (v20) = vor.vv[e1m1, ud, fm](v2, v14, v18)
  (v22) = vslide1down.vx[e1m1, ud, fm](v2, v20, v8)
  (v24) = vid.v[e1m1, ud, fm](v2)
  (v27) = scalar[xmul=1, imm=0x0000000000000001]()
  (v26) = vand.vx[e1m1, ud, fm](v2, v24, v27)
  (v29) = vmseq.vx[e1m1, ud, fm](v2, v26, v8)
  (v31) = vmerge.vvm[e1m1, ud](v2, v20, v22, v29)
  (v33) = vmsne.vx[e1m1, ud, fm](v2, v31, v8)
  return (v33)

def _Z10Lt128UpperIu14__rvv_bool64_tu16__rvv_uint64m1_tET_T0_S1_(v0: vec<e1m1>, v1: vec<e1m1>) -> mask<mmul=1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmsltu.vv[e1m1, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m1, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m1, ud](v2, v5, v10, v3)
  (v12) = vslide1down.vx[e1m1, ud, fm](v2, v9, v8)
  (v14) = first_element_to_scalar[e1m1](v12)
  (v15) = vid.v[e1m1, ud, fm](v2)
  (v18) = scalar[xmul=1, imm=0x0000000000000001]()
  (v17) = vand.vx[e1m1, ud, fm](v2, v15, v18)
  (v20) = vmseq.vx[e1m1, ud, fm](v2, v17, v8)
  (v22) = vmerge.vvm[e1m1, ud](v2, v9, v12, v20)
  (v24) = vmsne.vx[e1m1, ud, fm](v2, v22, v8)
  return (v24)

def _Z5Eq128Iu14__rvv_bool64_tu16__rvv_uint64m1_tET_T0_S1_(v0: vec<e1m1>, v1: vec<e1m1>) -> mask<mmul=1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmseq.vv[e1m1, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m1, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m1, ud](v2, v5, v10, v3)
  (v12) = vslide1up.vx[e1m1, ud, fm](v2, v9, v8)
  (v14) = vslide1down.vx[e1m1, ud, fm](v2, v9, v8)
  (v16) = vid.v[e1m1, ud, fm](v2)
  (v19) = scalar[xmul=1, imm=0x0000000000000001]()
  (v18) = vand.vx[e1m1, ud, fm](v2, v16, v19)
  (v21) = vmseq.vx[e1m1, ud, fm](v2, v18, v8)
  (v23) = vmerge.vvm[e1m1, ud](v2, v12, v14, v21)
  (v25) = vand.vv[e1m1, ud, fm](v2, v9, v23)
  (v27) = first_element_to_scalar[e1m1](v25)
  (v28) = vmsne.vx[e1m1, ud, fm](v2, v25, v8)
  return (v28)

def _Z10Eq128UpperIu14__rvv_bool64_tu16__rvv_uint64m1_tET_T0_S1_(v0: vec<e1m1>, v1: vec<e1m1>) -> mask<mmul=1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmseq.vv[e1m1, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m1, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m1, ud](v2, v5, v10, v3)
  (v12) = vslide1down.vx[e1m1, ud, fm](v2, v9, v8)
  (v14) = vid.v[e1m1, ud, fm](v2)
  (v17) = scalar[xmul=1, imm=0x0000000000000001]()
  (v16) = vand.vx[e1m1, ud, fm](v2, v14, v17)
  (v19) = vmseq.vx[e1m1, ud, fm](v2, v16, v8)
  (v21) = vmerge.vvm[e1m1, ud](v2, v9, v12, v19)
  (v23) = vmsne.vx[e1m1, ud, fm](v2, v21, v8)
  return (v23)

def _Z5Ne128Iu14__rvv_bool64_tu16__rvv_uint64m1_tET_T0_S1_(v0: vec<e1m1>, v1: vec<e1m1>) -> mask<mmul=1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmsne.vv[e1m1, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m1, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m1, ud](v2, v5, v10, v3)
  (v12) = vslide1up.vx[e1m1, ud, fm](v2, v9, v8)
  (v14) = vslide1down.vx[e1m1, ud, fm](v2, v9, v8)
  (v16) = vid.v[e1m1, ud, fm](v2)
  (v19) = scalar[xmul=1, imm=0x0000000000000001]()
  (v18) = vand.vx[e1m1, ud, fm](v2, v16, v19)
  (v21) = vmseq.vx[e1m1, ud, fm](v2, v18, v8)
  (v23) = vmerge.vvm[e1m1, ud](v2, v12, v14, v21)
  (v25) = first_element_to_scalar[e1m1](v23)
  (v26) = vor.vv[e1m1, ud, fm](v2, v9, v23)
  (v28) = vmsne.vx[e1m1, ud, fm](v2, v26, v8)
  return (v28)

def _Z10Ne128UpperIu14__rvv_bool64_tu16__rvv_uint64m1_tET_T0_S1_(v0: vec<e1m1>, v1: vec<e1m1>) -> mask<mmul=1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vmsne.vv[e1m1, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m1, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m1, ud](v2, v5, v10, v3)
  (v12) = vslide1down.vx[e1m1, ud, fm](v2, v9, v8)
  (v14) = first_element_to_scalar[e1m1](v12)
  (v15) = vid.v[e1m1, ud, fm](v2)
  (v18) = scalar[xmul=1, imm=0x0000000000000001]()
  (v17) = vand.vx[e1m1, ud, fm](v2, v15, v18)
  (v20) = vmseq.vx[e1m1, ud, fm](v2, v17, v8)
  (v22) = vmerge.vvm[e1m1, ud](v2, v9, v12, v20)
  (v24) = vmsne.vx[e1m1, ud, fm](v2, v22, v8)
  return (v24)

def _Z5Lt128Iu14__rvv_bool32_tu16__rvv_uint64m2_tET_T0_S1_(v0: vec<e1m2>, v1: vec<e1m2>) -> mask<mmul=2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmseq.vv[e1m2, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m2, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m2, ud](v2, v5, v10, v3)
  (v12) = vmsltu.vv[e1m2, ud, fm](v2, v0, v1)
  (v14) = vmerge.vxm[e1m2, ud](v2, v5, v10, v12)
  (v16) = vslide1up.vx[e1m2, ud, fm](v2, v14, v8)
  (v18) = vand.vv[e1m2, ud, fm](v2, v9, v16)
  (v20) = vor.vv[e1m2, ud, fm](v2, v14, v18)
  (v22) = vslide1down.vx[e1m2, ud, fm](v2, v20, v8)
  (v24) = vid.v[e1m2, ud, fm](v2)
  (v27) = scalar[xmul=1, imm=0x0000000000000001]()
  (v26) = vand.vx[e1m2, ud, fm](v2, v24, v27)
  (v29) = vmseq.vx[e1m2, ud, fm](v2, v26, v8)
  (v31) = vmerge.vvm[e1m2, ud](v2, v20, v22, v29)
  (v33) = vmsne.vx[e1m2, ud, fm](v2, v31, v8)
  return (v33)

def _Z10Lt128UpperIu14__rvv_bool32_tu16__rvv_uint64m2_tET_T0_S1_(v0: vec<e1m2>, v1: vec<e1m2>) -> mask<mmul=2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmsltu.vv[e1m2, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m2, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m2, ud](v2, v5, v10, v3)
  (v12) = vslide1down.vx[e1m2, ud, fm](v2, v9, v8)
  (v14) = first_element_to_scalar[e1m2](v12)
  (v15) = vid.v[e1m2, ud, fm](v2)
  (v18) = scalar[xmul=1, imm=0x0000000000000001]()
  (v17) = vand.vx[e1m2, ud, fm](v2, v15, v18)
  (v20) = vmseq.vx[e1m2, ud, fm](v2, v17, v8)
  (v22) = vmerge.vvm[e1m2, ud](v2, v9, v12, v20)
  (v24) = vmsne.vx[e1m2, ud, fm](v2, v22, v8)
  return (v24)

def _Z5Eq128Iu14__rvv_bool32_tu16__rvv_uint64m2_tET_T0_S1_(v0: vec<e1m2>, v1: vec<e1m2>) -> mask<mmul=2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmseq.vv[e1m2, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m2, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m2, ud](v2, v5, v10, v3)
  (v12) = vslide1up.vx[e1m2, ud, fm](v2, v9, v8)
  (v14) = vslide1down.vx[e1m2, ud, fm](v2, v9, v8)
  (v16) = vid.v[e1m2, ud, fm](v2)
  (v19) = scalar[xmul=1, imm=0x0000000000000001]()
  (v18) = vand.vx[e1m2, ud, fm](v2, v16, v19)
  (v21) = vmseq.vx[e1m2, ud, fm](v2, v18, v8)
  (v23) = vmerge.vvm[e1m2, ud](v2, v12, v14, v21)
  (v25) = vand.vv[e1m2, ud, fm](v2, v9, v23)
  (v27) = first_element_to_scalar[e1m2](v25)
  (v28) = vmsne.vx[e1m2, ud, fm](v2, v25, v8)
  return (v28)

def _Z10Eq128UpperIu14__rvv_bool32_tu16__rvv_uint64m2_tET_T0_S1_(v0: vec<e1m2>, v1: vec<e1m2>) -> mask<mmul=2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmseq.vv[e1m2, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m2, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m2, ud](v2, v5, v10, v3)
  (v12) = vslide1down.vx[e1m2, ud, fm](v2, v9, v8)
  (v14) = vid.v[e1m2, ud, fm](v2)
  (v17) = scalar[xmul=1, imm=0x0000000000000001]()
  (v16) = vand.vx[e1m2, ud, fm](v2, v14, v17)
  (v19) = vmseq.vx[e1m2, ud, fm](v2, v16, v8)
  (v21) = vmerge.vvm[e1m2, ud](v2, v9, v12, v19)
  (v23) = vmsne.vx[e1m2, ud, fm](v2, v21, v8)
  return (v23)

def _Z5Ne128Iu14__rvv_bool32_tu16__rvv_uint64m2_tET_T0_S1_(v0: vec<e1m2>, v1: vec<e1m2>) -> mask<mmul=2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmsne.vv[e1m2, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m2, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m2, ud](v2, v5, v10, v3)
  (v12) = vslide1up.vx[e1m2, ud, fm](v2, v9, v8)
  (v14) = vslide1down.vx[e1m2, ud, fm](v2, v9, v8)
  (v16) = vid.v[e1m2, ud, fm](v2)
  (v19) = scalar[xmul=1, imm=0x0000000000000001]()
  (v18) = vand.vx[e1m2, ud, fm](v2, v16, v19)
  (v21) = vmseq.vx[e1m2, ud, fm](v2, v18, v8)
  (v23) = vmerge.vvm[e1m2, ud](v2, v12, v14, v21)
  (v25) = first_element_to_scalar[e1m2](v23)
  (v26) = vor.vv[e1m2, ud, fm](v2, v9, v23)
  (v28) = vmsne.vx[e1m2, ud, fm](v2, v26, v8)
  return (v28)

def _Z10Ne128UpperIu14__rvv_bool32_tu16__rvv_uint64m2_tET_T0_S1_(v0: vec<e1m2>, v1: vec<e1m2>) -> mask<mmul=2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vmsne.vv[e1m2, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m2, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m2, ud](v2, v5, v10, v3)
  (v12) = vslide1down.vx[e1m2, ud, fm](v2, v9, v8)
  (v14) = first_element_to_scalar[e1m2](v12)
  (v15) = vid.v[e1m2, ud, fm](v2)
  (v18) = scalar[xmul=1, imm=0x0000000000000001]()
  (v17) = vand.vx[e1m2, ud, fm](v2, v15, v18)
  (v20) = vmseq.vx[e1m2, ud, fm](v2, v17, v8)
  (v22) = vmerge.vvm[e1m2, ud](v2, v9, v12, v20)
  (v24) = vmsne.vx[e1m2, ud, fm](v2, v22, v8)
  return (v24)

def _Z5Lt128Iu14__rvv_bool16_tu16__rvv_uint64m4_tET_T0_S1_(v0: vec<e1m4>, v1: vec<e1m4>) -> mask<mmul=4>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmseq.vv[e1m4, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m4, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m4, ud](v2, v5, v10, v3)
  (v12) = vmsltu.vv[e1m4, ud, fm](v2, v0, v1)
  (v14) = vmerge.vxm[e1m4, ud](v2, v5, v10, v12)
  (v16) = vslide1up.vx[e1m4, ud, fm](v2, v14, v8)
  (v18) = vand.vv[e1m4, ud, fm](v2, v9, v16)
  (v20) = vor.vv[e1m4, ud, fm](v2, v14, v18)
  (v22) = vslide1down.vx[e1m4, ud, fm](v2, v20, v8)
  (v24) = vid.v[e1m4, ud, fm](v2)
  (v27) = scalar[xmul=1, imm=0x0000000000000001]()
  (v26) = vand.vx[e1m4, ud, fm](v2, v24, v27)
  (v29) = vmseq.vx[e1m4, ud, fm](v2, v26, v8)
  (v31) = vmerge.vvm[e1m4, ud](v2, v20, v22, v29)
  (v33) = vmsne.vx[e1m4, ud, fm](v2, v31, v8)
  return (v33)

def _Z10Lt128UpperIu14__rvv_bool16_tu16__rvv_uint64m4_tET_T0_S1_(v0: vec<e1m4>, v1: vec<e1m4>) -> mask<mmul=4>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmsltu.vv[e1m4, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m4, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m4, ud](v2, v5, v10, v3)
  (v12) = vslide1down.vx[e1m4, ud, fm](v2, v9, v8)
  (v14) = first_element_to_scalar[e1m4](v12)
  (v15) = vid.v[e1m4, ud, fm](v2)
  (v18) = scalar[xmul=1, imm=0x0000000000000001]()
  (v17) = vand.vx[e1m4, ud, fm](v2, v15, v18)
  (v20) = vmseq.vx[e1m4, ud, fm](v2, v17, v8)
  (v22) = vmerge.vvm[e1m4, ud](v2, v9, v12, v20)
  (v24) = vmsne.vx[e1m4, ud, fm](v2, v22, v8)
  return (v24)

def _Z5Eq128Iu14__rvv_bool16_tu16__rvv_uint64m4_tET_T0_S1_(v0: vec<e1m4>, v1: vec<e1m4>) -> mask<mmul=4>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmseq.vv[e1m4, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m4, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m4, ud](v2, v5, v10, v3)
  (v12) = vslide1up.vx[e1m4, ud, fm](v2, v9, v8)
  (v14) = vslide1down.vx[e1m4, ud, fm](v2, v9, v8)
  (v16) = vid.v[e1m4, ud, fm](v2)
  (v19) = scalar[xmul=1, imm=0x0000000000000001]()
  (v18) = vand.vx[e1m4, ud, fm](v2, v16, v19)
  (v21) = vmseq.vx[e1m4, ud, fm](v2, v18, v8)
  (v23) = vmerge.vvm[e1m4, ud](v2, v12, v14, v21)
  (v25) = vand.vv[e1m4, ud, fm](v2, v9, v23)
  (v27) = first_element_to_scalar[e1m4](v25)
  (v28) = vmsne.vx[e1m4, ud, fm](v2, v25, v8)
  return (v28)

def _Z10Eq128UpperIu14__rvv_bool16_tu16__rvv_uint64m4_tET_T0_S1_(v0: vec<e1m4>, v1: vec<e1m4>) -> mask<mmul=4>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmseq.vv[e1m4, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m4, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m4, ud](v2, v5, v10, v3)
  (v12) = vslide1down.vx[e1m4, ud, fm](v2, v9, v8)
  (v14) = vid.v[e1m4, ud, fm](v2)
  (v17) = scalar[xmul=1, imm=0x0000000000000001]()
  (v16) = vand.vx[e1m4, ud, fm](v2, v14, v17)
  (v19) = vmseq.vx[e1m4, ud, fm](v2, v16, v8)
  (v21) = vmerge.vvm[e1m4, ud](v2, v9, v12, v19)
  (v23) = vmsne.vx[e1m4, ud, fm](v2, v21, v8)
  return (v23)

def _Z5Ne128Iu14__rvv_bool16_tu16__rvv_uint64m4_tET_T0_S1_(v0: vec<e1m4>, v1: vec<e1m4>) -> mask<mmul=4>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmsne.vv[e1m4, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m4, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m4, ud](v2, v5, v10, v3)
  (v12) = vslide1up.vx[e1m4, ud, fm](v2, v9, v8)
  (v14) = vslide1down.vx[e1m4, ud, fm](v2, v9, v8)
  (v16) = vid.v[e1m4, ud, fm](v2)
  (v19) = scalar[xmul=1, imm=0x0000000000000001]()
  (v18) = vand.vx[e1m4, ud, fm](v2, v16, v19)
  (v21) = vmseq.vx[e1m4, ud, fm](v2, v18, v8)
  (v23) = vmerge.vvm[e1m4, ud](v2, v12, v14, v21)
  (v25) = first_element_to_scalar[e1m4](v23)
  (v26) = vor.vv[e1m4, ud, fm](v2, v9, v23)
  (v28) = vmsne.vx[e1m4, ud, fm](v2, v26, v8)
  return (v28)

def _Z10Ne128UpperIu14__rvv_bool16_tu16__rvv_uint64m4_tET_T0_S1_(v0: vec<e1m4>, v1: vec<e1m4>) -> mask<mmul=4>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vmsne.vv[e1m4, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m4, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m4, ud](v2, v5, v10, v3)
  (v12) = vslide1down.vx[e1m4, ud, fm](v2, v9, v8)
  (v14) = first_element_to_scalar[e1m4](v12)
  (v15) = vid.v[e1m4, ud, fm](v2)
  (v18) = scalar[xmul=1, imm=0x0000000000000001]()
  (v17) = vand.vx[e1m4, ud, fm](v2, v15, v18)
  (v20) = vmseq.vx[e1m4, ud, fm](v2, v17, v8)
  (v22) = vmerge.vvm[e1m4, ud](v2, v9, v12, v20)
  (v24) = vmsne.vx[e1m4, ud, fm](v2, v22, v8)
  return (v24)

def _Z5Lt128Iu13__rvv_bool8_tu16__rvv_uint64m8_tET_T0_S1_(v0: vec<e1m8>, v1: vec<e1m8>) -> mask<mmul=8>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmseq.vv[e1m8, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m8, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m8, ud](v2, v5, v10, v3)
  (v12) = vmsltu.vv[e1m8, ud, fm](v2, v0, v1)
  (v14) = vmerge.vxm[e1m8, ud](v2, v5, v10, v12)
  (v16) = vslide1up.vx[e1m8, ud, fm](v2, v14, v8)
  (v18) = vand.vv[e1m8, ud, fm](v2, v9, v16)
  (v20) = vor.vv[e1m8, ud, fm](v2, v14, v18)
  (v22) = vslide1down.vx[e1m8, ud, fm](v2, v20, v8)
  (v24) = vid.v[e1m8, ud, fm](v2)
  (v27) = scalar[xmul=1, imm=0x0000000000000001]()
  (v26) = vand.vx[e1m8, ud, fm](v2, v24, v27)
  (v29) = vmseq.vx[e1m8, ud, fm](v2, v26, v8)
  (v31) = vmerge.vvm[e1m8, ud](v2, v20, v22, v29)
  (v33) = vmsne.vx[e1m8, ud, fm](v2, v31, v8)
  return (v33)

def _Z10Lt128UpperIu13__rvv_bool8_tu16__rvv_uint64m8_tET_T0_S1_(v0: vec<e1m8>, v1: vec<e1m8>) -> mask<mmul=8>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmsltu.vv[e1m8, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m8, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m8, ud](v2, v5, v10, v3)
  (v12) = vslide1down.vx[e1m8, ud, fm](v2, v9, v8)
  (v14) = first_element_to_scalar[e1m8](v12)
  (v15) = vid.v[e1m8, ud, fm](v2)
  (v18) = scalar[xmul=1, imm=0x0000000000000001]()
  (v17) = vand.vx[e1m8, ud, fm](v2, v15, v18)
  (v20) = vmseq.vx[e1m8, ud, fm](v2, v17, v8)
  (v22) = vmerge.vvm[e1m8, ud](v2, v9, v12, v20)
  (v24) = vmsne.vx[e1m8, ud, fm](v2, v22, v8)
  return (v24)

def _Z5Eq128Iu13__rvv_bool8_tu16__rvv_uint64m8_tET_T0_S1_(v0: vec<e1m8>, v1: vec<e1m8>) -> mask<mmul=8>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmseq.vv[e1m8, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m8, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m8, ud](v2, v5, v10, v3)
  (v12) = vslide1up.vx[e1m8, ud, fm](v2, v9, v8)
  (v14) = vslide1down.vx[e1m8, ud, fm](v2, v9, v8)
  (v16) = vid.v[e1m8, ud, fm](v2)
  (v19) = scalar[xmul=1, imm=0x0000000000000001]()
  (v18) = vand.vx[e1m8, ud, fm](v2, v16, v19)
  (v21) = vmseq.vx[e1m8, ud, fm](v2, v18, v8)
  (v23) = vmerge.vvm[e1m8, ud](v2, v12, v14, v21)
  (v25) = vand.vv[e1m8, ud, fm](v2, v9, v23)
  (v27) = first_element_to_scalar[e1m8](v25)
  (v28) = vmsne.vx[e1m8, ud, fm](v2, v25, v8)
  return (v28)

def _Z10Eq128UpperIu13__rvv_bool8_tu16__rvv_uint64m8_tET_T0_S1_(v0: vec<e1m8>, v1: vec<e1m8>) -> mask<mmul=8>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmseq.vv[e1m8, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m8, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m8, ud](v2, v5, v10, v3)
  (v12) = vslide1down.vx[e1m8, ud, fm](v2, v9, v8)
  (v14) = vid.v[e1m8, ud, fm](v2)
  (v17) = scalar[xmul=1, imm=0x0000000000000001]()
  (v16) = vand.vx[e1m8, ud, fm](v2, v14, v17)
  (v19) = vmseq.vx[e1m8, ud, fm](v2, v16, v8)
  (v21) = vmerge.vvm[e1m8, ud](v2, v9, v12, v19)
  (v23) = vmsne.vx[e1m8, ud, fm](v2, v21, v8)
  return (v23)

def _Z5Ne128Iu13__rvv_bool8_tu16__rvv_uint64m8_tET_T0_S1_(v0: vec<e1m8>, v1: vec<e1m8>) -> mask<mmul=8>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmsne.vv[e1m8, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m8, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m8, ud](v2, v5, v10, v3)
  (v12) = vslide1up.vx[e1m8, ud, fm](v2, v9, v8)
  (v14) = vslide1down.vx[e1m8, ud, fm](v2, v9, v8)
  (v16) = vid.v[e1m8, ud, fm](v2)
  (v19) = scalar[xmul=1, imm=0x0000000000000001]()
  (v18) = vand.vx[e1m8, ud, fm](v2, v16, v19)
  (v21) = vmseq.vx[e1m8, ud, fm](v2, v18, v8)
  (v23) = vmerge.vvm[e1m8, ud](v2, v12, v14, v21)
  (v25) = first_element_to_scalar[e1m8](v23)
  (v26) = vor.vv[e1m8, ud, fm](v2, v9, v23)
  (v28) = vmsne.vx[e1m8, ud, fm](v2, v26, v8)
  return (v28)

def _Z10Ne128UpperIu13__rvv_bool8_tu16__rvv_uint64m8_tET_T0_S1_(v0: vec<e1m8>, v1: vec<e1m8>) -> mask<mmul=8>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vmsne.vv[e1m8, ud, fm](v2, v0, v1)
  (v8) = scalar[xmul=1, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[e1m8, ud](v2, v8)
  (v10) = scalar[xmul=1, imm=0xffffffffffffffff]()
  (v9) = vmerge.vxm[e1m8, ud](v2, v5, v10, v3)
  (v12) = vslide1down.vx[e1m8, ud, fm](v2, v9, v8)
  (v14) = first_element_to_scalar[e1m8](v12)
  (v15) = vid.v[e1m8, ud, fm](v2)
  (v18) = scalar[xmul=1, imm=0x0000000000000001]()
  (v17) = vand.vx[e1m8, ud, fm](v2, v15, v18)
  (v20) = vmseq.vx[e1m8, ud, fm](v2, v17, v8)
  (v22) = vmerge.vvm[e1m8, ud](v2, v9, v12, v20)
  (v24) = vmsne.vx[e1m8, ud, fm](v2, v22, v8)
  return (v24)

