; ModuleID = 'zero_extend_resize_bit_cast.cpp'
source_filename = "zero_extend_resize_bit_cast.cpp"
target datalayout = "e-m:e-p:64:64-i64:64-i128:128-n32:64-S128"
target triple = "riscv64-unknown-linux-gnu"

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_uint8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_uint8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_uint8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_uint8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu17__rvv_uint16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu17__rvv_uint16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu17__rvv_uint32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint64m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int8mf8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int8mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int8mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu14__rvv_int8m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu14__rvv_int8m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu14__rvv_int8m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu14__rvv_int8m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_int16mf4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_int16mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int16m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int16m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int16m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int16m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_int32mf2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int32m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int32m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int32m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int32m8_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int64m1_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int64m2_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int64m4_tET_T0_ = comdat any

$_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int64m8_tET_T0_ = comdat any

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %3 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %4, <vscale x 1 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %5, <vscale x 1 x i8> %4, i64 %2)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %3, <vscale x 1 x i8> %0, <vscale x 1 x i1> %6, i64 %2)
  ret <vscale x 1 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %6, <vscale x 1 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %7, <vscale x 1 x i8> %6, i64 %4)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %5, <vscale x 1 x i8> %2, <vscale x 1 x i1> %8, i64 %4)
  ret <vscale x 1 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %7, <vscale x 1 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %6, <vscale x 1 x i8> %3, <vscale x 1 x i1> %9, i64 %5)
  ret <vscale x 1 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %7, <vscale x 1 x i8> %4, <vscale x 1 x i1> %10, i64 %6)
  ret <vscale x 1 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %7, <vscale x 1 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %6, <vscale x 1 x i8> %3, <vscale x 1 x i1> %9, i64 %5)
  ret <vscale x 1 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %7, <vscale x 1 x i8> %4, <vscale x 1 x i1> %10, i64 %6)
  ret <vscale x 1 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %7, i64 0)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %10)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %10)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9, i64 %10, i64 3)
  %14 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %13, <vscale x 1 x i8> %12, i64 %10)
  %15 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %11, <vscale x 1 x i8> %8, <vscale x 1 x i1> %14, i64 %10)
  ret <vscale x 1 x i8> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %7, <vscale x 1 x i8> %4, <vscale x 1 x i1> %10, i64 %6)
  ret <vscale x 1 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %7, i64 0)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %10)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %10)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9, i64 %10, i64 3)
  %14 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %13, <vscale x 1 x i8> %12, i64 %10)
  %15 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %11, <vscale x 1 x i8> %8, <vscale x 1 x i1> %14, i64 %10)
  ret <vscale x 1 x i8> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %7, i64 0)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %10)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %10)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9, i64 %10, i64 3)
  %14 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %13, <vscale x 1 x i8> %12, i64 %10)
  %15 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %11, <vscale x 1 x i8> %8, <vscale x 1 x i1> %14, i64 %10)
  ret <vscale x 1 x i8> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %3 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %4, <vscale x 1 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %5, <vscale x 1 x i8> %4, i64 %2)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %3, <vscale x 1 x i8> %0, <vscale x 1 x i1> %6, i64 %2)
  ret <vscale x 1 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %6, <vscale x 1 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %7, <vscale x 1 x i8> %6, i64 %4)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %5, <vscale x 1 x i8> %2, <vscale x 1 x i1> %8, i64 %4)
  ret <vscale x 1 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %7, <vscale x 1 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %6, <vscale x 1 x i8> %3, <vscale x 1 x i1> %9, i64 %5)
  ret <vscale x 1 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %7, <vscale x 1 x i8> %4, <vscale x 1 x i1> %10, i64 %6)
  ret <vscale x 1 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %7, <vscale x 1 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %6, <vscale x 1 x i8> %3, <vscale x 1 x i1> %9, i64 %5)
  ret <vscale x 1 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %7, <vscale x 1 x i8> %4, <vscale x 1 x i1> %10, i64 %6)
  ret <vscale x 1 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %7, i64 0)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %10)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %10)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9, i64 %10, i64 3)
  %14 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %13, <vscale x 1 x i8> %12, i64 %10)
  %15 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %11, <vscale x 1 x i8> %8, <vscale x 1 x i1> %14, i64 %10)
  ret <vscale x 1 x i8> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %7, <vscale x 1 x i8> %4, <vscale x 1 x i1> %10, i64 %6)
  ret <vscale x 1 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %7, i64 0)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %10)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %10)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9, i64 %10, i64 3)
  %14 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %13, <vscale x 1 x i8> %12, i64 %10)
  %15 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %11, <vscale x 1 x i8> %8, <vscale x 1 x i1> %14, i64 %10)
  ret <vscale x 1 x i8> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %7, i64 0)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %10)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %10)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9, i64 %10, i64 3)
  %14 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %13, <vscale x 1 x i8> %12, i64 %10)
  %15 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %11, <vscale x 1 x i8> %8, <vscale x 1 x i1> %14, i64 %10)
  ret <vscale x 1 x i8> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  ret <vscale x 2 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %3 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %4, <vscale x 2 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %2)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %3, <vscale x 2 x i8> %0, <vscale x 2 x i1> %6, i64 %2)
  ret <vscale x 2 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  ret <vscale x 2 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  ret <vscale x 2 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %4, <vscale x 2 x i8> %2, <vscale x 2 x i1> %7, i64 %3)
  ret <vscale x 2 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  ret <vscale x 2 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  ret <vscale x 2 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  ret <vscale x 2 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  ret <vscale x 2 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  ret <vscale x 2 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  ret <vscale x 2 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %3 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %4, <vscale x 2 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %2)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %3, <vscale x 2 x i8> %0, <vscale x 2 x i1> %6, i64 %2)
  ret <vscale x 2 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  ret <vscale x 2 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  ret <vscale x 2 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %4, <vscale x 2 x i8> %2, <vscale x 2 x i1> %7, i64 %3)
  ret <vscale x 2 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  ret <vscale x 2 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  ret <vscale x 2 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  ret <vscale x 2 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  ret <vscale x 2 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  ret <vscale x 2 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  ret <vscale x 4 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %3 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %4, <vscale x 4 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %2)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %3, <vscale x 4 x i8> %0, <vscale x 4 x i1> %6, i64 %2)
  ret <vscale x 4 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  ret <vscale x 4 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  ret <vscale x 4 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  ret <vscale x 4 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  ret <vscale x 4 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  ret <vscale x 4 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  ret <vscale x 4 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  ret <vscale x 4 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %3 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %4, <vscale x 4 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %2)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %3, <vscale x 4 x i8> %0, <vscale x 4 x i1> %6, i64 %2)
  ret <vscale x 4 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  ret <vscale x 4 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  ret <vscale x 4 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  ret <vscale x 4 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  ret <vscale x 4 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  ret <vscale x 4 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  ret <vscale x 4 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  ret <vscale x 8 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  ret <vscale x 8 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  ret <vscale x 8 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  ret <vscale x 8 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  ret <vscale x 8 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  ret <vscale x 8 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  ret <vscale x 8 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  ret <vscale x 8 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  ret <vscale x 8 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  ret <vscale x 8 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  ret <vscale x 8 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  ret <vscale x 8 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  ret <vscale x 8 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  ret <vscale x 8 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  ret <vscale x 8 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  ret <vscale x 8 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  ret <vscale x 8 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  ret <vscale x 8 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  ret <vscale x 16 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  ret <vscale x 16 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  ret <vscale x 16 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  ret <vscale x 16 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  ret <vscale x 16 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  ret <vscale x 16 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  ret <vscale x 16 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  ret <vscale x 16 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  ret <vscale x 16 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  ret <vscale x 16 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  ret <vscale x 16 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  ret <vscale x 16 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  ret <vscale x 16 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  ret <vscale x 16 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  ret <vscale x 16 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  ret <vscale x 16 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  ret <vscale x 32 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  ret <vscale x 32 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  ret <vscale x 32 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  ret <vscale x 32 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  ret <vscale x 32 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  ret <vscale x 32 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  ret <vscale x 32 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  ret <vscale x 32 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  ret <vscale x 32 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  ret <vscale x 32 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  ret <vscale x 32 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  ret <vscale x 32 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  ret <vscale x 32 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  ret <vscale x 32 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  ret <vscale x 32 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  ret <vscale x 32 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  ret <vscale x 32 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  ret <vscale x 32 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  ret <vscale x 32 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  ret <vscale x 32 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  ret <vscale x 32 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  ret <vscale x 32 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  ret <vscale x 64 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  ret <vscale x 64 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  ret <vscale x 64 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  ret <vscale x 64 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  ret <vscale x 64 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  ret <vscale x 64 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  ret <vscale x 64 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  ret <vscale x 64 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  ret <vscale x 64 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  ret <vscale x 64 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  ret <vscale x 64 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  ret <vscale x 64 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  ret <vscale x 64 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  ret <vscale x 64 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  ret <vscale x 64 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  ret <vscale x 64 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  ret <vscale x 64 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  ret <vscale x 64 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  ret <vscale x 64 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  ret <vscale x 64 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 2 x i8> %9 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %3 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %4, <vscale x 2 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %2)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %3, <vscale x 2 x i8> %0, <vscale x 2 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 2 x i8> %7 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 2 x i8> %9 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 2 x i8> %10 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %4, <vscale x 2 x i8> %2, <vscale x 2 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 2 x i8> %8 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 2 x i8> %10 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 2 x i8> %14 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 2 x i8> %10 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 2 x i8> %14 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 2 x i8> %14 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 2 x i8> %9 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %3 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %4, <vscale x 2 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %2)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %3, <vscale x 2 x i8> %0, <vscale x 2 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 2 x i8> %7 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 2 x i8> %9 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 2 x i8> %10 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %4, <vscale x 2 x i8> %2, <vscale x 2 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 2 x i8> %8 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 2 x i8> %10 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 2 x i8> %14 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 2 x i8> %10 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 2 x i8> %14 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 2 x i8> %14 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %3 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %4, <vscale x 4 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %2)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %3, <vscale x 4 x i8> %0, <vscale x 4 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 4 x i8> %7 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %3 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %4, <vscale x 4 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %2)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %3, <vscale x 4 x i8> %0, <vscale x 4 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 4 x i8> %7 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 8 x i8> %7 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 8 x i8> %7 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 16 x i8> %7 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 16 x i8> %7 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 32 x i8> %7 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 32 x i8> %7 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 64 x i8> %9 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 64 x i8> %7 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 64 x i8> %9 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 64 x i8> %7 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %3 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %4, <vscale x 4 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %2)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %3, <vscale x 4 x i8> %0, <vscale x 4 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 4 x i8> %7 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %3 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %4, <vscale x 4 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %2)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %3, <vscale x 4 x i8> %0, <vscale x 4 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 4 x i8> %7 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 8 x i8> %7 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 8 x i8> %7 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 16 x i8> %7 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 16 x i8> %7 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 32 x i8> %7 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 32 x i8> %7 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 64 x i8> %9 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 64 x i8> %7 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 64 x i8> %9 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 64 x i8> %7 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 8 x i8> %7 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 8 x i8> %7 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 16 x i8> %7 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 16 x i8> %7 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 32 x i8> %7 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 32 x i8> %7 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 64 x i8> %9 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 64 x i8> %7 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 64 x i8> %9 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 64 x i8> %7 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %3 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %4, <vscale x 1 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %5, <vscale x 1 x i8> %4, i64 %2)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %3, <vscale x 1 x i8> %0, <vscale x 1 x i1> %6, i64 %2)
  ret <vscale x 1 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %6, <vscale x 1 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %7, <vscale x 1 x i8> %6, i64 %4)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %5, <vscale x 1 x i8> %2, <vscale x 1 x i1> %8, i64 %4)
  ret <vscale x 1 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %7, <vscale x 1 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %6, <vscale x 1 x i8> %3, <vscale x 1 x i1> %9, i64 %5)
  ret <vscale x 1 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %7, <vscale x 1 x i8> %4, <vscale x 1 x i1> %10, i64 %6)
  ret <vscale x 1 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %7, <vscale x 1 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %6, <vscale x 1 x i8> %3, <vscale x 1 x i1> %9, i64 %5)
  ret <vscale x 1 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %7, <vscale x 1 x i8> %4, <vscale x 1 x i1> %10, i64 %6)
  ret <vscale x 1 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %7, i64 0)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %10)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %10)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9, i64 %10, i64 3)
  %14 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %13, <vscale x 1 x i8> %12, i64 %10)
  %15 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %11, <vscale x 1 x i8> %8, <vscale x 1 x i1> %14, i64 %10)
  ret <vscale x 1 x i8> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %7, <vscale x 1 x i8> %4, <vscale x 1 x i1> %10, i64 %6)
  ret <vscale x 1 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %7, i64 0)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %10)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %10)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9, i64 %10, i64 3)
  %14 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %13, <vscale x 1 x i8> %12, i64 %10)
  %15 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %11, <vscale x 1 x i8> %8, <vscale x 1 x i1> %14, i64 %10)
  ret <vscale x 1 x i8> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %7, i64 0)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %10)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %10)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9, i64 %10, i64 3)
  %14 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %13, <vscale x 1 x i8> %12, i64 %10)
  %15 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %11, <vscale x 1 x i8> %8, <vscale x 1 x i1> %14, i64 %10)
  ret <vscale x 1 x i8> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %3 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %4, <vscale x 1 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %5, <vscale x 1 x i8> %4, i64 %2)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %3, <vscale x 1 x i8> %0, <vscale x 1 x i1> %6, i64 %2)
  ret <vscale x 1 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %6, <vscale x 1 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %7, <vscale x 1 x i8> %6, i64 %4)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %5, <vscale x 1 x i8> %2, <vscale x 1 x i1> %8, i64 %4)
  ret <vscale x 1 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %7, <vscale x 1 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %6, <vscale x 1 x i8> %3, <vscale x 1 x i1> %9, i64 %5)
  ret <vscale x 1 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %7, <vscale x 1 x i8> %4, <vscale x 1 x i1> %10, i64 %6)
  ret <vscale x 1 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %7, <vscale x 1 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %6, <vscale x 1 x i8> %3, <vscale x 1 x i1> %9, i64 %5)
  ret <vscale x 1 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %7, <vscale x 1 x i8> %4, <vscale x 1 x i1> %10, i64 %6)
  ret <vscale x 1 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %7, i64 0)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %10)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %10)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9, i64 %10, i64 3)
  %14 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %13, <vscale x 1 x i8> %12, i64 %10)
  %15 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %11, <vscale x 1 x i8> %8, <vscale x 1 x i1> %14, i64 %10)
  ret <vscale x 1 x i8> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %8, <vscale x 1 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %7, <vscale x 1 x i8> %4, <vscale x 1 x i1> %10, i64 %6)
  ret <vscale x 1 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %7, i64 0)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %10)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %10)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9, i64 %10, i64 3)
  %14 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %13, <vscale x 1 x i8> %12, i64 %10)
  %15 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %11, <vscale x 1 x i8> %8, <vscale x 1 x i1> %14, i64 %10)
  ret <vscale x 1 x i8> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %9, <vscale x 1 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %8, <vscale x 1 x i8> %5, <vscale x 1 x i1> %11, i64 %7)
  ret <vscale x 1 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %10, <vscale x 1 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %9, <vscale x 1 x i8> %6, <vscale x 1 x i1> %12, i64 %8)
  ret <vscale x 1 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %10 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %11, <vscale x 1 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9)
  %14 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %10, <vscale x 1 x i8> %7, <vscale x 1 x i1> %13, i64 %9)
  ret <vscale x 1 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8> %7, i64 0)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %11 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 0, i64 %10)
  %12 = tail call <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8> poison, i8 1, i64 %10)
  %13 = tail call <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8> %12, <vscale x 1 x i8> %11, i64 %9, i64 %10, i64 3)
  %14 = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> %13, <vscale x 1 x i8> %12, i64 %10)
  %15 = tail call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8> poison, <vscale x 1 x i8> %11, <vscale x 1 x i8> %8, <vscale x 1 x i1> %14, i64 %10)
  ret <vscale x 1 x i8> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  ret <vscale x 2 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %3 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %4, <vscale x 2 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %2)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %3, <vscale x 2 x i8> %0, <vscale x 2 x i1> %6, i64 %2)
  ret <vscale x 2 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  ret <vscale x 2 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  ret <vscale x 2 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %4, <vscale x 2 x i8> %2, <vscale x 2 x i1> %7, i64 %3)
  ret <vscale x 2 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  ret <vscale x 2 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  ret <vscale x 2 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  ret <vscale x 2 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  ret <vscale x 2 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  ret <vscale x 2 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  ret <vscale x 2 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %3 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %4, <vscale x 2 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %2)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %3, <vscale x 2 x i8> %0, <vscale x 2 x i1> %6, i64 %2)
  ret <vscale x 2 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  ret <vscale x 2 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  ret <vscale x 2 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %4, <vscale x 2 x i8> %2, <vscale x 2 x i1> %7, i64 %3)
  ret <vscale x 2 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  ret <vscale x 2 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  ret <vscale x 2 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  ret <vscale x 2 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  ret <vscale x 2 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  ret <vscale x 2 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  ret <vscale x 2 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  ret <vscale x 2 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  ret <vscale x 2 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  ret <vscale x 4 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %3 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %4, <vscale x 4 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %2)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %3, <vscale x 4 x i8> %0, <vscale x 4 x i1> %6, i64 %2)
  ret <vscale x 4 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  ret <vscale x 4 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  ret <vscale x 4 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  ret <vscale x 4 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  ret <vscale x 4 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  ret <vscale x 4 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  ret <vscale x 4 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  ret <vscale x 4 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %3 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %4, <vscale x 4 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %2)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %3, <vscale x 4 x i8> %0, <vscale x 4 x i1> %6, i64 %2)
  ret <vscale x 4 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  ret <vscale x 4 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  ret <vscale x 4 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  ret <vscale x 4 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  ret <vscale x 4 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  ret <vscale x 4 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  ret <vscale x 4 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  ret <vscale x 4 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  ret <vscale x 4 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i8> @_Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  ret <vscale x 4 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  ret <vscale x 8 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  ret <vscale x 8 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  ret <vscale x 8 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  ret <vscale x 8 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  ret <vscale x 8 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  ret <vscale x 8 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  ret <vscale x 8 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  ret <vscale x 8 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  ret <vscale x 8 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  ret <vscale x 8 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  ret <vscale x 8 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  ret <vscale x 8 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  ret <vscale x 8 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  ret <vscale x 8 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  ret <vscale x 8 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  ret <vscale x 8 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  ret <vscale x 8 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  ret <vscale x 8 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  ret <vscale x 8 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  ret <vscale x 8 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  ret <vscale x 16 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  ret <vscale x 16 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  ret <vscale x 16 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  ret <vscale x 16 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  ret <vscale x 16 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  ret <vscale x 16 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  ret <vscale x 16 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  ret <vscale x 16 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  ret <vscale x 16 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  ret <vscale x 16 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  ret <vscale x 16 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  ret <vscale x 16 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  ret <vscale x 16 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  ret <vscale x 16 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  ret <vscale x 16 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  ret <vscale x 16 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  ret <vscale x 16 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  ret <vscale x 16 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  ret <vscale x 32 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  ret <vscale x 32 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  ret <vscale x 32 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  ret <vscale x 32 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  ret <vscale x 32 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  ret <vscale x 32 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  ret <vscale x 32 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  ret <vscale x 32 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  ret <vscale x 32 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  ret <vscale x 32 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  ret <vscale x 32 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  ret <vscale x 32 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  ret <vscale x 32 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  ret <vscale x 32 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  ret <vscale x 32 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  ret <vscale x 32 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  ret <vscale x 32 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  ret <vscale x 32 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  ret <vscale x 32 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  ret <vscale x 32 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  ret <vscale x 32 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  ret <vscale x 32 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  ret <vscale x 32 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  ret <vscale x 32 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  ret <vscale x 64 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  ret <vscale x 64 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  ret <vscale x 64 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  ret <vscale x 64 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  ret <vscale x 64 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  ret <vscale x 64 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  ret <vscale x 64 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  ret <vscale x 64 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  ret <vscale x 64 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  ret <vscale x 64 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  ret <vscale x 64 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  ret <vscale x 64 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  ret <vscale x 64 x i8> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  ret <vscale x 64 x i8> %7
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  ret <vscale x 64 x i8> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  ret <vscale x 64 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  ret <vscale x 64 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  ret <vscale x 64 x i8> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  ret <vscale x 64 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  ret <vscale x 64 x i8> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  ret <vscale x 64 x i8> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  ret <vscale x 64 x i8> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 64 x i8> @_Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  ret <vscale x 64 x i8> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 2 x i8> %9 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %3 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %4, <vscale x 2 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %2)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %3, <vscale x 2 x i8> %0, <vscale x 2 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 2 x i8> %7 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 2 x i8> %9 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 2 x i8> %10 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %4, <vscale x 2 x i8> %2, <vscale x 2 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 2 x i8> %8 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 2 x i8> %10 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 2 x i8> %14 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 2 x i8> %10 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 2 x i8> %14 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 2 x i8> %14 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 2 x i8> %9 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %3 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %4, <vscale x 2 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %2)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %3, <vscale x 2 x i8> %0, <vscale x 2 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 2 x i8> %7 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %5, <vscale x 2 x i8> %2, <vscale x 2 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 2 x i8> %9 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 2 x i8> %10 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %5, <vscale x 2 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %6, <vscale x 2 x i8> %5, i64 %3)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %4, <vscale x 2 x i8> %2, <vscale x 2 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 2 x i8> %8 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 2 x i8> %10 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 2 x i8> %14 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %7, <vscale x 2 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %6, <vscale x 2 x i8> %3, <vscale x 2 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 2 x i8> %10 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 2 x i8> %14 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %8, <vscale x 2 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %7, <vscale x 2 x i8> %4, <vscale x 2 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 2 x i8> %11 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %9, <vscale x 2 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %8, <vscale x 2 x i8> %5, <vscale x 2 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 2 x i8> %12 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %10, <vscale x 2 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8)
  %13 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %9, <vscale x 2 x i8> %6, <vscale x 2 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 2 x i8> %13 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %10 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8> %11, <vscale x 2 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> %12, <vscale x 2 x i8> %11, i64 %9)
  %14 = tail call <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8> poison, <vscale x 2 x i8> %10, <vscale x 2 x i8> %7, <vscale x 2 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 2 x i8> %14 to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %3 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %4, <vscale x 4 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %2)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %3, <vscale x 4 x i8> %0, <vscale x 4 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 4 x i8> %7 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %3 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %4, <vscale x 4 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %2)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %3, <vscale x 4 x i8> %0, <vscale x 4 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 4 x i8> %7 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i16> @_Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 8 x i8> %7 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 8 x i8> %7 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 16 x i8> %7 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 16 x i8> %7 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 32 x i8> %7 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 32 x i8> %7 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 64 x i8> %9 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 64 x i8> %7 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 64 x i8> %9 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 64 x i8> %7 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 32 x i16> @_Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %3 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %4, <vscale x 4 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %2)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %3, <vscale x 4 x i8> %0, <vscale x 4 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 4 x i8> %7 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %3 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %4, <vscale x 4 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %2)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %3, <vscale x 4 x i8> %0, <vscale x 4 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 4 x i8> %7 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %5, <vscale x 4 x i8> %2, <vscale x 4 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 4 x i8> %9 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %5, <vscale x 4 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %6, <vscale x 4 x i8> %5, i64 %3)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %4, <vscale x 4 x i8> %2, <vscale x 4 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 4 x i8> %8 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %7, <vscale x 4 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %6, <vscale x 4 x i8> %3, <vscale x 4 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 4 x i8> %10 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %8, <vscale x 4 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %7, <vscale x 4 x i8> %4, <vscale x 4 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 4 x i8> %11 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %9, <vscale x 4 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7)
  %12 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %8, <vscale x 4 x i8> %5, <vscale x 4 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 4 x i8> %12 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i32> @_Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %9 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8> %10, <vscale x 4 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> %11, <vscale x 4 x i8> %10, i64 %8)
  %13 = tail call <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8> poison, <vscale x 4 x i8> %9, <vscale x 4 x i8> %6, <vscale x 4 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 4 x i8> %13 to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 8 x i8> %7 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 8 x i8> %7 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 16 x i8> %7 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 16 x i8> %7 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 32 x i8> %7 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 32 x i8> %7 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 64 x i8> %9 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 64 x i8> %7 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 64 x i8> %9 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 64 x i8> %7 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 16 x i32> @_Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 8 x i8> %7 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %4, <vscale x 8 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %2)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %3, <vscale x 8 x i8> %0, <vscale x 8 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 8 x i8> %7 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %5, <vscale x 8 x i8> %2, <vscale x 8 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 8 x i8> %9 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %5, <vscale x 8 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %6, <vscale x 8 x i8> %5, i64 %3)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %4, <vscale x 8 x i8> %2, <vscale x 8 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 8 x i8> %8 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %7, <vscale x 8 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %6, <vscale x 8 x i8> %3, <vscale x 8 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 8 x i8> %10 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %8, <vscale x 8 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6)
  %11 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %7, <vscale x 8 x i8> %4, <vscale x 8 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 8 x i8> %11 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 1 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %8 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> %9, <vscale x 8 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> %10, <vscale x 8 x i8> %9, i64 %7)
  %12 = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %8, <vscale x 8 x i8> %5, <vscale x 8 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 8 x i8> %12 to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 16 x i8> %7 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %3 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %4, <vscale x 16 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %2)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %3, <vscale x 16 x i8> %0, <vscale x 16 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 16 x i8> %7 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %5, <vscale x 16 x i8> %2, <vscale x 16 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 16 x i8> %9 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %10, <vscale x 16 x i8> %9, i64 %7)
  %12 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %8, <vscale x 16 x i8> %5, <vscale x 16 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 16 x i8> %12 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %5, <vscale x 16 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %6, <vscale x 16 x i8> %5, i64 %3)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %4, <vscale x 16 x i8> %2, <vscale x 16 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 16 x i8> %8 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %7, <vscale x 16 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5)
  %10 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %6, <vscale x 16 x i8> %3, <vscale x 16 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 16 x i8> %10 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 2 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %7 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> %8, <vscale x 16 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> %9, <vscale x 16 x i8> %8, i64 %6)
  %11 = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> %7, <vscale x 16 x i8> %4, <vscale x 16 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 16 x i8> %11 to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 32 x i8> %7 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %3 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %4, <vscale x 32 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %3, <vscale x 32 x i8> %0, <vscale x 32 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 32 x i8> %7 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %5, <vscale x 32 x i8> %2, <vscale x 32 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 32 x i8> %9 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %11, <vscale x 32 x i8> %10, i64 %8)
  %13 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %9, <vscale x 32 x i8> %6, <vscale x 32 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 32 x i8> %13 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %10, <vscale x 32 x i8> %9, i64 %7)
  %12 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %8, <vscale x 32 x i8> %5, <vscale x 32 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 32 x i8> %12 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %9, <vscale x 32 x i8> %8, i64 %6)
  %11 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %7, <vscale x 32 x i8> %4, <vscale x 32 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 32 x i8> %11 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %5, <vscale x 32 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %6, <vscale x 32 x i8> %5, i64 %3)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %4, <vscale x 32 x i8> %2, <vscale x 32 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 32 x i8> %8 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 4 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %6 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> %7, <vscale x 32 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> %8, <vscale x 32 x i8> %7, i64 %5)
  %10 = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> %6, <vscale x 32 x i8> %3, <vscale x 32 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 32 x i8> %10 to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_uint8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_uint8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_uint8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 64 x i8> %9 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_uint8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 64 x i8> %7 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu17__rvv_uint16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu17__rvv_uint16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu17__rvv_uint32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int8mf8_tET_T0_(<vscale x 1 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8> poison, <vscale x 1 x i8> %0, i64 0)
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 5)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int8mf4_tET_T0_(<vscale x 2 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %0, i64 0)
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int8mf2_tET_T0_(<vscale x 4 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %0, i64 0)
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu14__rvv_int8m1_tET_T0_(<vscale x 8 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %0, i64 0)
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu14__rvv_int8m2_tET_T0_(<vscale x 16 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %0, i64 0)
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu14__rvv_int8m4_tET_T0_(<vscale x 32 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %0, i64 0)
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %4)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %4)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3, i64 %4, i64 3)
  %8 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %5, <vscale x 64 x i8> %2, <vscale x 64 x i1> %8, i64 %4)
  %10 = bitcast <vscale x 64 x i8> %9 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %10
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu14__rvv_int8m8_tET_T0_(<vscale x 64 x i8> %0) local_unnamed_addr #0 comdat {
  %2 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %3 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %2)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %2)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %4, <vscale x 64 x i8> %3, i64 %2, i64 %2, i64 3)
  %6 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %2)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %3, <vscale x 64 x i8> %0, <vscale x 64 x i1> %6, i64 %2)
  %8 = bitcast <vscale x 64 x i8> %7 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %8
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_int16mf4_tET_T0_(<vscale x 1 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i16> %0 to <vscale x 2 x i8>
  %3 = tail call <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8> poison, <vscale x 2 x i8> %2, i64 0)
  %4 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %3, i64 0)
  %5 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %4, i64 0)
  %6 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %5, i64 0)
  %7 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %6, i64 0)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 6)
  %9 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %9)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %9)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8, i64 %9, i64 3)
  %13 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %12, <vscale x 64 x i8> %11, i64 %9)
  %14 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %10, <vscale x 64 x i8> %7, <vscale x 64 x i1> %13, i64 %9)
  %15 = bitcast <vscale x 64 x i8> %14 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %15
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_int16mf2_tET_T0_(<vscale x 2 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i16> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int16m1_tET_T0_(<vscale x 4 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i16> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int16m2_tET_T0_(<vscale x 8 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i16> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int16m4_tET_T0_(<vscale x 16 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i16> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int16m8_tET_T0_(<vscale x 32 x i16> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 32 x i16> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_int32mf2_tET_T0_(<vscale x 1 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i32> %0 to <vscale x 4 x i8>
  %3 = tail call <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8> poison, <vscale x 4 x i8> %2, i64 0)
  %4 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %3, i64 0)
  %5 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %4, i64 0)
  %6 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %5, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 7)
  %8 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %8)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %8)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7, i64 %8, i64 3)
  %12 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %11, <vscale x 64 x i8> %10, i64 %8)
  %13 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %9, <vscale x 64 x i8> %6, <vscale x 64 x i1> %12, i64 %8)
  %14 = bitcast <vscale x 64 x i8> %13 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %14
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int32m1_tET_T0_(<vscale x 2 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i32> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int32m2_tET_T0_(<vscale x 4 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i32> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int32m4_tET_T0_(<vscale x 8 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i32> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int32m8_tET_T0_(<vscale x 16 x i32> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 16 x i32> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %9
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int64m1_tET_T0_(<vscale x 1 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 1 x i64> %0 to <vscale x 8 x i8>
  %3 = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> %2, i64 0)
  %4 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %3, i64 0)
  %5 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %4, i64 0)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 0)
  %7 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %7)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %7)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6, i64 %7, i64 3)
  %11 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %10, <vscale x 64 x i8> %9, i64 %7)
  %12 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %8, <vscale x 64 x i8> %5, <vscale x 64 x i1> %11, i64 %7)
  %13 = bitcast <vscale x 64 x i8> %12 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %13
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int64m2_tET_T0_(<vscale x 2 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 2 x i64> %0 to <vscale x 16 x i8>
  %3 = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8> poison, <vscale x 16 x i8> %2, i64 0)
  %4 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %3, i64 0)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 1)
  %6 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %6)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %6)
  %9 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5, i64 %6, i64 3)
  %10 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %9, <vscale x 64 x i8> %8, i64 %6)
  %11 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %7, <vscale x 64 x i8> %4, <vscale x 64 x i1> %10, i64 %6)
  %12 = bitcast <vscale x 64 x i8> %11 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %12
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int64m4_tET_T0_(<vscale x 4 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 4 x i64> %0 to <vscale x 32 x i8>
  %3 = tail call <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8> poison, <vscale x 32 x i8> %2, i64 0)
  %4 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 2)
  %5 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %5)
  %7 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %5)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %7, <vscale x 64 x i8> %6, i64 %4, i64 %5, i64 3)
  %9 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %8, <vscale x 64 x i8> %7, i64 %5)
  %10 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %6, <vscale x 64 x i8> %3, <vscale x 64 x i1> %9, i64 %5)
  %11 = bitcast <vscale x 64 x i8> %10 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %11
}

; Function Attrs: mustprogress uwtable vscale_range(2,1024)
define weak_odr dso_local <vscale x 8 x i64> @_Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int64m8_tET_T0_(<vscale x 8 x i64> %0) local_unnamed_addr #0 comdat {
  %2 = bitcast <vscale x 8 x i64> %0 to <vscale x 64 x i8>
  %3 = tail call noundef i64 @llvm.riscv.vsetvlimax.i64(i64 0, i64 3)
  %4 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 0, i64 %3)
  %5 = tail call <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8> poison, i8 1, i64 %3)
  %6 = tail call <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8> %5, <vscale x 64 x i8> %4, i64 %3, i64 %3, i64 3)
  %7 = tail call <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> %6, <vscale x 64 x i8> %5, i64 %3)
  %8 = tail call <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8> poison, <vscale x 64 x i8> %4, <vscale x 64 x i8> %2, <vscale x 64 x i1> %7, i64 %3)
  %9 = bitcast <vscale x 64 x i8> %8 to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %9
}

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare i64 @llvm.riscv.vsetvlimax.i64(i64 immarg, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8.i64(<vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i8>, <vscale x 1 x i1>, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 1 x i8> @llvm.riscv.vmv.v.x.nxv1i8.i64(<vscale x 1 x i8>, i8, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i8.nxv1i8.i64(<vscale x 1 x i8>, <vscale x 1 x i8>, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 1 x i8> @llvm.riscv.vslideup.nxv1i8.i64(<vscale x 1 x i8>, <vscale x 1 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn 
declare <vscale x 1 x i8> @llvm.vector.extract.nxv1i8.nxv2i8(<vscale x 2 x i8>, i64 immarg) #2

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn 
declare <vscale x 2 x i8> @llvm.vector.extract.nxv2i8.nxv4i8(<vscale x 4 x i8>, i64 immarg) #2

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn 
declare <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8>, i64 immarg) #2

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn 
declare <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8>, i64 immarg) #2

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn 
declare <vscale x 16 x i8> @llvm.vector.extract.nxv16i8.nxv32i8(<vscale x 32 x i8>, i64 immarg) #2

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn 
declare <vscale x 32 x i8> @llvm.vector.extract.nxv32i8.nxv64i8(<vscale x 64 x i8>, i64 immarg) #2

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn 
declare <vscale x 2 x i8> @llvm.vector.insert.nxv2i8.nxv1i8(<vscale x 2 x i8>, <vscale x 1 x i8>, i64 immarg) #2

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 2 x i8> @llvm.riscv.vmerge.nxv2i8.nxv2i8.i64(<vscale x 2 x i8>, <vscale x 2 x i8>, <vscale x 2 x i8>, <vscale x 2 x i1>, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 2 x i8> @llvm.riscv.vmv.v.x.nxv2i8.i64(<vscale x 2 x i8>, i8, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i8.nxv2i8.i64(<vscale x 2 x i8>, <vscale x 2 x i8>, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 2 x i8> @llvm.riscv.vslideup.nxv2i8.i64(<vscale x 2 x i8>, <vscale x 2 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn 
declare <vscale x 4 x i8> @llvm.vector.insert.nxv4i8.nxv2i8(<vscale x 4 x i8>, <vscale x 2 x i8>, i64 immarg) #2

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 4 x i8> @llvm.riscv.vmerge.nxv4i8.nxv4i8.i64(<vscale x 4 x i8>, <vscale x 4 x i8>, <vscale x 4 x i8>, <vscale x 4 x i1>, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 4 x i8> @llvm.riscv.vmv.v.x.nxv4i8.i64(<vscale x 4 x i8>, i8, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i8.nxv4i8.i64(<vscale x 4 x i8>, <vscale x 4 x i8>, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 4 x i8> @llvm.riscv.vslideup.nxv4i8.i64(<vscale x 4 x i8>, <vscale x 4 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn 
declare <vscale x 8 x i8> @llvm.vector.insert.nxv8i8.nxv4i8(<vscale x 8 x i8>, <vscale x 4 x i8>, i64 immarg) #2

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i1>, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8>, i8, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8>, <vscale x 8 x i8>, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8>, <vscale x 8 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn 
declare <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8>, <vscale x 8 x i8>, i64 immarg) #2

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8>, <vscale x 16 x i8>, <vscale x 16 x i8>, <vscale x 16 x i1>, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8>, i8, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8>, <vscale x 16 x i8>, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8>, <vscale x 16 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn 
declare <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv16i8(<vscale x 32 x i8>, <vscale x 16 x i8>, i64 immarg) #2

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8>, <vscale x 32 x i8>, <vscale x 32 x i8>, <vscale x 32 x i1>, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8>, i8, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8>, <vscale x 32 x i8>, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8>, <vscale x 32 x i8>, i64, i64, i64 immarg) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind speculatable willreturn 
declare <vscale x 64 x i8> @llvm.vector.insert.nxv64i8.nxv32i8(<vscale x 64 x i8>, <vscale x 32 x i8>, i64 immarg) #2

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 64 x i8> @llvm.riscv.vmerge.nxv64i8.nxv64i8.i64(<vscale x 64 x i8>, <vscale x 64 x i8>, <vscale x 64 x i8>, <vscale x 64 x i1>, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 64 x i8> @llvm.riscv.vmv.v.x.nxv64i8.i64(<vscale x 64 x i8>, i8, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 64 x i1> @llvm.riscv.vmseq.nxv64i8.nxv64i8.i64(<vscale x 64 x i8>, <vscale x 64 x i8>, i64) #1

; Function Attrs: mustprogress nocallback nofree nosync nounwind willreturn 
declare <vscale x 64 x i8> @llvm.riscv.vslideup.nxv64i8.i64(<vscale x 64 x i8>, <vscale x 64 x i8>, i64, i64, i64 immarg) #1

attributes #0 = { mustprogress uwtable vscale_range(2,1024) "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="sifive-x280" "target-features"="+64bit,+a,+c,+d,+f,+m,+relax,+v,+zicsr,+zifencei,+zmmul,+zve32f,+zve32x,+zve64d,+zve64f,+zve64x,+zvl128b,+zvl32b,+zvl64b,-b,-e,-experimental-smmpm,-experimental-smnpm,-experimental-ssnpm,-experimental-sspm,-experimental-ssqosid,-experimental-supm,-experimental-zacas,-experimental-zalasr,-experimental-zicfilp,-experimental-zicfiss,-h,-shcounterenw,-shgatpa,-shtvala,-shvsatpa,-shvstvala,-shvstvecd,-smaia,-smcdeleg,-smcsrind,-smepmp,-smstateen,-ssaia,-ssccfg,-ssccptr,-sscofpmf,-sscounterenw,-sscsrind,-ssstateen,-ssstrict,-sstc,-sstvala,-sstvecd,-ssu64xl,-svade,-svadu,-svbare,-svinval,-svnapot,-svpbmt,-xcvalu,-xcvbi,-xcvbitmanip,-xcvelw,-xcvmac,-xcvmem,-xcvsimd,-xsfcease,-xsfvcp,-xsfvfnrclipxfqf,-xsfvfwmaccqqq,-xsfvqmaccdod,-xsfvqmaccqoq,-xsifivecdiscarddlone,-xsifivecflushdlone,-xtheadba,-xtheadbb,-xtheadbs,-xtheadcmo,-xtheadcondmov,-xtheadfmemidx,-xtheadmac,-xtheadmemidx,-xtheadmempair,-xtheadsync,-xtheadvdot,-xventanacondops,-xwchc,-za128rs,-za64rs,-zaamo,-zabha,-zalrsc,-zama16b,-zawrs,-zba,-zbb,-zbc,-zbkb,-zbkc,-zbkx,-zbs,-zca,-zcb,-zcd,-zce,-zcf,-zcmop,-zcmp,-zcmt,-zdinx,-zfa,-zfbfmin,-zfh,-zfhmin,-zfinx,-zhinx,-zhinxmin,-zic64b,-zicbom,-zicbop,-zicboz,-ziccamoa,-ziccif,-zicclsm,-ziccrse,-zicntr,-zicond,-zihintntl,-zihintpause,-zihpm,-zimop,-zk,-zkn,-zknd,-zkne,-zknh,-zkr,-zks,-zksed,-zksh,-zkt,-ztso,-zvbb,-zvbc,-zvfbfmin,-zvfbfwma,-zvfh,-zvfhmin,-zvkb,-zvkg,-zvkn,-zvknc,-zvkned,-zvkng,-zvknha,-zvknhb,-zvks,-zvksc,-zvksed,-zvksg,-zvksh,-zvkt,-zvl1024b,-zvl16384b,-zvl2048b,-zvl256b,-zvl32768b,-zvl4096b,-zvl512b,-zvl65536b,-zvl8192b" }
attributes #1 = { mustprogress nocallback nofree nosync nounwind willreturn  }
attributes #2 = { mustprogress nocallback nofree nosync nounwind speculatable willreturn  }

!llvm.module.flags = !{!0, !1, !2, !4, !5, !6, !7}
!llvm.ident = !{!8}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 1, !"target-abi", !"lp64d"}
!2 = !{i32 6, !"riscv-isa", !3}
!3 = !{!"rv64i2p1_m2p0_a2p1_f2p2_d2p2_c2p0_v1p0_zicsr2p0_zifencei2p0_zmmul1p0_zve32f1p0_zve32x1p0_zve64d1p0_zve64f1p0_zve64x1p0_zvl128b1p0_zvl32b1p0_zvl64b1p0"}
!4 = !{i32 8, !"PIC Level", i32 2}
!5 = !{i32 7, !"PIE Level", i32 2}
!6 = !{i32 7, !"uwtable", i32 2}
!7 = !{i32 8, !"SmallDataLimit", i32 8}
!8 = !{!"clang version 19.1.0-rc2"}
