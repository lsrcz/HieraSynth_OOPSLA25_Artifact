def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8mf8>:
  (v1) = vsetvlmax[mmul=1, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=1](v1)
  (v9) = vslideup.vx[ef8mf8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf8, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8mf8, src=ef8mf4, idx=0](v0)
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vsetvlmax[mmul=1, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf8, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf8, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=2](v2)
  (v11) = vslideup.vx[ef8mf8, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf8, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf8, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8mf4, src=ef8mf2, idx=0](v0)
  (v2) = vget[part=ef8mf8, src=ef8mf4, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=1, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf8, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf8, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf8, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf8, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf8, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vget[part=ef8mf8, src=ef8mf4, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=1, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf8, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf8, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf8, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf8, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf8, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vget[part=ef8mf8, src=ef8mf4, idx=0](v1)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=1, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf8, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf8, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=2](v3)
  (v12) = vslideup.vx[ef8mf8, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf8, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf8, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vget[part=ef8mf8, src=ef8mf4, idx=0](v2)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=1, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf8, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf8, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=4](v4)
  (v13) = vslideup.vx[ef8mf8, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf8, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf8, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=8](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=16](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=32, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=32](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vget[part=ef8mf8, src=ef8mf4, idx=0](v6)
  (v8) = vsetvlmax[mmul=64, tama]()
  (v9) = vsetvlmax[mmul=1, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8mf8, ud](v9, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8mf8, ud](v9, v16)
  (v18) = vl_to_scalar[mmul=64](v8)
  (v17) = vslideup.vx[ef8mf8, pd, fm](v9, v10, v18, v14)
  (v20) = vmseq.vv[ef8mf8, ud, fm](v9, v17, v14)
  (v22) = vmerge.vvm[ef8mf8, ud](v9, v10, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vget[part=ef8mf8, src=ef8mf4, idx=0](v2)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=1, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf8, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf8, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=4](v4)
  (v13) = vslideup.vx[ef8mf8, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf8, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf8, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=8](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=16](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=32, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=32](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vget[part=ef8mf8, src=ef8mf4, idx=0](v6)
  (v8) = vsetvlmax[mmul=64, tama]()
  (v9) = vsetvlmax[mmul=1, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8mf8, ud](v9, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8mf8, ud](v9, v16)
  (v18) = vl_to_scalar[mmul=64](v8)
  (v17) = vslideup.vx[ef8mf8, pd, fm](v9, v10, v18, v14)
  (v20) = vmseq.vv[ef8mf8, ud, fm](v9, v17, v14)
  (v22) = vmerge.vvm[ef8mf8, ud](v9, v10, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=8](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=16](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=32, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=32](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vget[part=ef8mf8, src=ef8mf4, idx=0](v6)
  (v8) = vsetvlmax[mmul=64, tama]()
  (v9) = vsetvlmax[mmul=1, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8mf8, ud](v9, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8mf8, ud](v9, v16)
  (v18) = vl_to_scalar[mmul=64](v8)
  (v17) = vslideup.vx[ef8mf8, pd, fm](v9, v10, v18, v14)
  (v20) = vmseq.vv[ef8mf8, ud, fm](v9, v17, v14)
  (v22) = vmerge.vvm[ef8mf8, ud](v9, v10, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8mf8>:
  (v1) = vsetvlmax[mmul=1, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=1](v1)
  (v9) = vslideup.vx[ef8mf8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf8, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8mf8, src=ef8mf4, idx=0](v0)
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vsetvlmax[mmul=1, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf8, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf8, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=2](v2)
  (v11) = vslideup.vx[ef8mf8, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf8, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf8, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8mf4, src=ef8mf2, idx=0](v0)
  (v2) = vget[part=ef8mf8, src=ef8mf4, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=1, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf8, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf8, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf8, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf8, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf8, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vget[part=ef8mf8, src=ef8mf4, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=1, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf8, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf8, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf8, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf8, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf8, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vget[part=ef8mf8, src=ef8mf4, idx=0](v1)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=1, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf8, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf8, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=2](v3)
  (v12) = vslideup.vx[ef8mf8, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf8, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf8, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vget[part=ef8mf8, src=ef8mf4, idx=0](v2)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=1, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf8, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf8, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=4](v4)
  (v13) = vslideup.vx[ef8mf8, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf8, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf8, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=8](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=16](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=32, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=32](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vget[part=ef8mf8, src=ef8mf4, idx=0](v6)
  (v8) = vsetvlmax[mmul=64, tama]()
  (v9) = vsetvlmax[mmul=1, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8mf8, ud](v9, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8mf8, ud](v9, v16)
  (v18) = vl_to_scalar[mmul=64](v8)
  (v17) = vslideup.vx[ef8mf8, pd, fm](v9, v10, v18, v14)
  (v20) = vmseq.vv[ef8mf8, ud, fm](v9, v17, v14)
  (v22) = vmerge.vvm[ef8mf8, ud](v9, v10, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vget[part=ef8mf8, src=ef8mf4, idx=0](v2)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=1, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf8, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf8, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=4](v4)
  (v13) = vslideup.vx[ef8mf8, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf8, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf8, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=8](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=16](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=32, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=32](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vget[part=ef8mf8, src=ef8mf4, idx=0](v6)
  (v8) = vsetvlmax[mmul=64, tama]()
  (v9) = vsetvlmax[mmul=1, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8mf8, ud](v9, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8mf8, ud](v9, v16)
  (v18) = vl_to_scalar[mmul=64](v8)
  (v17) = vslideup.vx[ef8mf8, pd, fm](v9, v10, v18, v14)
  (v20) = vmseq.vv[ef8mf8, ud, fm](v9, v17, v14)
  (v22) = vmerge.vvm[ef8mf8, ud](v9, v10, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=8](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=16](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=32, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=32](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf8_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vget[part=ef8mf8, src=ef8mf4, idx=0](v6)
  (v8) = vsetvlmax[mmul=64, tama]()
  (v9) = vsetvlmax[mmul=1, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8mf8, ud](v9, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8mf8, ud](v9, v16)
  (v18) = vl_to_scalar[mmul=64](v8)
  (v17) = vslideup.vx[ef8mf8, pd, fm](v9, v10, v18, v14)
  (v20) = vmseq.vv[ef8mf8, ud, fm](v9, v17, v14)
  (v22) = vmerge.vvm[ef8mf8, ud](v9, v10, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8mf4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=1, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=1](v3)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8mf4>:
  (v1) = vsetvlmax[mmul=2, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=2](v1)
  (v9) = vslideup.vx[ef8mf4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf4, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8mf4, src=ef8mf2, idx=0](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=4](v2)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vsetvlmax[mmul=2, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=2](v2)
  (v10) = vslideup.vx[ef8mf4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8mf4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=1, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=1](v3)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8mf4>:
  (v1) = vsetvlmax[mmul=2, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=2](v1)
  (v9) = vslideup.vx[ef8mf4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf4, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8mf4, src=ef8mf2, idx=0](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=4](v2)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vsetvlmax[mmul=2, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=2](v2)
  (v10) = vslideup.vx[ef8mf4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf4_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8mf2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=1, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=1](v5)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8mf2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=2](v3)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8mf2>:
  (v1) = vsetvlmax[mmul=4, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=4](v1)
  (v9) = vslideup.vx[ef8mf2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf2, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=8](v2)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=2, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=2](v4)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8mf2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=1, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=1](v5)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8mf2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=2](v3)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8mf2>:
  (v1) = vsetvlmax[mmul=4, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=4](v1)
  (v9) = vslideup.vx[ef8mf2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf2, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=8](v2)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=2, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=2](v4)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint8mf2_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m1_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m2_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m4_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_uint8m8_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4mf4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=1, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=1](v3)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4mf4>:
  (v1) = vsetvlmax[mmul=2, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=2](v1)
  (v9) = vslideup.vx[ef8mf4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8mf4, src=ef8mf2, idx=0](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=4](v2)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vsetvlmax[mmul=2, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=2](v2)
  (v10) = vslideup.vx[ef8mf4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  (v23) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  (v23) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  (v23) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4mf4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=1, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=1](v3)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4mf4>:
  (v1) = vsetvlmax[mmul=2, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=2](v1)
  (v9) = vslideup.vx[ef8mf4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8mf4, src=ef8mf2, idx=0](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=4](v2)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vsetvlmax[mmul=2, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=2](v2)
  (v10) = vslideup.vx[ef8mf4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  (v23) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  (v23) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf4_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  (v23) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4mf2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=1, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=1](v5)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4mf2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=2](v3)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4mf2>:
  (v1) = vsetvlmax[mmul=4, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=4](v1)
  (v9) = vslideup.vx[ef8mf2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=8](v2)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=2, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=2](v4)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4mf2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=1, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=1](v5)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4mf2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=2](v3)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4mf2>:
  (v1) = vsetvlmax[mmul=4, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=4](v1)
  (v9) = vslideup.vx[ef8mf2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=8](v2)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=2, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=2](v4)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint16mf2_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m1, dest=ef4m1](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef4m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m1, dest=ef4m1](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef4m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef4m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef4m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef4m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m1, dest=ef4m1](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef4m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m1, dest=ef4m1](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef4m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef4m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef4m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef4m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m1_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m2, dest=ef4m2](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m2, dest=ef4m2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef4m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m2, dest=ef4m2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef4m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m2, dest=ef4m2](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef4m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef4m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef4m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef4m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef4m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m2, dest=ef4m2](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m2, dest=ef4m2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef4m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m2, dest=ef4m2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef4m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m2, dest=ef4m2](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef4m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef4m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef4m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef4m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef4m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m2_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m4, dest=ef4m4](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m4, dest=ef4m4](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m4, dest=ef4m4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m4, dest=ef4m4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef4m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m4, dest=ef4m4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef4m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m4, dest=ef4m4](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef4m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef4m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef4m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef4m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef4m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m4, dest=ef4m4](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m4, dest=ef4m4](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m4, dest=ef4m4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m4, dest=ef4m4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef4m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m4, dest=ef4m4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef4m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m4, dest=ef4m4](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef4m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef4m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef4m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef4m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef4m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m4_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  (v28) = vec_to_vec[src=ef8m8, dest=ef4m8](v26)
  return (v28)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m8, dest=ef4m8](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m8, dest=ef4m8](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m8, dest=ef4m8](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m8, dest=ef4m8](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m8, dest=ef4m8](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m8, dest=ef4m8](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  (v27) = vec_to_vec[src=ef8m8, dest=ef4m8](v25)
  return (v27)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef4m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef4m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef4m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef4m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef4m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef4m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef4m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef4m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef4m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef4m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef4m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  (v28) = vec_to_vec[src=ef8m8, dest=ef4m8](v26)
  return (v28)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m8, dest=ef4m8](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m8, dest=ef4m8](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m8, dest=ef4m8](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m8, dest=ef4m8](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m8, dest=ef4m8](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m8, dest=ef4m8](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  (v27) = vec_to_vec[src=ef8m8, dest=ef4m8](v25)
  return (v27)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef4m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef4m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef4m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef4m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef4m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef4m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef4m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef4m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef4m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef4m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint16m8_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef4m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2mf2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=1, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=1](v5)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2mf2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=2](v3)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2mf2>:
  (v1) = vsetvlmax[mmul=4, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=4](v1)
  (v9) = vslideup.vx[ef8mf2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=8](v2)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=2, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=2](v4)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2mf2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=1, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=1](v5)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2mf2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=2](v3)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2mf2>:
  (v1) = vsetvlmax[mmul=4, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=4](v1)
  (v9) = vslideup.vx[ef8mf2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=8](v2)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=2, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=2](v4)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu17__rvv_uint32mf2_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m1, dest=ef2m1](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef2m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m1, dest=ef2m1](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef2m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef2m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef2m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef2m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m1, dest=ef2m1](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef2m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m1, dest=ef2m1](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef2m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef2m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef2m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef2m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m1_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m2, dest=ef2m2](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m2, dest=ef2m2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef2m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m2, dest=ef2m2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef2m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m2, dest=ef2m2](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef2m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef2m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef2m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef2m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef2m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m2, dest=ef2m2](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m2, dest=ef2m2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef2m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m2, dest=ef2m2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef2m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m2, dest=ef2m2](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef2m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef2m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef2m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef2m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef2m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m2_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m4, dest=ef2m4](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m4, dest=ef2m4](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m4, dest=ef2m4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m4, dest=ef2m4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef2m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m4, dest=ef2m4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef2m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m4, dest=ef2m4](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef2m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef2m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef2m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef2m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef2m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef2m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef2m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef2m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m4, dest=ef2m4](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m4, dest=ef2m4](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m4, dest=ef2m4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m4, dest=ef2m4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef2m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m4, dest=ef2m4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef2m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m4, dest=ef2m4](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef2m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef2m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef2m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef2m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef2m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef2m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef2m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef2m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m4_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  (v28) = vec_to_vec[src=ef8m8, dest=ef2m8](v26)
  return (v28)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m8, dest=ef2m8](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m8, dest=ef2m8](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m8, dest=ef2m8](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m8, dest=ef2m8](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m8, dest=ef2m8](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m8, dest=ef2m8](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  (v27) = vec_to_vec[src=ef8m8, dest=ef2m8](v25)
  return (v27)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef2m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef2m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef2m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef2m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef2m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef2m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef2m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef2m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef2m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef2m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef2m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef2m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef2m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef2m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  (v28) = vec_to_vec[src=ef8m8, dest=ef2m8](v26)
  return (v28)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m8, dest=ef2m8](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m8, dest=ef2m8](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m8, dest=ef2m8](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m8, dest=ef2m8](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m8, dest=ef2m8](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m8, dest=ef2m8](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  (v27) = vec_to_vec[src=ef8m8, dest=ef2m8](v25)
  return (v27)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef2m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef2m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef2m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef2m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef2m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef2m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef2m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef2m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef2m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef2m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef2m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef2m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef2m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint32m8_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef2m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m1, dest=e1m1](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=e1m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m1, dest=e1m1](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=e1m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=e1m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=e1m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=e1m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m1, dest=e1m1](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=e1m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m1, dest=e1m1](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=e1m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=e1m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=e1m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=e1m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m1_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m2, dest=e1m2](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m2, dest=e1m2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=e1m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m2, dest=e1m2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=e1m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m2, dest=e1m2](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=e1m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=e1m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=e1m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=e1m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=e1m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m2, dest=e1m2](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m2, dest=e1m2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=e1m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m2, dest=e1m2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=e1m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m2, dest=e1m2](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=e1m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=e1m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=e1m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=e1m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=e1m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m2_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m4, dest=e1m4](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m4, dest=e1m4](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m4, dest=e1m4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m4, dest=e1m4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=e1m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m4, dest=e1m4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=e1m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m4, dest=e1m4](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=e1m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=e1m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=e1m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=e1m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=e1m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=e1m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=e1m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=e1m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m4, dest=e1m4](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m4, dest=e1m4](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m4, dest=e1m4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m4, dest=e1m4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=e1m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m4, dest=e1m4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=e1m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m4, dest=e1m4](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=e1m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=e1m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=e1m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=e1m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=e1m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=e1m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=e1m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=e1m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m4_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  (v28) = vec_to_vec[src=ef8m8, dest=e1m8](v26)
  return (v28)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m8, dest=e1m8](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m8, dest=e1m8](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m8, dest=e1m8](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m8, dest=e1m8](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m8, dest=e1m8](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m8, dest=e1m8](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  (v27) = vec_to_vec[src=ef8m8, dest=e1m8](v25)
  return (v27)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=e1m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=e1m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=e1m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=e1m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=e1m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=e1m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=e1m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=e1m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=e1m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=e1m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=e1m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=e1m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=e1m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=e1m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  (v28) = vec_to_vec[src=ef8m8, dest=e1m8](v26)
  return (v28)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m8, dest=e1m8](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m8, dest=e1m8](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m8, dest=e1m8](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m8, dest=e1m8](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m8, dest=e1m8](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m8, dest=e1m8](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  (v27) = vec_to_vec[src=ef8m8, dest=e1m8](v25)
  return (v27)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=e1m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=e1m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=e1m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=e1m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=e1m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=e1m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=e1m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=e1m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=e1m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=e1m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=e1m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=e1m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=e1m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_uint64m8_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=e1m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8mf8>:
  (v1) = vsetvlmax[mmul=1, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=1](v1)
  (v9) = vslideup.vx[ef8mf8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf8, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8mf8, src=ef8mf4, idx=0](v0)
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vsetvlmax[mmul=1, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf8, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf8, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=2](v2)
  (v11) = vslideup.vx[ef8mf8, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf8, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf8, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8mf4, src=ef8mf2, idx=0](v0)
  (v2) = vget[part=ef8mf8, src=ef8mf4, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=1, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf8, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf8, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf8, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf8, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf8, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vget[part=ef8mf8, src=ef8mf4, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=1, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf8, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf8, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf8, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf8, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf8, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vget[part=ef8mf8, src=ef8mf4, idx=0](v1)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=1, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf8, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf8, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=2](v3)
  (v12) = vslideup.vx[ef8mf8, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf8, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf8, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vget[part=ef8mf8, src=ef8mf4, idx=0](v2)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=1, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf8, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf8, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=4](v4)
  (v13) = vslideup.vx[ef8mf8, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf8, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf8, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=8](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=16](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=32, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=32](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vget[part=ef8mf8, src=ef8mf4, idx=0](v6)
  (v8) = vsetvlmax[mmul=64, tama]()
  (v9) = vsetvlmax[mmul=1, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8mf8, ud](v9, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8mf8, ud](v9, v16)
  (v18) = vl_to_scalar[mmul=64](v8)
  (v17) = vslideup.vx[ef8mf8, pd, fm](v9, v10, v18, v14)
  (v20) = vmseq.vv[ef8mf8, ud, fm](v9, v17, v14)
  (v22) = vmerge.vvm[ef8mf8, ud](v9, v10, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vget[part=ef8mf8, src=ef8mf4, idx=0](v2)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=1, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf8, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf8, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=4](v4)
  (v13) = vslideup.vx[ef8mf8, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf8, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf8, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=8](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=16](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=32, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=32](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vget[part=ef8mf8, src=ef8mf4, idx=0](v6)
  (v8) = vsetvlmax[mmul=64, tama]()
  (v9) = vsetvlmax[mmul=1, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8mf8, ud](v9, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8mf8, ud](v9, v16)
  (v18) = vl_to_scalar[mmul=64](v8)
  (v17) = vslideup.vx[ef8mf8, pd, fm](v9, v10, v18, v14)
  (v20) = vmseq.vv[ef8mf8, ud, fm](v9, v17, v14)
  (v22) = vmerge.vvm[ef8mf8, ud](v9, v10, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=8](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=16](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=32, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=32](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vget[part=ef8mf8, src=ef8mf4, idx=0](v6)
  (v8) = vsetvlmax[mmul=64, tama]()
  (v9) = vsetvlmax[mmul=1, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8mf8, ud](v9, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8mf8, ud](v9, v16)
  (v18) = vl_to_scalar[mmul=64](v8)
  (v17) = vslideup.vx[ef8mf8, pd, fm](v9, v10, v18, v14)
  (v20) = vmseq.vv[ef8mf8, ud, fm](v9, v17, v14)
  (v22) = vmerge.vvm[ef8mf8, ud](v9, v10, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8mf8>:
  (v1) = vsetvlmax[mmul=1, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=1](v1)
  (v9) = vslideup.vx[ef8mf8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf8, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8mf8, src=ef8mf4, idx=0](v0)
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vsetvlmax[mmul=1, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf8, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf8, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=2](v2)
  (v11) = vslideup.vx[ef8mf8, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf8, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf8, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8mf4, src=ef8mf2, idx=0](v0)
  (v2) = vget[part=ef8mf8, src=ef8mf4, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=1, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf8, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf8, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf8, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf8, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf8, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vget[part=ef8mf8, src=ef8mf4, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=1, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf8, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf8, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf8, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf8, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf8, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8mf8>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vget[part=ef8mf8, src=ef8mf4, idx=0](v1)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=1, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf8, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf8, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=2](v3)
  (v12) = vslideup.vx[ef8mf8, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf8, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf8, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vget[part=ef8mf8, src=ef8mf4, idx=0](v2)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=1, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf8, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf8, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=4](v4)
  (v13) = vslideup.vx[ef8mf8, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf8, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf8, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=8](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=16](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=32, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=32](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vget[part=ef8mf8, src=ef8mf4, idx=0](v6)
  (v8) = vsetvlmax[mmul=64, tama]()
  (v9) = vsetvlmax[mmul=1, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8mf8, ud](v9, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8mf8, ud](v9, v16)
  (v18) = vl_to_scalar[mmul=64](v8)
  (v17) = vslideup.vx[ef8mf8, pd, fm](v9, v10, v18, v14)
  (v20) = vmseq.vv[ef8mf8, ud, fm](v9, v17, v14)
  (v22) = vmerge.vvm[ef8mf8, ud](v9, v10, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vget[part=ef8mf8, src=ef8mf4, idx=0](v2)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=1, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf8, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf8, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=4](v4)
  (v13) = vslideup.vx[ef8mf8, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf8, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf8, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=8](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=16](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=32, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=32](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vget[part=ef8mf8, src=ef8mf4, idx=0](v6)
  (v8) = vsetvlmax[mmul=64, tama]()
  (v9) = vsetvlmax[mmul=1, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8mf8, ud](v9, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8mf8, ud](v9, v16)
  (v18) = vl_to_scalar[mmul=64](v8)
  (v17) = vslideup.vx[ef8mf8, pd, fm](v9, v10, v18, v14)
  (v20) = vmseq.vv[ef8mf8, ud, fm](v9, v17, v14)
  (v22) = vmerge.vvm[ef8mf8, ud](v9, v10, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vget[part=ef8mf8, src=ef8mf4, idx=0](v3)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=1, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf8, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf8, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=8](v5)
  (v14) = vslideup.vx[ef8mf8, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf8, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf8, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vget[part=ef8mf8, src=ef8mf4, idx=0](v4)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=1, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf8, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf8, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=16](v6)
  (v15) = vslideup.vx[ef8mf8, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf8, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf8, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vget[part=ef8mf8, src=ef8mf4, idx=0](v5)
  (v7) = vsetvlmax[mmul=32, tama]()
  (v8) = vsetvlmax[mmul=1, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf8, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf8, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=32](v7)
  (v16) = vslideup.vx[ef8mf8, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf8, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf8, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf8_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8mf8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vget[part=ef8mf8, src=ef8mf4, idx=0](v6)
  (v8) = vsetvlmax[mmul=64, tama]()
  (v9) = vsetvlmax[mmul=1, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8mf8, ud](v9, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8mf8, ud](v9, v16)
  (v18) = vl_to_scalar[mmul=64](v8)
  (v17) = vslideup.vx[ef8mf8, pd, fm](v9, v10, v18, v14)
  (v20) = vmseq.vv[ef8mf8, ud, fm](v9, v17, v14)
  (v22) = vmerge.vvm[ef8mf8, ud](v9, v10, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8mf4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=1, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=1](v3)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8mf4>:
  (v1) = vsetvlmax[mmul=2, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=2](v1)
  (v9) = vslideup.vx[ef8mf4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf4, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8mf4, src=ef8mf2, idx=0](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=4](v2)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vsetvlmax[mmul=2, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=2](v2)
  (v10) = vslideup.vx[ef8mf4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8mf4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=1, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=1](v3)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8mf4>:
  (v1) = vsetvlmax[mmul=2, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=2](v1)
  (v9) = vslideup.vx[ef8mf4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf4, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8mf4, src=ef8mf2, idx=0](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=4](v2)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8mf4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vsetvlmax[mmul=2, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=2](v2)
  (v10) = vslideup.vx[ef8mf4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf4_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8mf4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8mf2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=1, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=1](v5)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8mf2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=2](v3)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8mf2>:
  (v1) = vsetvlmax[mmul=4, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=4](v1)
  (v9) = vslideup.vx[ef8mf2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf2, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=8](v2)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=2, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=2](v4)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8mf2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=1, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=1](v5)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8mf2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=2](v3)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8mf2>:
  (v1) = vsetvlmax[mmul=4, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=4](v1)
  (v9) = vslideup.vx[ef8mf2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf2, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=8](v2)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8mf2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=2, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=2](v4)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int8mf2_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8mf2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m1_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m2_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m4_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef8m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef8m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef8m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef8m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef8m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef8m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef8m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  return (v14)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu14__rvv_int8m8_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef8m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  return (v15)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4mf4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=1, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=1](v3)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4mf4>:
  (v1) = vsetvlmax[mmul=2, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=2](v1)
  (v9) = vslideup.vx[ef8mf4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8mf4, src=ef8mf2, idx=0](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=4](v2)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vsetvlmax[mmul=2, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=2](v2)
  (v10) = vslideup.vx[ef8mf4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  (v23) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  (v23) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  (v23) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4mf4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=1, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=1](v3)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4mf4>:
  (v1) = vsetvlmax[mmul=2, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=2](v1)
  (v9) = vslideup.vx[ef8mf4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8mf4, src=ef8mf2, idx=0](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vsetvlmax[mmul=2, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=4](v2)
  (v11) = vslideup.vx[ef8mf4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4mf4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vsetvlmax[mmul=2, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=2](v2)
  (v10) = vslideup.vx[ef8mf4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  (v23) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vget[part=ef8mf4, src=ef8mf2, idx=0](v1)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=2, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=4](v3)
  (v12) = vslideup.vx[ef8mf4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  (v23) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vget[part=ef8mf4, src=ef8mf2, idx=0](v2)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=2, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf4, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf4, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=8](v4)
  (v13) = vslideup.vx[ef8mf4, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf4, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf4, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vget[part=ef8mf4, src=ef8mf2, idx=0](v3)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=2, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf4, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf4, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=16](v5)
  (v14) = vslideup.vx[ef8mf4, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf4, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf4, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vget[part=ef8mf4, src=ef8mf2, idx=0](v4)
  (v6) = vsetvlmax[mmul=32, tama]()
  (v7) = vsetvlmax[mmul=2, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf4, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf4, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=32](v6)
  (v15) = vslideup.vx[ef8mf4, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf4, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf4, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf4_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4mf4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vget[part=ef8mf4, src=ef8mf2, idx=0](v5)
  (v7) = vsetvlmax[mmul=64, tama]()
  (v8) = vsetvlmax[mmul=2, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8mf4, ud](v8, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8mf4, ud](v8, v15)
  (v17) = vl_to_scalar[mmul=64](v7)
  (v16) = vslideup.vx[ef8mf4, pd, fm](v8, v9, v17, v13)
  (v19) = vmseq.vv[ef8mf4, ud, fm](v8, v16, v13)
  (v21) = vmerge.vvm[ef8mf4, ud](v8, v9, v6, v19)
  (v23) = vec_to_vec[src=ef8mf4, dest=ef4mf4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4mf2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=1, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=1](v5)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4mf2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=2](v3)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4mf2>:
  (v1) = vsetvlmax[mmul=4, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=4](v1)
  (v9) = vslideup.vx[ef8mf2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=8](v2)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=2, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=2](v4)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4mf2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=1, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=1](v5)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4mf2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=2](v3)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4mf2>:
  (v1) = vsetvlmax[mmul=4, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=4](v1)
  (v9) = vslideup.vx[ef8mf2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=8](v2)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4mf2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=2, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=2](v4)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int16mf2_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4mf2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef4mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m1, dest=ef4m1](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef4m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m1, dest=ef4m1](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef4m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef4m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef4m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef4m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m1, dest=ef4m1](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef4m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m1, dest=ef4m1](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef4m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef4m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef4m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef4m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef4m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef4m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m1_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef4m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m2, dest=ef4m2](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m2, dest=ef4m2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef4m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m2, dest=ef4m2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef4m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m2, dest=ef4m2](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef4m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef4m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef4m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef4m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef4m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m2, dest=ef4m2](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m2, dest=ef4m2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef4m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m2, dest=ef4m2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef4m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m2, dest=ef4m2](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef4m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef4m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef4m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef4m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef4m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef4m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m2_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef4m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m4, dest=ef4m4](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m4, dest=ef4m4](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m4, dest=ef4m4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m4, dest=ef4m4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef4m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m4, dest=ef4m4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef4m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m4, dest=ef4m4](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef4m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef4m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef4m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef4m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef4m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m4, dest=ef4m4](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m4, dest=ef4m4](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m4, dest=ef4m4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m4, dest=ef4m4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef4m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m4, dest=ef4m4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef4m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m4, dest=ef4m4](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef4m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef4m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef4m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef4m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef4m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef4m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m4_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef4m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  (v28) = vec_to_vec[src=ef8m8, dest=ef4m8](v26)
  return (v28)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m8, dest=ef4m8](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m8, dest=ef4m8](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m8, dest=ef4m8](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m8, dest=ef4m8](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m8, dest=ef4m8](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m8, dest=ef4m8](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  (v27) = vec_to_vec[src=ef8m8, dest=ef4m8](v25)
  return (v27)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef4m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef4m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef4m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef4m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef4m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef4m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef4m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef4m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef4m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef4m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef4m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef4m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  (v28) = vec_to_vec[src=ef8m8, dest=ef4m8](v26)
  return (v28)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef4m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m8, dest=ef4m8](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef4m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m8, dest=ef4m8](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef4m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m8, dest=ef4m8](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef4m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m8, dest=ef4m8](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef4m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m8, dest=ef4m8](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef4m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m8, dest=ef4m8](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  (v27) = vec_to_vec[src=ef8m8, dest=ef4m8](v25)
  return (v27)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef4m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef4m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef4m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef4m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef4m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef4m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef4m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef4m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef4m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef4m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef4m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int16m8_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef4m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef4m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2mf2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=1, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=1](v5)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2mf2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=2](v3)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2mf2>:
  (v1) = vsetvlmax[mmul=4, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=4](v1)
  (v9) = vslideup.vx[ef8mf2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=8](v2)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=2, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=2](v4)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2mf2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=1, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=1](v5)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2mf2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=2, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=2](v3)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2mf2>:
  (v1) = vsetvlmax[mmul=4, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8mf2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8mf2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=4](v1)
  (v9) = vslideup.vx[ef8mf2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8mf2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8mf2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8mf2, src=ef8m1, idx=0](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vsetvlmax[mmul=4, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8mf2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8mf2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=8](v2)
  (v11) = vslideup.vx[ef8mf2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8mf2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8mf2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2mf2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=2, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=2](v4)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vsetvlmax[mmul=4, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8mf2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8mf2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=4](v2)
  (v10) = vslideup.vx[ef8mf2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8mf2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vget[part=ef8mf2, src=ef8m1, idx=0](v1)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=4, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8mf2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8mf2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=8](v3)
  (v12) = vslideup.vx[ef8mf2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8mf2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8mf2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vget[part=ef8mf2, src=ef8m1, idx=0](v2)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=4, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8mf2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8mf2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=16](v4)
  (v13) = vslideup.vx[ef8mf2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8mf2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8mf2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vget[part=ef8mf2, src=ef8m1, idx=0](v3)
  (v5) = vsetvlmax[mmul=32, tama]()
  (v6) = vsetvlmax[mmul=4, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8mf2, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8mf2, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=32](v5)
  (v14) = vslideup.vx[ef8mf2, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8mf2, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8mf2, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu16__rvv_int32mf2_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2mf2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vget[part=ef8mf2, src=ef8m1, idx=0](v4)
  (v6) = vsetvlmax[mmul=64, tama]()
  (v7) = vsetvlmax[mmul=4, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8mf2, ud](v7, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8mf2, ud](v7, v14)
  (v16) = vl_to_scalar[mmul=64](v6)
  (v15) = vslideup.vx[ef8mf2, pd, fm](v7, v8, v16, v12)
  (v18) = vmseq.vv[ef8mf2, ud, fm](v7, v15, v12)
  (v20) = vmerge.vvm[ef8mf2, ud](v7, v8, v5, v18)
  (v22) = vec_to_vec[src=ef8mf2, dest=ef2mf2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m1, dest=ef2m1](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef2m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m1, dest=ef2m1](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef2m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef2m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef2m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef2m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m1, dest=ef2m1](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef2m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m1, dest=ef2m1](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=ef2m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef2m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef2m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=ef2m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=ef2m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=ef2m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m1_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=ef2m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m2, dest=ef2m2](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m2, dest=ef2m2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef2m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m2, dest=ef2m2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef2m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m2, dest=ef2m2](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef2m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef2m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef2m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef2m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef2m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m2, dest=ef2m2](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m2, dest=ef2m2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef2m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m2, dest=ef2m2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=ef2m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m2, dest=ef2m2](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef2m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef2m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=ef2m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef2m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=ef2m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=ef2m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m2_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=ef2m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m4, dest=ef2m4](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m4, dest=ef2m4](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m4, dest=ef2m4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m4, dest=ef2m4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef2m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m4, dest=ef2m4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef2m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m4, dest=ef2m4](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef2m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef2m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef2m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef2m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef2m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef2m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef2m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef2m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m4, dest=ef2m4](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m4, dest=ef2m4](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m4, dest=ef2m4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m4, dest=ef2m4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef2m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m4, dest=ef2m4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=ef2m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m4, dest=ef2m4](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef2m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef2m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef2m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=ef2m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef2m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef2m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=ef2m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=ef2m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m4_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=ef2m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  (v28) = vec_to_vec[src=ef8m8, dest=ef2m8](v26)
  return (v28)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m8, dest=ef2m8](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m8, dest=ef2m8](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m8, dest=ef2m8](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m8, dest=ef2m8](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m8, dest=ef2m8](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m8, dest=ef2m8](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  (v27) = vec_to_vec[src=ef8m8, dest=ef2m8](v25)
  return (v27)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef2m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef2m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef2m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef2m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef2m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef2m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef2m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef2m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef2m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef2m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef2m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef2m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef2m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef2m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<ef2m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  (v28) = vec_to_vec[src=ef8m8, dest=ef2m8](v26)
  return (v28)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<ef2m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m8, dest=ef2m8](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<ef2m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m8, dest=ef2m8](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<ef2m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m8, dest=ef2m8](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<ef2m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m8, dest=ef2m8](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<ef2m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m8, dest=ef2m8](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<ef2m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m8, dest=ef2m8](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  (v27) = vec_to_vec[src=ef8m8, dest=ef2m8](v25)
  return (v27)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef2m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef2m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef2m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef2m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef2m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=ef2m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef2m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef2m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef2m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef2m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=ef2m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=ef2m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=ef2m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int32m8_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<ef2m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=ef2m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m1, dest=e1m1](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=e1m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m1, dest=e1m1](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=e1m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=e1m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=e1m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=e1m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m1>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=1, tama]()
  (v8) = vsetvlmax[mmul=8, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m1, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m1, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=1](v7)
  (v15) = vslideup.vx[ef8m1, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m1, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m1, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m1, dest=e1m1](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m1>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=2, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=2](v5)
  (v13) = vslideup.vx[ef8m1, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m1>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=4, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=4](v3)
  (v11) = vslideup.vx[ef8m1, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=e1m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m1>:
  (v1) = vsetvlmax[mmul=8, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m1, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m1, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=8](v1)
  (v9) = vslideup.vx[ef8m1, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m1, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m1, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m1, dest=e1m1](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m1>:
  (v1) = vget[part=ef8m1, src=ef8m2, idx=0](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vsetvlmax[mmul=8, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m1, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m1, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=16](v2)
  (v11) = vslideup.vx[ef8m1, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m1, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m1, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m1, dest=e1m1](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m1>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m1>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=2, tama]()
  (v7) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m1, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=2](v6)
  (v14) = vslideup.vx[ef8m1, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=e1m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=4, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=4](v4)
  (v12) = vslideup.vx[ef8m1, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=e1m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vsetvlmax[mmul=8, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m1, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m1, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=8](v2)
  (v10) = vslideup.vx[ef8m1, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m1, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m1, dest=e1m1](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vget[part=ef8m1, src=ef8m2, idx=0](v1)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=8, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m1, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m1, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=16](v3)
  (v12) = vslideup.vx[ef8m1, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m1, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m1, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m1, dest=e1m1](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vget[part=ef8m1, src=ef8m2, idx=0](v2)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=8, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m1, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m1, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=32](v4)
  (v13) = vslideup.vx[ef8m1, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m1, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m1, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m1, dest=e1m1](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m1_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m1>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vget[part=ef8m1, src=ef8m2, idx=0](v3)
  (v5) = vsetvlmax[mmul=64, tama]()
  (v6) = vsetvlmax[mmul=8, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m1, ud](v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m1, ud](v6, v13)
  (v15) = vl_to_scalar[mmul=64](v5)
  (v14) = vslideup.vx[ef8m1, pd, fm](v6, v7, v15, v11)
  (v17) = vmseq.vv[ef8m1, ud, fm](v6, v14, v11)
  (v19) = vmerge.vvm[ef8m1, ud](v6, v7, v4, v17)
  (v21) = vec_to_vec[src=ef8m1, dest=e1m1](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m2, dest=e1m2](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m2, dest=e1m2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=e1m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m2, dest=e1m2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=e1m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m2, dest=e1m2](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=e1m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=e1m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=e1m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=e1m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=e1m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m2>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=1, tama]()
  (v10) = vsetvlmax[mmul=16, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m2, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m2, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=1](v9)
  (v17) = vslideup.vx[ef8m2, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m2, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m2, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m2, dest=e1m2](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m2>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=2, tama]()
  (v8) = vsetvlmax[mmul=16, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m2, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m2, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=2](v7)
  (v15) = vslideup.vx[ef8m2, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m2, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m2, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m2, dest=e1m2](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m2>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=4, tama]()
  (v6) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m2, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=4](v5)
  (v13) = vslideup.vx[ef8m2, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m2>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=8, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=8](v3)
  (v11) = vslideup.vx[ef8m2, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=e1m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m2>:
  (v1) = vsetvlmax[mmul=16, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m2, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m2, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=16](v1)
  (v9) = vslideup.vx[ef8m2, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m2, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m2, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m2, dest=e1m2](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m2>:
  (v1) = vget[part=ef8m2, src=ef8m4, idx=0](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vsetvlmax[mmul=16, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m2, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m2, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=32](v2)
  (v11) = vslideup.vx[ef8m2, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m2, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m2, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m2, dest=e1m2](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m2>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=2, tama]()
  (v9) = vsetvlmax[mmul=16, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m2, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m2, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=2](v8)
  (v16) = vslideup.vx[ef8m2, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m2, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m2, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m2, dest=e1m2](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=e1m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=e1m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=4, tama]()
  (v7) = vsetvlmax[mmul=16, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m2, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m2, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=4](v6)
  (v14) = vslideup.vx[ef8m2, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m2, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m2, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m2, dest=e1m2](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=e1m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=8, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=8](v4)
  (v12) = vslideup.vx[ef8m2, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vsetvlmax[mmul=16, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m2, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m2, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=16](v2)
  (v10) = vslideup.vx[ef8m2, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m2, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m2, dest=e1m2](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vget[part=ef8m2, src=ef8m4, idx=0](v1)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=16, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m2, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m2, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=32](v3)
  (v12) = vslideup.vx[ef8m2, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m2, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m2, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m2, dest=e1m2](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m2_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m2>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vget[part=ef8m2, src=ef8m4, idx=0](v2)
  (v4) = vsetvlmax[mmul=64, tama]()
  (v5) = vsetvlmax[mmul=16, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m2, ud](v5, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m2, ud](v5, v12)
  (v14) = vl_to_scalar[mmul=64](v4)
  (v13) = vslideup.vx[ef8m2, pd, fm](v5, v6, v14, v10)
  (v16) = vmseq.vv[ef8m2, ud, fm](v5, v13, v10)
  (v18) = vmerge.vvm[ef8m2, ud](v5, v6, v3, v16)
  (v20) = vec_to_vec[src=ef8m2, dest=e1m2](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m4, dest=e1m4](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m4, dest=e1m4](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m4, dest=e1m4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m4, dest=e1m4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=e1m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m4, dest=e1m4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=e1m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m4, dest=e1m4](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=e1m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=e1m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=e1m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=e1m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=e1m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=e1m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=e1m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=e1m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m4>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=1, tama]()
  (v12) = vsetvlmax[mmul=32, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m4, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m4, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=1](v11)
  (v19) = vslideup.vx[ef8m4, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m4, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m4, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m4, dest=e1m4](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m4>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=2, tama]()
  (v10) = vsetvlmax[mmul=32, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m4, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m4, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=2](v9)
  (v17) = vslideup.vx[ef8m4, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m4, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m4, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m4, dest=e1m4](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m4>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=4, tama]()
  (v8) = vsetvlmax[mmul=32, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m4, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m4, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=4](v7)
  (v15) = vslideup.vx[ef8m4, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m4, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m4, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m4, dest=e1m4](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m4>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=8, tama]()
  (v6) = vsetvlmax[mmul=32, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m4, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m4, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=8](v5)
  (v13) = vslideup.vx[ef8m4, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m4, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m4, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m4, dest=e1m4](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m4>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=16, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=16](v3)
  (v11) = vslideup.vx[ef8m4, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=e1m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m4>:
  (v1) = vsetvlmax[mmul=32, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m4, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m4, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=32](v1)
  (v9) = vslideup.vx[ef8m4, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m4, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m4, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m4, dest=e1m4](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m4>:
  (v1) = vget[part=ef8m4, src=ef8m8, idx=0](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vsetvlmax[mmul=32, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v4) = scalar_to_vec[ef8m4, ud](v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m4, ud](v3, v10)
  (v12) = vl_to_scalar[mmul=64](v2)
  (v11) = vslideup.vx[ef8m4, pd, fm](v3, v4, v12, v8)
  (v14) = vmseq.vv[ef8m4, ud, fm](v3, v11, v8)
  (v16) = vmerge.vvm[ef8m4, ud](v3, v4, v1, v14)
  (v18) = vec_to_vec[src=ef8m4, dest=e1m4](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=2, tama]()
  (v11) = vsetvlmax[mmul=32, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m4, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m4, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=2](v10)
  (v18) = vslideup.vx[ef8m4, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m4, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m4, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m4, dest=e1m4](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=e1m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=e1m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=e1m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=4, tama]()
  (v9) = vsetvlmax[mmul=32, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m4, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m4, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=4](v8)
  (v16) = vslideup.vx[ef8m4, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m4, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m4, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m4, dest=e1m4](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=e1m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=e1m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=8, tama]()
  (v7) = vsetvlmax[mmul=32, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m4, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m4, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=8](v6)
  (v14) = vslideup.vx[ef8m4, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m4, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m4, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m4, dest=e1m4](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=16, tama]()
  (v5) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m4, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=16](v4)
  (v12) = vslideup.vx[ef8m4, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vsetvlmax[mmul=32, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m4, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m4, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=32](v2)
  (v10) = vslideup.vx[ef8m4, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m4, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m4, dest=e1m4](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m4_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m4>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vget[part=ef8m4, src=ef8m8, idx=0](v1)
  (v3) = vsetvlmax[mmul=64, tama]()
  (v4) = vsetvlmax[mmul=32, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m4, ud](v4, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m4, ud](v4, v11)
  (v13) = vl_to_scalar[mmul=64](v3)
  (v12) = vslideup.vx[ef8m4, pd, fm](v4, v5, v13, v9)
  (v15) = vmseq.vv[ef8m4, ud, fm](v4, v12, v9)
  (v17) = vmerge.vvm[ef8m4, ud](v4, v5, v2, v15)
  (v19) = vec_to_vec[src=ef8m4, dest=e1m4](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  (v28) = vec_to_vec[src=ef8m8, dest=e1m8](v26)
  return (v28)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m8, dest=e1m8](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m8, dest=e1m8](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_uint8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m8, dest=e1m8](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_uint8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m8, dest=e1m8](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_uint8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m8, dest=e1m8](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_uint8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m8, dest=e1m8](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu17__rvv_uint16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  (v27) = vec_to_vec[src=ef8m8, dest=e1m8](v25)
  return (v27)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu17__rvv_uint16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=e1m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=e1m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=e1m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=e1m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=e1m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu17__rvv_uint32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=e1m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=e1m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=e1m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=e1m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=e1m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=e1m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=e1m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=e1m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_uint64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=e1m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int8mf8_tET_T0_(v0: vec<ef8mf8>) -> vec<e1m8>:
  (v1) = vset[part=ef8mf8, dest=ef8mf4, ud, idx=0](v0)
  (v3) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v5) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v3)
  (v7) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v5)
  (v9) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v7)
  (v11) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v9)
  (v13) = vsetvlmax[mmul=1, tama]()
  (v14) = vsetvlmax[mmul=64, tama]()
  (v17) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v15) = scalar_to_vec[ef8m8, ud](v14, v17)
  (v20) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v18) = scalar_to_vec[ef8m8, ud](v14, v20)
  (v22) = vl_to_scalar[mmul=1](v13)
  (v21) = vslideup.vx[ef8m8, pd, fm](v14, v15, v22, v18)
  (v24) = vmseq.vv[ef8m8, ud, fm](v14, v21, v18)
  (v26) = vmerge.vvm[ef8m8, ud](v14, v15, v11, v24)
  (v28) = vec_to_vec[src=ef8m8, dest=e1m8](v26)
  return (v28)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int8mf4_tET_T0_(v0: vec<ef8mf4>) -> vec<e1m8>:
  (v1) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v0)
  (v3) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v5) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v3)
  (v7) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v5)
  (v9) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v7)
  (v11) = vsetvlmax[mmul=2, tama]()
  (v12) = vsetvlmax[mmul=64, tama]()
  (v15) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v13) = scalar_to_vec[ef8m8, ud](v12, v15)
  (v18) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v16) = scalar_to_vec[ef8m8, ud](v12, v18)
  (v20) = vl_to_scalar[mmul=2](v11)
  (v19) = vslideup.vx[ef8m8, pd, fm](v12, v13, v20, v16)
  (v22) = vmseq.vv[ef8m8, ud, fm](v12, v19, v16)
  (v24) = vmerge.vvm[ef8m8, ud](v12, v13, v9, v22)
  (v26) = vec_to_vec[src=ef8m8, dest=e1m8](v24)
  return (v26)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int8mf2_tET_T0_(v0: vec<ef8mf2>) -> vec<e1m8>:
  (v1) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v0)
  (v3) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v5) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v3)
  (v7) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v5)
  (v9) = vsetvlmax[mmul=4, tama]()
  (v10) = vsetvlmax[mmul=64, tama]()
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v11) = scalar_to_vec[ef8m8, ud](v10, v13)
  (v16) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v14) = scalar_to_vec[ef8m8, ud](v10, v16)
  (v18) = vl_to_scalar[mmul=4](v9)
  (v17) = vslideup.vx[ef8m8, pd, fm](v10, v11, v18, v14)
  (v20) = vmseq.vv[ef8m8, ud, fm](v10, v17, v14)
  (v22) = vmerge.vvm[ef8m8, ud](v10, v11, v7, v20)
  (v24) = vec_to_vec[src=ef8m8, dest=e1m8](v22)
  return (v24)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu14__rvv_int8m1_tET_T0_(v0: vec<ef8m1>) -> vec<e1m8>:
  (v1) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v0)
  (v3) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v5) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v3)
  (v7) = vsetvlmax[mmul=8, tama]()
  (v8) = vsetvlmax[mmul=64, tama]()
  (v11) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v9) = scalar_to_vec[ef8m8, ud](v8, v11)
  (v14) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v12) = scalar_to_vec[ef8m8, ud](v8, v14)
  (v16) = vl_to_scalar[mmul=8](v7)
  (v15) = vslideup.vx[ef8m8, pd, fm](v8, v9, v16, v12)
  (v18) = vmseq.vv[ef8m8, ud, fm](v8, v15, v12)
  (v20) = vmerge.vvm[ef8m8, ud](v8, v9, v5, v18)
  (v22) = vec_to_vec[src=ef8m8, dest=e1m8](v20)
  return (v22)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu14__rvv_int8m2_tET_T0_(v0: vec<ef8m2>) -> vec<e1m8>:
  (v1) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v0)
  (v3) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v5) = vsetvlmax[mmul=16, tama]()
  (v6) = vsetvlmax[mmul=64, tama]()
  (v9) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v7) = scalar_to_vec[ef8m8, ud](v6, v9)
  (v12) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v10) = scalar_to_vec[ef8m8, ud](v6, v12)
  (v14) = vl_to_scalar[mmul=16](v5)
  (v13) = vslideup.vx[ef8m8, pd, fm](v6, v7, v14, v10)
  (v16) = vmseq.vv[ef8m8, ud, fm](v6, v13, v10)
  (v18) = vmerge.vvm[ef8m8, ud](v6, v7, v3, v16)
  (v20) = vec_to_vec[src=ef8m8, dest=e1m8](v18)
  return (v20)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu14__rvv_int8m4_tET_T0_(v0: vec<ef8m4>) -> vec<e1m8>:
  (v1) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v0)
  (v3) = vsetvlmax[mmul=32, tama]()
  (v4) = vsetvlmax[mmul=64, tama]()
  (v7) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v5) = scalar_to_vec[ef8m8, ud](v4, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v8) = scalar_to_vec[ef8m8, ud](v4, v10)
  (v12) = vl_to_scalar[mmul=32](v3)
  (v11) = vslideup.vx[ef8m8, pd, fm](v4, v5, v12, v8)
  (v14) = vmseq.vv[ef8m8, ud, fm](v4, v11, v8)
  (v16) = vmerge.vvm[ef8m8, ud](v4, v5, v1, v14)
  (v18) = vec_to_vec[src=ef8m8, dest=e1m8](v16)
  return (v18)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu14__rvv_int8m8_tET_T0_(v0: vec<ef8m8>) -> vec<e1m8>:
  (v1) = vsetvlmax[mmul=64, tama]()
  (v5) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v2) = scalar_to_vec[ef8m8, ud](v1, v5)
  (v8) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v6) = scalar_to_vec[ef8m8, ud](v1, v8)
  (v10) = vl_to_scalar[mmul=64](v1)
  (v9) = vslideup.vx[ef8m8, pd, fm](v1, v2, v10, v6)
  (v12) = vmseq.vv[ef8m8, ud, fm](v1, v9, v6)
  (v14) = vmerge.vvm[ef8m8, ud](v1, v2, v0, v12)
  (v16) = vec_to_vec[src=ef8m8, dest=e1m8](v14)
  return (v16)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_int16mf4_tET_T0_(v0: vec<ef4mf4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4mf4, dest=ef8mf4](v0)
  (v2) = vset[part=ef8mf4, dest=ef8mf2, ud, idx=0](v1)
  (v4) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v2)
  (v6) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v4)
  (v8) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v6)
  (v10) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v8)
  (v12) = vsetvlmax[mmul=2, tama]()
  (v13) = vsetvlmax[mmul=64, tama]()
  (v16) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v14) = scalar_to_vec[ef8m8, ud](v13, v16)
  (v19) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v17) = scalar_to_vec[ef8m8, ud](v13, v19)
  (v21) = vl_to_scalar[mmul=2](v12)
  (v20) = vslideup.vx[ef8m8, pd, fm](v13, v14, v21, v17)
  (v23) = vmseq.vv[ef8m8, ud, fm](v13, v20, v17)
  (v25) = vmerge.vvm[ef8m8, ud](v13, v14, v10, v23)
  (v27) = vec_to_vec[src=ef8m8, dest=e1m8](v25)
  return (v27)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_int16mf2_tET_T0_(v0: vec<ef4mf2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=e1m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int16m1_tET_T0_(v0: vec<ef4m1>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=e1m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int16m2_tET_T0_(v0: vec<ef4m2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=e1m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int16m4_tET_T0_(v0: vec<ef4m4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=e1m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int16m8_tET_T0_(v0: vec<ef4m8>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef4m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=e1m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu16__rvv_int32mf2_tET_T0_(v0: vec<ef2mf2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2mf2, dest=ef8mf2](v0)
  (v2) = vset[part=ef8mf2, dest=ef8m1, ud, idx=0](v1)
  (v4) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v2)
  (v6) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v4)
  (v8) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v6)
  (v10) = vsetvlmax[mmul=4, tama]()
  (v11) = vsetvlmax[mmul=64, tama]()
  (v14) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = scalar_to_vec[ef8m8, ud](v11, v14)
  (v17) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v15) = scalar_to_vec[ef8m8, ud](v11, v17)
  (v19) = vl_to_scalar[mmul=4](v10)
  (v18) = vslideup.vx[ef8m8, pd, fm](v11, v12, v19, v15)
  (v21) = vmseq.vv[ef8m8, ud, fm](v11, v18, v15)
  (v23) = vmerge.vvm[ef8m8, ud](v11, v12, v8, v21)
  (v25) = vec_to_vec[src=ef8m8, dest=e1m8](v23)
  return (v25)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int32m1_tET_T0_(v0: vec<ef2m1>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=e1m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int32m2_tET_T0_(v0: vec<ef2m2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=e1m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int32m4_tET_T0_(v0: vec<ef2m4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=e1m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int32m8_tET_T0_(v0: vec<ef2m8>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=ef2m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=e1m8](v15)
  return (v17)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int64m1_tET_T0_(v0: vec<e1m1>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m1, dest=ef8m1](v0)
  (v2) = vset[part=ef8m1, dest=ef8m2, ud, idx=0](v1)
  (v4) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v2)
  (v6) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v4)
  (v8) = vsetvlmax[mmul=8, tama]()
  (v9) = vsetvlmax[mmul=64, tama]()
  (v12) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v10) = scalar_to_vec[ef8m8, ud](v9, v12)
  (v15) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v13) = scalar_to_vec[ef8m8, ud](v9, v15)
  (v17) = vl_to_scalar[mmul=8](v8)
  (v16) = vslideup.vx[ef8m8, pd, fm](v9, v10, v17, v13)
  (v19) = vmseq.vv[ef8m8, ud, fm](v9, v16, v13)
  (v21) = vmerge.vvm[ef8m8, ud](v9, v10, v6, v19)
  (v23) = vec_to_vec[src=ef8m8, dest=e1m8](v21)
  return (v23)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int64m2_tET_T0_(v0: vec<e1m2>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m2, dest=ef8m2](v0)
  (v2) = vset[part=ef8m2, dest=ef8m4, ud, idx=0](v1)
  (v4) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v2)
  (v6) = vsetvlmax[mmul=16, tama]()
  (v7) = vsetvlmax[mmul=64, tama]()
  (v10) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v8) = scalar_to_vec[ef8m8, ud](v7, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v11) = scalar_to_vec[ef8m8, ud](v7, v13)
  (v15) = vl_to_scalar[mmul=16](v6)
  (v14) = vslideup.vx[ef8m8, pd, fm](v7, v8, v15, v11)
  (v17) = vmseq.vv[ef8m8, ud, fm](v7, v14, v11)
  (v19) = vmerge.vvm[ef8m8, ud](v7, v8, v4, v17)
  (v21) = vec_to_vec[src=ef8m8, dest=e1m8](v19)
  return (v21)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int64m4_tET_T0_(v0: vec<e1m4>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m4, dest=ef8m4](v0)
  (v2) = vset[part=ef8m4, dest=ef8m8, ud, idx=0](v1)
  (v4) = vsetvlmax[mmul=32, tama]()
  (v5) = vsetvlmax[mmul=64, tama]()
  (v8) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v6) = scalar_to_vec[ef8m8, ud](v5, v8)
  (v11) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = scalar_to_vec[ef8m8, ud](v5, v11)
  (v13) = vl_to_scalar[mmul=32](v4)
  (v12) = vslideup.vx[ef8m8, pd, fm](v5, v6, v13, v9)
  (v15) = vmseq.vv[ef8m8, ud, fm](v5, v12, v9)
  (v17) = vmerge.vvm[ef8m8, ud](v5, v6, v2, v15)
  (v19) = vec_to_vec[src=ef8m8, dest=e1m8](v17)
  return (v19)

def _Z23ZeroExtendResizeBitCastIu15__rvv_int64m8_tu15__rvv_int64m8_tET_T0_(v0: vec<e1m8>) -> vec<e1m8>:
  (v1) = vec_to_vec[src=e1m8, dest=ef8m8](v0)
  (v2) = vsetvlmax[mmul=64, tama]()
  (v6) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v3) = scalar_to_vec[ef8m8, ud](v2, v6)
  (v9) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v7) = scalar_to_vec[ef8m8, ud](v2, v9)
  (v11) = vl_to_scalar[mmul=64](v2)
  (v10) = vslideup.vx[ef8m8, pd, fm](v2, v3, v11, v7)
  (v13) = vmseq.vv[ef8m8, ud, fm](v2, v10, v7)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v3, v1, v13)
  (v17) = vec_to_vec[src=ef8m8, dest=e1m8](v15)
  return (v17)

