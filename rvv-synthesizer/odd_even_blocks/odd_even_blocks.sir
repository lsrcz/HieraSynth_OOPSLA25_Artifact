def _Z13OddEvenBlocksIu16__rvv_uint8mf8_tET_S0_S0_(v0: vec<ef8mf8>, v1: vec<ef8mf8>) -> vec<ef8mf8>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vid.v[ef8mf8, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000004]()
  (v6) = vsrl.vx.full[ef8mf8, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = vand.vx[ef8mf8, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef8mf8, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef8mf8, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu16__rvv_uint8mf4_tET_S0_S0_(v0: vec<ef8mf4>, v1: vec<ef8mf4>) -> vec<ef8mf4>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vid.v[ef8mf4, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000004]()
  (v6) = vsrl.vx.full[ef8mf4, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = vand.vx[ef8mf4, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef8mf4, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef8mf4, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu16__rvv_uint8mf2_tET_S0_S0_(v0: vec<ef8mf2>, v1: vec<ef8mf2>) -> vec<ef8mf2>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vid.v[ef8mf2, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000004]()
  (v6) = vsrl.vx.full[ef8mf2, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = vand.vx[ef8mf2, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef8mf2, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_uint8m1_tET_S0_S0_(v0: vec<ef8m1>, v1: vec<ef8m1>) -> vec<ef8m1>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vid.v[ef8m1, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000004]()
  (v6) = vsrl.vx.full[ef8m1, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = vand.vx[ef8m1, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef8m1, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_uint8m2_tET_S0_S0_(v0: vec<ef8m2>, v1: vec<ef8m2>) -> vec<ef8m2>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vid.v[ef8m2, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000004]()
  (v6) = vsrl.vx.full[ef8m2, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = vand.vx[ef8m2, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef8m2, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_uint8m4_tET_S0_S0_(v0: vec<ef8m4>, v1: vec<ef8m4>) -> vec<ef8m4>:
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vid.v[ef8m4, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000004]()
  (v6) = vsrl.vx.full[ef8m4, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = vand.vx[ef8m4, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef8m4, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_uint8m8_tET_S0_S0_(v0: vec<ef8m8>, v1: vec<ef8m8>) -> vec<ef8m8>:
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vid.v[ef8m8, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000004]()
  (v6) = vsrl.vx.full[ef8m8, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = vand.vx[ef8m8, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef8m8, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu17__rvv_uint16mf4_tET_S0_S0_(v0: vec<ef4mf4>, v1: vec<ef4mf4>) -> vec<ef4mf4>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vid.v[ef4mf4, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000003]()
  (v6) = vsrl.vx.full[ef4mf4, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v9) = vand.vx[ef4mf4, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef4mf4, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef4mf4, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu17__rvv_uint16mf2_tET_S0_S0_(v0: vec<ef4mf2>, v1: vec<ef4mf2>) -> vec<ef4mf2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vid.v[ef4mf2, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000003]()
  (v6) = vsrl.vx.full[ef4mf2, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v9) = vand.vx[ef4mf2, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef4mf2, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef4mf2, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu16__rvv_uint16m1_tET_S0_S0_(v0: vec<ef4m1>, v1: vec<ef4m1>) -> vec<ef4m1>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vid.v[ef4m1, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000003]()
  (v6) = vsrl.vx.full[ef4m1, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v9) = vand.vx[ef4m1, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef4m1, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef4m1, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu16__rvv_uint16m2_tET_S0_S0_(v0: vec<ef4m2>, v1: vec<ef4m2>) -> vec<ef4m2>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vid.v[ef4m2, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000003]()
  (v6) = vsrl.vx.full[ef4m2, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v9) = vand.vx[ef4m2, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef4m2, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef4m2, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu16__rvv_uint16m4_tET_S0_S0_(v0: vec<ef4m4>, v1: vec<ef4m4>) -> vec<ef4m4>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vid.v[ef4m4, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000003]()
  (v6) = vsrl.vx.full[ef4m4, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v9) = vand.vx[ef4m4, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef4m4, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef4m4, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu16__rvv_uint16m8_tET_S0_S0_(v0: vec<ef4m8>, v1: vec<ef4m8>) -> vec<ef4m8>:
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vid.v[ef4m8, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000003]()
  (v6) = vsrl.vx.full[ef4m8, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v9) = vand.vx[ef4m8, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef4m8, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef4m8, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu17__rvv_uint32mf2_tET_S0_S0_(v0: vec<ef2mf2>, v1: vec<ef2mf2>) -> vec<ef2mf2>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vid.v[ef2mf2, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000002]()
  (v6) = vsrl.vx.full[ef2mf2, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v9) = vand.vx[ef2mf2, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef2mf2, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef2mf2, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu16__rvv_uint32m1_tET_S0_S0_(v0: vec<ef2m1>, v1: vec<ef2m1>) -> vec<ef2m1>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vid.v[ef2m1, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000002]()
  (v6) = vsrl.vx.full[ef2m1, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v9) = vand.vx[ef2m1, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef2m1, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef2m1, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu16__rvv_uint32m2_tET_S0_S0_(v0: vec<ef2m2>, v1: vec<ef2m2>) -> vec<ef2m2>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vid.v[ef2m2, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000002]()
  (v6) = vsrl.vx.full[ef2m2, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v9) = vand.vx[ef2m2, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef2m2, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef2m2, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu16__rvv_uint32m4_tET_S0_S0_(v0: vec<ef2m4>, v1: vec<ef2m4>) -> vec<ef2m4>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vid.v[ef2m4, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000002]()
  (v6) = vsrl.vx.full[ef2m4, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v9) = vand.vx[ef2m4, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef2m4, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef2m4, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu16__rvv_uint32m8_tET_S0_S0_(v0: vec<ef2m8>, v1: vec<ef2m8>) -> vec<ef2m8>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vid.v[ef2m8, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000002]()
  (v6) = vsrl.vx.full[ef2m8, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v9) = vand.vx[ef2m8, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef2m8, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef2m8, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu16__rvv_uint64m1_tET_S0_S0_(v0: vec<e1m1>, v1: vec<e1m1>) -> vec<e1m1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vid.v[e1m1, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000001]()
  (v6) = vsrl.vx.full[e1m1, ud, fm](v2, v3, v7)
  (v9) = vand.vx[e1m1, ud, fm](v2, v6, v7)
  (v12) = scalar[xmul=1, imm=0x0000000000000000]()
  (v11) = vmseq.vx[e1m1, ud, fm](v2, v9, v12)
  (v14) = vmerge.vvm[e1m1, ud](v2, v0, v1, v11)
  return (v14)

def _Z13OddEvenBlocksIu16__rvv_uint64m2_tET_S0_S0_(v0: vec<e1m2>, v1: vec<e1m2>) -> vec<e1m2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vid.v[e1m2, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000001]()
  (v6) = vsrl.vx.full[e1m2, ud, fm](v2, v3, v7)
  (v9) = vand.vx[e1m2, ud, fm](v2, v6, v7)
  (v12) = scalar[xmul=1, imm=0x0000000000000000]()
  (v11) = vmseq.vx[e1m2, ud, fm](v2, v9, v12)
  (v14) = vmerge.vvm[e1m2, ud](v2, v0, v1, v11)
  return (v14)

def _Z13OddEvenBlocksIu16__rvv_uint64m4_tET_S0_S0_(v0: vec<e1m4>, v1: vec<e1m4>) -> vec<e1m4>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vid.v[e1m4, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000001]()
  (v6) = vsrl.vx.full[e1m4, ud, fm](v2, v3, v7)
  (v9) = vand.vx[e1m4, ud, fm](v2, v6, v7)
  (v12) = scalar[xmul=1, imm=0x0000000000000000]()
  (v11) = vmseq.vx[e1m4, ud, fm](v2, v9, v12)
  (v14) = vmerge.vvm[e1m4, ud](v2, v0, v1, v11)
  return (v14)

def _Z13OddEvenBlocksIu16__rvv_uint64m8_tET_S0_S0_(v0: vec<e1m8>, v1: vec<e1m8>) -> vec<e1m8>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vid.v[e1m8, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000001]()
  (v6) = vsrl.vx.full[e1m8, ud, fm](v2, v3, v7)
  (v9) = vand.vx[e1m8, ud, fm](v2, v6, v7)
  (v12) = scalar[xmul=1, imm=0x0000000000000000]()
  (v11) = vmseq.vx[e1m8, ud, fm](v2, v9, v12)
  (v14) = vmerge.vvm[e1m8, ud](v2, v0, v1, v11)
  return (v14)

def _Z13OddEvenBlocksIu15__rvv_int8mf8_tET_S0_S0_(v0: vec<ef8mf8>, v1: vec<ef8mf8>) -> vec<ef8mf8>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vid.v[ef8mf8, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000004]()
  (v6) = vsrl.vx.full[ef8mf8, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = vand.vx[ef8mf8, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef8mf8, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef8mf8, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_int8mf4_tET_S0_S0_(v0: vec<ef8mf4>, v1: vec<ef8mf4>) -> vec<ef8mf4>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vid.v[ef8mf4, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000004]()
  (v6) = vsrl.vx.full[ef8mf4, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = vand.vx[ef8mf4, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef8mf4, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef8mf4, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_int8mf2_tET_S0_S0_(v0: vec<ef8mf2>, v1: vec<ef8mf2>) -> vec<ef8mf2>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vid.v[ef8mf2, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000004]()
  (v6) = vsrl.vx.full[ef8mf2, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = vand.vx[ef8mf2, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef8mf2, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef8mf2, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu14__rvv_int8m1_tET_S0_S0_(v0: vec<ef8m1>, v1: vec<ef8m1>) -> vec<ef8m1>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vid.v[ef8m1, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000004]()
  (v6) = vsrl.vx.full[ef8m1, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = vand.vx[ef8m1, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef8m1, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef8m1, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu14__rvv_int8m2_tET_S0_S0_(v0: vec<ef8m2>, v1: vec<ef8m2>) -> vec<ef8m2>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vid.v[ef8m2, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000004]()
  (v6) = vsrl.vx.full[ef8m2, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = vand.vx[ef8m2, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef8m2, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef8m2, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu14__rvv_int8m4_tET_S0_S0_(v0: vec<ef8m4>, v1: vec<ef8m4>) -> vec<ef8m4>:
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vid.v[ef8m4, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000004]()
  (v6) = vsrl.vx.full[ef8m4, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = vand.vx[ef8m4, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef8m4, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef8m4, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu14__rvv_int8m8_tET_S0_S0_(v0: vec<ef8m8>, v1: vec<ef8m8>) -> vec<ef8m8>:
  (v2) = vsetvlmax[mmul=64, tama]()
  (v3) = vid.v[ef8m8, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000004]()
  (v6) = vsrl.vx.full[ef8m8, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f8, imm=0x0000000000000001]()
  (v9) = vand.vx[ef8m8, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f8, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef8m8, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef8m8, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu16__rvv_int16mf4_tET_S0_S0_(v0: vec<ef4mf4>, v1: vec<ef4mf4>) -> vec<ef4mf4>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vid.v[ef4mf4, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000003]()
  (v6) = vsrl.vx.full[ef4mf4, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v9) = vand.vx[ef4mf4, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef4mf4, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef4mf4, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu16__rvv_int16mf2_tET_S0_S0_(v0: vec<ef4mf2>, v1: vec<ef4mf2>) -> vec<ef4mf2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vid.v[ef4mf2, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000003]()
  (v6) = vsrl.vx.full[ef4mf2, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v9) = vand.vx[ef4mf2, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef4mf2, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef4mf2, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_int16m1_tET_S0_S0_(v0: vec<ef4m1>, v1: vec<ef4m1>) -> vec<ef4m1>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vid.v[ef4m1, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000003]()
  (v6) = vsrl.vx.full[ef4m1, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v9) = vand.vx[ef4m1, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef4m1, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef4m1, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_int16m2_tET_S0_S0_(v0: vec<ef4m2>, v1: vec<ef4m2>) -> vec<ef4m2>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vid.v[ef4m2, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000003]()
  (v6) = vsrl.vx.full[ef4m2, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v9) = vand.vx[ef4m2, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef4m2, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef4m2, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_int16m4_tET_S0_S0_(v0: vec<ef4m4>, v1: vec<ef4m4>) -> vec<ef4m4>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vid.v[ef4m4, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000003]()
  (v6) = vsrl.vx.full[ef4m4, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v9) = vand.vx[ef4m4, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef4m4, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef4m4, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_int16m8_tET_S0_S0_(v0: vec<ef4m8>, v1: vec<ef4m8>) -> vec<ef4m8>:
  (v2) = vsetvlmax[mmul=32, tama]()
  (v3) = vid.v[ef4m8, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000003]()
  (v6) = vsrl.vx.full[ef4m8, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f4, imm=0x0000000000000001]()
  (v9) = vand.vx[ef4m8, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f4, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef4m8, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef4m8, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu16__rvv_int32mf2_tET_S0_S0_(v0: vec<ef2mf2>, v1: vec<ef2mf2>) -> vec<ef2mf2>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vid.v[ef2mf2, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000002]()
  (v6) = vsrl.vx.full[ef2mf2, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v9) = vand.vx[ef2mf2, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef2mf2, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef2mf2, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_int32m1_tET_S0_S0_(v0: vec<ef2m1>, v1: vec<ef2m1>) -> vec<ef2m1>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vid.v[ef2m1, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000002]()
  (v6) = vsrl.vx.full[ef2m1, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v9) = vand.vx[ef2m1, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef2m1, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef2m1, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_int32m2_tET_S0_S0_(v0: vec<ef2m2>, v1: vec<ef2m2>) -> vec<ef2m2>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vid.v[ef2m2, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000002]()
  (v6) = vsrl.vx.full[ef2m2, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v9) = vand.vx[ef2m2, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef2m2, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef2m2, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_int32m4_tET_S0_S0_(v0: vec<ef2m4>, v1: vec<ef2m4>) -> vec<ef2m4>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vid.v[ef2m4, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000002]()
  (v6) = vsrl.vx.full[ef2m4, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v9) = vand.vx[ef2m4, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef2m4, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef2m4, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_int32m8_tET_S0_S0_(v0: vec<ef2m8>, v1: vec<ef2m8>) -> vec<ef2m8>:
  (v2) = vsetvlmax[mmul=16, tama]()
  (v3) = vid.v[ef2m8, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000002]()
  (v6) = vsrl.vx.full[ef2m8, ud, fm](v2, v3, v7)
  (v10) = scalar[xmul=f2, imm=0x0000000000000001]()
  (v9) = vand.vx[ef2m8, ud, fm](v2, v6, v10)
  (v13) = scalar[xmul=f2, imm=0x0000000000000000]()
  (v12) = vmseq.vx[ef2m8, ud, fm](v2, v9, v13)
  (v15) = vmerge.vvm[ef2m8, ud](v2, v0, v1, v12)
  return (v15)

def _Z13OddEvenBlocksIu15__rvv_int64m1_tET_S0_S0_(v0: vec<e1m1>, v1: vec<e1m1>) -> vec<e1m1>:
  (v2) = vsetvlmax[mmul=1, tama]()
  (v3) = vid.v[e1m1, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000001]()
  (v6) = vsrl.vx.full[e1m1, ud, fm](v2, v3, v7)
  (v9) = vand.vx[e1m1, ud, fm](v2, v6, v7)
  (v12) = scalar[xmul=1, imm=0x0000000000000000]()
  (v11) = vmseq.vx[e1m1, ud, fm](v2, v9, v12)
  (v14) = vmerge.vvm[e1m1, ud](v2, v0, v1, v11)
  return (v14)

def _Z13OddEvenBlocksIu15__rvv_int64m2_tET_S0_S0_(v0: vec<e1m2>, v1: vec<e1m2>) -> vec<e1m2>:
  (v2) = vsetvlmax[mmul=2, tama]()
  (v3) = vid.v[e1m2, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000001]()
  (v6) = vsrl.vx.full[e1m2, ud, fm](v2, v3, v7)
  (v9) = vand.vx[e1m2, ud, fm](v2, v6, v7)
  (v12) = scalar[xmul=1, imm=0x0000000000000000]()
  (v11) = vmseq.vx[e1m2, ud, fm](v2, v9, v12)
  (v14) = vmerge.vvm[e1m2, ud](v2, v0, v1, v11)
  return (v14)

def _Z13OddEvenBlocksIu15__rvv_int64m4_tET_S0_S0_(v0: vec<e1m4>, v1: vec<e1m4>) -> vec<e1m4>:
  (v2) = vsetvlmax[mmul=4, tama]()
  (v3) = vid.v[e1m4, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000001]()
  (v6) = vsrl.vx.full[e1m4, ud, fm](v2, v3, v7)
  (v9) = vand.vx[e1m4, ud, fm](v2, v6, v7)
  (v12) = scalar[xmul=1, imm=0x0000000000000000]()
  (v11) = vmseq.vx[e1m4, ud, fm](v2, v9, v12)
  (v14) = vmerge.vvm[e1m4, ud](v2, v0, v1, v11)
  return (v14)

def _Z13OddEvenBlocksIu15__rvv_int64m8_tET_S0_S0_(v0: vec<e1m8>, v1: vec<e1m8>) -> vec<e1m8>:
  (v2) = vsetvlmax[mmul=8, tama]()
  (v3) = vid.v[e1m8, ud, fm](v2)
  (v7) = scalar[xmul=1, imm=0x0000000000000001]()
  (v6) = vsrl.vx.full[e1m8, ud, fm](v2, v3, v7)
  (v9) = vand.vx[e1m8, ud, fm](v2, v6, v7)
  (v12) = scalar[xmul=1, imm=0x0000000000000000]()
  (v11) = vmseq.vx[e1m8, ud, fm](v2, v9, v12)
  (v14) = vmerge.vvm[e1m8, ud](v2, v0, v1, v11)
  return (v14)

